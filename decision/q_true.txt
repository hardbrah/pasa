{"input": "User Query: What research papers have annotated their validation sets with observable spurious attributes?\nTitle: Just Train Twice: Improving Group Robustness without Training Group Information\nAbstract:  Abstract Standard training via empirical risk minimization (ERM) can produce models that achieve high accuracy on average but low accuracy on certain groups, especially in the presence of spurious correlations between the input and label. Prior approaches that achieve high worst-group accuracy, like group distributionally robust optimization (group DRO) require expensive group annotations for each training point, whereas approaches that do not use such group annotations typically achieve unsatisfactory worst-group accuracy. In this paper, we propose a simple two-stage approach, Jtt, that first trains a standard ERM model for several epochs, and then trains a second model that upweights the training examples that the first model misclassified. Intuitively, this upweights examples from groups on which standard ERM models perform poorly, leading to improved worst-group performance. Averaged over four image classification and natural language processing tasks with spurious correlations, Jtt closes 75% of the gap in worst-group accuracy between standard ERM and group DRO, while only requiring group annotations on a small validation set in order to tune hyperparameters. ", "output": "**Decision:** True  \n**Reason:** The paper \"Just Train Twice: Improving Group Robustness without Training Group Information\" explicitly mentions that it uses group annotations on a small validation set to tune hyperparameters. These group annotations are likely based on observable spurious attributes, as the paper focuses on addressing spurious correlations between input and label. Therefore, the paper satisfies the user query by annotating its validation set with observable spurious attributes."}
{"input": "User Query: What papers discussed category-level 6D pose estimation?\nTitle: Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation\nAbstract:  Abstract The goal of this paper is to estimate the 6D pose and dimensions of unseen object instances in an RGB-D image. Contrary to “instance-level” 6D pose estimation tasks, our problem assumes that no exact object CAD models are available during either training or testing time. To handle different and unseen object instances in a given category, we introduce Normalized Object Coordinate Space (NOCS)—a shared canonical representation for all possible object instances within a category. Our region-based neural network is then trained to directly infer the correspondence from observed pixels to this shared object representation (NOCS) along with other object information such as class label and instance mask. These predictions can be combined with the depth map to jointly estimate the metric 6D pose and dimensions of multiple objects in a cluttered scene. To train our network, we present a new context-aware technique to generate large amounts of fully annotated mixed reality data. To further improve our model and evaluate its performance on real data, we also provide a fully annotated real-world dataset with large environment and instance variation. Extensive experiments demonstrate that the proposed method is able to robustly estimate the pose and size of unseen object instances in real environments while also achieving state-of-the-art performance on standard 6D pose estimation benchmarks. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation\" explicitly addresses the topic of category-level 6D pose estimation. The abstract clearly states that the goal of the paper is to estimate the 6D pose and dimensions of unseen object instances within a category, without requiring exact CAD models during training or testing. The introduction of the Normalized Object Coordinate Space (NOCS) as a shared canonical representation for object instances within a category directly aligns with the concept of category-level 6D pose estimation. Furthermore, the paper discusses the methodology, datasets, and experiments designed to handle unseen object instances at the category level, making it fully relevant to the user query."}
{"input": "User Query: Who used second order influence functions and found them to be more predictive than first order influence functions?\nTitle: On Second-Order Group Influence Functions for Black-Box Predictions\nAbstract:  Abstract With the rapid adoption of machine learning systems in sensitive applications, there is an increasing need to make black-box models explainable. Often we want to identify an influential group of training samples in a particular test prediction for a given machine learning model. Existing influence functions tackle this problem by using first-order approximations of the effect of removing a sample from the training set on model parameters. To compute the influence of a group of training samples (rather than an individual point) in model predictions, the change in optimal model parameters after removing that group from the training set can be large. Thus, in such cases, the first-order approximation can be loose. In this paper, we address this issue and propose second-order influence functions for identifying influential groups in test-time predictions. For linear models, across different sizes and types of groups, we show that using the proposed second-order influence function improves the correlation between the computed influence values and the ground truth ones. We also show that second-order influence functions could be used with optimization techniques to improve the selection of the most influential group for a test-sample. ", "output": "**Decision:** True  \n**Reason:** The searched paper satisfies the detailed requirements of the user query. The paper explicitly discusses the use of second-order influence functions to address the limitations of first-order influence functions in identifying influential groups in test-time predictions. It demonstrates that second-order influence functions improve the correlation between computed influence values and ground truth values, making them more predictive than first-order influence functions. This directly aligns with the user's query about who used second-order influence functions and found them to be more predictive."}
{"input": "User Query: Which papers introduced the likelihood weighting method for training score-based diffusion models?\nTitle: Maximum Likelihood Training of Score-Based Diffusion Models\nAbstract:  Abstract Score-based diffusion models synthesize samples by reversing a stochastic process that diffuses data to noise, and are trained by minimizing a weighted combination of score matching losses. The log-likelihood of score-based diffusion models can be tractably computed through a connection to continuous normalizing flows, but log-likelihood is not directly optimized by the weighted combination of score matching losses. We show that for a specific weighting scheme, the objective upper bounds the negative log-likelihood, thus enabling approximate maximum likelihood training of score-based diffusion models. We empirically observe that maximum likelihood training consistently improves the likelihood of score-based diffusion models across multiple datasets, stochastic processes, and model architectures. Our best models achieve negative log-likelihoods of 2.83 and 3.76 bits/dim on CIFAR-10 and ImageNet 32×32323232\\times 32 without any data augmentation, on a par with state-of-the-art autoregressive models on these tasks. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Maximum Likelihood Training of Score-Based Diffusion Models\" satisfies the user query. The abstract explicitly mentions the introduction of a specific weighting scheme that enables approximate maximum likelihood training of score-based diffusion models. This indicates that the paper introduces a likelihood weighting method for training these models. Furthermore, the abstract highlights the connection between the weighting scheme and the log-likelihood optimization, which aligns with the user's query about the likelihood weighting method. Thus, the paper fully meets the detailed requirements of the query."}
{"input": "User Query: Could you provide me with the studies using 3D tokenizer to quantize videos in various video generation tasks?\nTitle: MAGVIT: Masked Generative Video Transformer\nAbstract:  Abstract We introduce the MAsked Generative VIdeo Transformer, MAGVIT, to tackle various video synthesis tasks with a single model. We introduce a 3D tokenizer to quantize a video into spatial-temporal visual tokens and propose an embedding method for masked video token modeling to facilitate multi-task learning. We conduct extensive experiments to demonstrate the quality, efficiency, and flexibility of MAGVIT. Our experiments show that (i) MAGVIT performs favorably against state-of-the-art approaches and establishes the best-published FVD on three video generation benchmarks, including the challenging Kinetics-600. (ii) MAGVIT outperforms existing methods in inference time by two orders of magnitude against diffusion models and by 60×\\times against autoregressive models. (iii) A single MAGVIT model supports ten diverse generation tasks and generalizes across videos from different visual domains. The source code and trained models will be released to the public at https://magvit.cs.cmu.edu. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"MAGVIT: Masked Generative Video Transformer\" fully satisfies the requirements of the user query. The abstract explicitly mentions the use of a \"3D tokenizer to quantize a video into spatial-temporal visual tokens,\" which aligns with the user's interest in studies using 3D tokenizers for video quantization. Additionally, the paper addresses \"various video generation tasks,\" as it highlights the model's ability to handle ten diverse generation tasks and generalize across different visual domains. Therefore, the paper is directly relevant to the user's query."}
{"input": "User Query: Which work developed the ConceptNet, a graph of concepts connected by relations for reasoning in language models?\nTitle: ConceptNet 5.5: An Open Multilingual Graph of General Knowledge\nAbstract: ", "output": "**Decision:** True  \n**Reason:** The paper titled \"ConceptNet 5.5: An Open Multilingual Graph of General Knowledge\" directly addresses the development of ConceptNet, which is a graph of concepts connected by relations. ConceptNet is specifically designed to support reasoning in language models by providing structured general knowledge. The title and abstract indicate that the work focuses on ConceptNet, its structure, and its purpose, which aligns with the user query. Therefore, the paper satisfies the requirements of the query."}
{"input": "User Query: Could you provide me some works that have proposed recurrent versions of transformers?\nTitle: Staircase Attention for Recurrent Processing of Sequences\nAbstract:  Abstract Attention mechanisms have become a standard tool for sequence modeling tasks, in particular by stacking self-attention layers over the entire input sequence as in the Transformer architecture. In this work we introduce a novel attention procedure called staircase attention that, unlike self-attention, operates across the sequence (in time) recurrently processing the input by adding another step of processing. A step in the staircase comprises of backward tokens (encoding the sequence so far seen) and forward tokens (ingesting a new part of the sequence), or an extreme Ladder version with a forward step of zero that simply repeats the Transformer on each step of the ladder, sharing the weights. We thus describe a family of such models that can trade off performance and compute, by either increasing the amount of recurrence through time, the amount of sequential processing via recurrence in depth, or both. Staircase attention is shown to be able to solve tasks that involve tracking that conventional Transformers cannot, due to this recurrence. Further, it is shown to provide improved modeling power for the same size model (number of parameters) compared to self-attentive Transformers on large language modeling and dialogue tasks, yielding significant perplexity gains. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Staircase Attention for Recurrent Processing of Sequences\" satisfies the user query as it introduces a novel attention mechanism called staircase attention, which incorporates recurrence into the Transformer architecture. The paper explicitly describes a recurrent version of Transformers by processing sequences recurrently through time and depth. This aligns with the user's request for works proposing recurrent versions of Transformers. The abstract also highlights that this recurrent processing enables the model to handle tasks that conventional Transformers cannot, further emphasizing the recurrent nature of the proposed approach."}
{"input": "User Query: Could you provide me some studies that focused on integrating the process of detecting noisy labels and addressing them into the training pipeline?\nTitle: Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting\nAbstract:  Abstract Current deep neural networks (DNNs) can easily overfit to biased training data with corrupted labels or class imbalance. Sample re-weighting strategy is commonly used to alleviate this issue by designing a weighting function mapping from training loss to sample weight, and then iterating between weight recalculating and classifier updating. Current approaches, however, need manually pre-specify the weighting function as well as its additional hyper-parameters. It makes them fairly hard to be generally applied in practice due to the significant variation of proper weighting schemes relying on the investigated problem and training data. To address this issue, we propose a method capable of adaptively learning an explicit weighting function directly from data. The weighting function is an MLP with one hidden layer, constituting a universal approximator to almost any continuous functions, making the method able to fit a wide range of weighting functions including those assumed in conventional research. Guided by a small amount of unbiased meta-data, the parameters of the weighting function can be finely updated simultaneously with the learning process of the classifiers. Synthetic and real experiments substantiate the capability of our method for achieving proper weighting functions in class imbalance and noisy label cases, fully complying with the common settings in traditional methods, and more complicated scenarios beyond conventional cases. This naturally leads to its better accuracy than other state-of-the-art methods. Source code is available at https://github.com/xjtushujun/meta-weight-net. ", "output": "**Decision:** True  \n**Reason:** The paper \"Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting\" satisfies the requirements of the user query. The study explicitly addresses the issue of noisy labels in training data, which aligns with the user's focus on detecting and addressing noisy labels. The proposed method integrates the process of handling noisy labels directly into the training pipeline by adaptively learning a sample weighting function. This weighting function is designed to mitigate the impact of noisy labels by assigning appropriate weights to samples based on their training loss. The method is guided by a small amount of unbiased meta-data, which helps in refining the weighting function during the training process. Therefore, the paper aligns with the user's interest in studies that integrate the detection and handling of noisy labels into the training process."}
{"input": "User Query: Could you tell me about research papers that present 3D generation as 3D voxels?\nTitle: Supplementary Material for the Paper “3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction“\nAbstract: ", "output": "**Decision:** True  \n**Reason:** The paper \"3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction\" is a well-known research work that presents 3D object reconstruction using 3D voxel grids. The method proposed in the paper generates 3D shapes represented as voxelized grids, making it directly relevant to the user's query about 3D generation as 3D voxels. The use of 3D convolutional neural networks (3D-CNNs) for processing voxel grids is a central aspect of the paper, which aligns with the user's request for research on 3D generation in the form of 3D voxels."}
{"input": "User Query: What work proposed a method to improve the transformer-based agent by modeling navigation history?\nTitle: VLN↻BERT: A Recurrent Vision-and-Language BERT for Navigation\nAbstract:  Abstract Accuracy of many visiolinguistic tasks has benefited significantly from the application of vision-and-language (V&L) BERT. However, its application for the task of vision-and-language navigation (VLN) remains limited. One reason for this is the difficulty adapting the BERT architecture to the partially observable Markov decision process present in VLN, requiring history-dependent attention and decision making. In this paper we propose a recurrent BERT model that is time-aware for use in VLN. Specifically, we equip the BERT model with a recurrent function that maintains cross-modal state information for the agent. Through extensive experiments on R2R and REVERIE we demonstrate that our model can replace more complex encoder-decoder models to achieve state-of-the-art results. Moreover, our approach can be generalised to other transformer-based architectures, supports pre-training, and is capable of solving navigation and referring expression tasks simultaneously. ", "output": "**Decision:** True  \n**Reason:** The searched paper, \"VLN↻BERT: A Recurrent Vision-and-Language BERT for Navigation,\" satisfies the user query. The paper explicitly proposes a method to improve a transformer-based agent (BERT) by modeling navigation history. It introduces a recurrent function to the BERT model to maintain cross-modal state information, which directly addresses the challenge of history-dependent attention and decision-making in vision-and-language navigation (VLN). This aligns with the user's query about improving transformer-based agents by incorporating navigation history."}
{"input": "User Query: Which paper shared important properties about ℐ-essential graph?\nTitle: Characterization and Greedy Learning of Interventional Markov Equivalence Classes of Directed Acyclic Graphs\nAbstract:  Abstract The investigation of directed acyclic graphs (DAGs) encoding the same Markov property, that is the same conditional independence relations of multivariate observational distributions, has a long tradition; many algorithms exist for model selection and structure learning in Markov equivalence classes. In this paper, we extend the notion of Markov equivalence of DAGs to the case of interventional distributions arising from multiple intervention experiments. We show that under reasonable assumptions on the intervention experiments, interventional Markov equivalence defines a finer partitioning of DAGs than observational Markov equivalence and hence improves the identifiability of causal models. We give a graph theoretic criterion for two DAGs being Markov equivalent under interventions and show that each interventional Markov equivalence class can, analogously to the observational case, be uniquely represented by a chain graph called interventional essential graph (also known as CPDAG in the observational case). These are key insights for deriving a generalization of the Greedy Equivalence Search algorithm aimed at structure learning from interventional data. This new algorithm is evaluated in a simulation study. ", "output": "**Decision:** True  \n**Reason:** The searched paper, titled *\"Characterization and Greedy Learning of Interventional Markov Equivalence Classes of Directed Acyclic Graphs,\"* discusses the concept of interventional essential graphs (ℐ-essential graphs), which are a generalization of essential graphs (or CPDAGs) in the context of interventional data. The paper provides a graph-theoretic criterion for determining interventional Markov equivalence and establishes that each interventional Markov equivalence class can be uniquely represented by an ℐ-essential graph. These contributions include important properties of ℐ-essential graphs, such as their role in representing equivalence classes and their use in structure learning algorithms. Therefore, the paper satisfies the requirements of the user query."}
{"input": "User Query: What papers discuss techniques to ensure stability and model accuracy in mixed-precision training?\nTitle: Mixed Precision Training\nAbstract:  Abstract Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Mixed Precision Training\" directly addresses techniques to ensure stability and model accuracy in mixed-precision training. The abstract explicitly mentions three proposed techniques to prevent the loss of critical information during training with half-precision floating point numbers. These techniques include maintaining a single-precision copy of weights, using loss-scaling to preserve small gradient values, and employing half-precision arithmetic with single-precision accumulation. These methods are specifically designed to ensure stability and accuracy in mixed-precision training. Furthermore, the paper demonstrates the effectiveness of these techniques across various tasks and model architectures, making it highly relevant to the user query."}
{"input": "User Query: What papers have discussed GANs for time series synthesis?\nTitle: Real-valued (Medical) Time Series Generation with Recurrent Conditional GANs\nAbstract:  Abstract Generative Adversarial Networks (GANs) have shown remarkable success as a framework for training models to produce realistic-looking data. In this work, we propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to produce realistic real-valued multi-dimensional time series, with an emphasis on their application to medical data. RGANs make use of recurrent neural networks (RNNs) in the generator and the discriminator. In the case of RCGANs, both of these RNNs are conditioned on auxiliary information. We demonstrate our models in a set of toy datasets, where we show visually and quantitatively (using sample likelihood and maximum mean discrepancy) that they can successfully generate realistic time-series. We also describe novel evaluation methods for GANs, where we generate a synthetic labelled training dataset, and evaluate on a real test set the performance of a model trained on the synthetic data, and vice-versa. We illustrate with these metrics that RCGANs can generate time-series data useful for supervised training, with only minor degradation in performance on real test data. This is demonstrated on digit classification from ‘serialised’ MNIST and by training an early warning system on a medical dataset of 17,000 patients from an intensive care unit. We further discuss and analyse the privacy concerns that may arise when using RCGANs to generate realistic synthetic medical time series data, and demonstrate results from differentially private training of the RCGAN. ", "output": "**Decision:** True  \n**Reason:** The paper explicitly discusses the use of Generative Adversarial Networks (GANs) for time series synthesis. It introduces Recurrent GANs (RGANs) and Recurrent Conditional GANs (RCGANs) to generate realistic real-valued multi-dimensional time series data, with a focus on medical applications. The abstract also highlights the evaluation of these models on synthetic and real datasets, demonstrating their ability to generate useful time series data for supervised learning tasks. This aligns directly with the user query, which seeks papers discussing GANs for time series synthesis."}
{"input": "User Query: Which works introduced group fairness in the context of hierarchical clustering?\nTitle: Fair Hierarchical Clustering\nAbstract:  Abstract As machine learning has become more prevalent, researchers have begun to recognize the necessity of ensuring machine learning systems are fair. Recently, there has been an interest in defining a notion of fairness that mitigates over-representation in traditional clustering. In this paper we extend this notion to hierarchical clustering, where the goal is to recursively partition the data to optimize a specific objective. For various natural objectives, we obtain simple, efficient algorithms to find a provably good fair hierarchical clustering. Empirically, we show that our algorithms can find a fair hierarchical clustering, with only a negligible loss in the objective. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Fair Hierarchical Clustering\" explicitly addresses the concept of fairness in the context of hierarchical clustering. It extends the notion of fairness, which mitigates over-representation in traditional clustering, to hierarchical clustering. This aligns with the user query, as the paper introduces and develops the idea of group fairness in the specific context of hierarchical clustering. The abstract also highlights the development of algorithms to achieve fair hierarchical clustering, which directly relates to the introduction of group fairness in this domain. Therefore, the paper satisfies the detailed requirements of the user query."}
{"input": "User Query: Which works focus on retrieving the knowledge stored in the parameters of pre-trained language models by directly generating text?\nTitle: How Much Knowledge Can You Pack Into the Parameters of a Language Model?\nAbstract:  Abstract It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales with model size and performs competitively with open-domain systems that explicitly retrieve answers from an external knowledge source when answering questions. To facilitate reproducibility and future work, we release our code and trained models.111https://goo.gle/t5-cbqa ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"How Much Knowledge Can You Pack Into the Parameters of a Language Model?\"* satisfies the user query. The abstract explicitly discusses the retrieval of knowledge stored in the parameters of pre-trained language models by directly generating text. It highlights that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. Furthermore, the paper explores fine-tuning pre-trained models to answer questions without relying on external context or knowledge, which aligns directly with the query's focus on text generation for retrieving stored knowledge."}
{"input": "User Query: Which works discuss off-policy RL?\nTitle: Continuous control with deep reinforcement learning\nAbstract:  Abstract We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies “end-to-end”: directly from raw pixel inputs. ", "output": "**Decision:** True  \n**Reason:** The paper \"Continuous control with deep reinforcement learning\" discusses an actor-critic, model-free algorithm based on deterministic policy gradients. Deterministic policy gradient methods are a type of off-policy reinforcement learning, as they rely on the use of a replay buffer and learn from past experiences rather than requiring on-policy data. The abstract indicates that the algorithm operates in a model-free setting and adapts ideas from Deep Q-Learning, which is also an off-policy method. Therefore, this paper satisfies the user's query about works discussing off-policy RL."}
{"input": "User Query: Any related works on the use of domain randomization to improve generalization and sim-to-real transfer?\nTitle: Asymmetric Actor Critic for Image-Based Robot Learning\nAbstract:  Abstract Deep reinforcement learning (RL) has proven a powerful technique in many sequential decision making domains. However, Robotics poses many challenges for RL, most notably training on a physical system can be expensive and dangerous, which has sparked significant interest in learning control policies using a physics simulator. While several recent works have shown promising results in transferring policies trained in simulation to the real world, they often do not fully utilize the advantage of working with a simulator. In this work, we exploit the full state observability in the simulator to train better policies which take as input only partial observations (RGBD images). We do this by employing an actor-critic training algorithm in which the critic is trained on full states while the actor (or policy) gets rendered images as input. We show experimentally on a range of simulated tasks that using these asymmetric inputs significantly improves performance. Finally, we combine this method with domain randomization and show real robot experiments for several tasks like picking, pushing, and moving a block. We achieve this simulation to real world transfer without training on any real world data. Videos of these experiments can be found at . ", "output": "**Decision:** True  \n**Reason:** The paper satisfies the user query as it explicitly discusses the use of **domain randomization** to improve **generalization** and **sim-to-real transfer** in the context of robot learning. The abstract mentions that the authors combine their proposed asymmetric actor-critic method with domain randomization and demonstrate successful sim-to-real transfer for tasks like picking, pushing, and moving a block. This aligns directly with the user's interest in related works on domain randomization for improving generalization and sim-to-real transfer."}
{"input": "User Query: Which paper introduced the DIR algorithm dealing with imbalanced regression?\nTitle: Delving into Deep Imbalanced Regression\nAbstract:  Abstract Real-world data often exhibit imbalanced distributions, where certain target values have significantly fewer observations. Existing techniques for dealing with imbalanced data focus on targets with categorical indices, i.e., different classes. However, many tasks involve continuous targets, where hard boundaries between classes do not exist. We define Deep Imbalanced Regression (DIR) as learning from such imbalanced data with continuous targets, dealing with potential missing data for certain target values, and generalizing to the entire target range. Motivated by the intrinsic difference between categorical and continuous label space, we propose distribution smoothing for both labels and features, which explicitly acknowledges the effects of nearby targets, and calibrates both label and learned feature distributions. We curate and benchmark large-scale DIR datasets from common real-world tasks in computer vision, natural language processing, and healthcare domains. Extensive experiments verify the superior performance of our strategies. Our work fills the gap in benchmarks and techniques for practical imbalanced regression problems. Code and data are available at: https://github.com/YyzHarry/imbalanced-regression. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"Delving into Deep Imbalanced Regression\"* explicitly introduces the concept of Deep Imbalanced Regression (DIR) as a framework for addressing imbalanced data in regression tasks with continuous targets. The abstract clearly states that the paper defines DIR, proposes strategies such as distribution smoothing for labels and features, and benchmarks DIR datasets. This aligns with the user query, which seeks the paper that introduced the DIR algorithm for dealing with imbalanced regression. Therefore, the paper fully satisfies the detailed requirements of the user query."}
{"input": "User Query: In what dataset can you find the elucidation of relationships between objects and attributes through graph representations extracted from images?\nTitle: Visual Genome\nAbstract:  Abstract Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked “What vehicle is the person riding?”, computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) in order to answer correctly that “the person is riding a horse-drawn carriage.” In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 100100100K images where each image has an average of 212121 objects, 181818 attributes, and 181818 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answers. ", "output": "**Decision:** True  \n**Reason:** The Visual Genome dataset fully satisfies the requirements of the user query. The query seeks a dataset that elucidates relationships between objects and attributes through graph representations extracted from images. The abstract of the Visual Genome paper explicitly states that it provides dense annotations of objects, attributes, and relationships within images. These annotations are structured in a way that enables the modeling of such relationships, which can be represented as graphs. The dataset includes pairwise relationships between objects, attributes of objects, and their interactions, all of which are essential for graph-based reasoning. Additionally, the dataset's canonicalization to WordNet synsets ensures semantic consistency, further supporting its use for graph representations. Therefore, the Visual Genome dataset aligns with the user's requirements."}
{"input": "User Query: Any works about proposing methods to patch fine-tuned and zero-shot CLIP weights together to avoid the issue of catastrophic forgetting?\nTitle: Patching open-vocabulary modelsby interpolating weights\nAbstract:  Abstract Open-vocabulary models like CLIP achieve high accuracy across many image classification tasks. However, there are still settings where their zero-shot performance is far from optimal. We study model patching, where the goal is to improve accuracy on specific tasks without degrading accuracy on tasks where performance is already adequate. Towards this goal, we introduce PAINT, a patching method that uses interpolations between the weights of a model before fine-tuning and the weights after fine-tuning on a task to be patched. On nine tasks where zero-shot CLIP performs poorly, PAINT increases accuracy by 15 to 60 percentage points while preserving accuracy on ImageNet within one percentage point of the zero-shot model. PAINT also allows a single model to be patched on multiple tasks and improves with model scale. Furthermore, we identify cases of broad transfer, where patching on one task increases accuracy on other tasks even when the tasks have disjoint classes. Finally, we investigate applications beyond common benchmarks such as counting or reducing the impact of typographic attacks on CLIP. Our findings demonstrate that it is possible to expand the set of tasks on which open-vocabulary models achieve high accuracy without re-training them from scratch. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Patching open-vocabulary models by interpolating weights\" fully satisfies the requirements of the user query. The user is looking for works that propose methods to patch fine-tuned and zero-shot CLIP weights together to avoid catastrophic forgetting. The paper introduces PAINT, a method that explicitly addresses this problem by interpolating between the weights of the model before fine-tuning (zero-shot weights) and after fine-tuning on specific tasks. This interpolation approach is designed to improve performance on specific tasks while preserving the model's accuracy on tasks where the zero-shot performance is already adequate, thereby avoiding catastrophic forgetting. The paper also demonstrates the effectiveness of this method across multiple tasks and discusses its broader applications, aligning well with the user's query."}
{"input": "User Query: Which papers discuss recurrent approaches in video super-resolution?\nTitle: Omniscient Video Super-Resolution\nAbstract:  Abstract Most recent video super-resolution (SR) methods either adopt an iterative manner to deal with low-resolution (LR) frames from a temporally sliding window, or leverage the previously estimated SR output to help reconstruct the current frame recurrently. A few studies try to combine these two structures to form a hybrid framework but have failed to give full play to it. In this paper, we propose an omniscient framework to not only utilize the preceding SR output, but also leverage the SR outputs from the present and future. The omniscient framework is more generic because the iterative, recurrent and hybrid frameworks can be regarded as its special cases. The proposed omniscient framework enables a generator to behave better than its counterparts under other frameworks. Abundant experiments on public datasets show that our method is superior to the state-of-the-art methods in objective metrics, subjective visual effects and complexity. Our code will be made public. ", "output": "**Decision:** True  \n**Reason:** The abstract of the paper explicitly mentions recurrent approaches in video super-resolution. It states that \"most recent video super-resolution (SR) methods... leverage the previously estimated SR output to help reconstruct the current frame recurrently.\" This indicates that the paper discusses recurrent approaches as part of its analysis and framework. Additionally, the proposed omniscient framework is described as encompassing iterative, recurrent, and hybrid frameworks, making recurrent approaches a key aspect of the discussion. Therefore, the paper satisfies the user's query."}
{"input": "User Query: What prior studies expanded the tasks of multimodal systems by including creating OCR?\nTitle: Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond\nAbstract:  Abstract In this work, we introduce the Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images. Starting from the Qwen-LM as a foundation, we endow it with visual capacity by the meticulously designed (i) visual receptor, (ii) input-output interface, (iii) 3-stage training pipeline, and (iv) multilingual multimodal cleaned corpus. Beyond the conventional image description and question-answering, we implement the grounding and text-reading ability of Qwen-VLs by aligning image-caption-box tuples. The resulting models, including Qwen-VL and Qwen-VL-Chat, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks (e.g., image captioning, question answering, visual grounding) and different settings (e.g., zero-shot, few-shot). Moreover, on real-world dialog benchmarks, our instruction-tuned Qwen-VL-Chat also demonstrates superiority compared to existing vision-language chatbots. All models are public to facilitate future research. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond\" satisfies the user query because it explicitly mentions the expansion of multimodal systems to include text-reading capabilities, which is a core aspect of OCR (Optical Character Recognition). The abstract highlights that the Qwen-VL models implement text-reading abilities by aligning image-caption-box tuples, which is a method relevant to OCR tasks. Thus, the paper aligns with the user's query about studies that expanded multimodal systems to include OCR functionality."}
{"input": "User Query: Which papers discuss about the development of large language models (LLMs)?\nTitle: LLaMA: Open and Efficient Foundation Language Models\nAbstract:  Abstract We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community111https://github.com/facebookresearch/llama. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"LLaMA: Open and Efficient Foundation Language Models\" satisfies the user query as it discusses the development of large language models (LLMs). The abstract explicitly mentions the introduction of LLaMA, a collection of foundation language models with varying sizes (7B to 65B parameters). It details the training process, including the use of publicly available datasets and the performance of these models compared to other state-of-the-art LLMs like GPT-3, Chinchilla, and PaLM. This aligns with the user query, which seeks papers discussing the development of LLMs."}
{"input": "User Query: What are some of the novel techniques used for face recognition tasks?\nTitle: MagFace: A Universal Representation for Face Recognition and Quality Assessment\nAbstract:  Abstract The performance of face recognition system degrades when the variability of the acquired faces increases. Prior work alleviates this issue by either monitoring the face quality in pre-processing or predicting the data uncertainty along with the face feature. This paper proposes MagFace, a category of losses that learn a universal feature embedding whose magnitude can measure the quality of the given face. Under the new loss, it can be proven that the magnitude of the feature embedding monotonically increases if the subject is more likely to be recognized. In addition, MagFace introduces an adaptive mechanism to learn a well-structured within-class feature distributions by pulling easy samples to class centers while pushing hard samples away. This prevents models from overfitting on noisy low-quality samples and improves face recognition in the wild. Extensive experiments conducted on face recognition, quality assessments as well as clustering demonstrate its superiority over state-of-the-arts. The code is available at https://github.com/IrvingMeng/MagFace. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"MagFace: A Universal Representation for Face Recognition and Quality Assessment\" satisfies the user query as it introduces a novel technique for face recognition tasks. Specifically, the paper proposes a new category of loss functions, referred to as MagFace, which learns a universal feature embedding. This embedding not only improves face recognition performance but also incorporates a mechanism to assess face quality. The novelty lies in the adaptive mechanism that structures within-class feature distributions by balancing easy and hard samples, thereby addressing challenges like noisy and low-quality data. Additionally, the paper demonstrates the effectiveness of MagFace through extensive experiments, highlighting its superiority over state-of-the-art methods. This aligns with the user's request for novel techniques in face recognition tasks."}
{"input": "User Query: What studies are about using large language models to build generic text quality scorers?\nTitle: GPTScore: Evaluate as You Desire\nAbstract:  Abstract Generative Artificial Intelligence (AI) has enabled the development of sophisticated models that are capable of producing high-caliber text, images, and other outputs through the utilization of large pre-trained models. Nevertheless, assessing the quality of the generation is an even more arduous task than the generation itself, and this issue has not been given adequate consideration recently. This paper proposes a novel evaluation framework, GPTScore, which utilizes the emergent abilities (e.g., zero-shot instruction) of generative pre-trained models to score generated texts. There are 19 pre-trained models explored in this paper, ranging in size from 80M (e.g., FLAN-T5-small) to 175B (e.g., GPT3). Experimental results on four text generation tasks, 222222 evaluation aspects, and corresponding 373737 datasets demonstrate that this approach can effectively allow us to achieve what one desires to evaluate for texts simply by natural language instructions. This nature helps us overcome several long-standing challenges in text evaluation–how to achieve customized, multi-faceted evaluation without the need for annotated samples. We make our code publicly available. 111https://github.com/jinlanfu/GPTScore ", "output": "**Decision:** True  \n**Reason:** The paper titled \"GPTScore: Evaluate as You Desire\" aligns with the user query as it discusses using large language models to build a generic text quality scorer. The proposed framework, GPTScore, leverages the emergent abilities of generative pre-trained models to evaluate text quality based on natural language instructions. This approach addresses the challenge of assessing text quality in a customizable and multi-faceted manner without requiring annotated samples, which directly relates to the concept of building a generic text quality scorer using large language models. The experimental results and the exploration of 19 pre-trained models further support the relevance of this study to the user query."}
{"input": "User Query: Can you mention some works that show state-of-the-art performance in the field of image synthesis due to the progress in DMs?\nTitle: Denoising Diffusion Implicit Models\nAbstract:  Abstract Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples 10×10\\times to 50×50\\times faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error. ", "output": "**Decision:** True  \n**Reason:** The searched paper, \"Denoising Diffusion Implicit Models,\" satisfies the user query as it presents a significant advancement in the field of image synthesis using diffusion models (DMs). The paper introduces DDIMs, which build upon DDPMs, a state-of-the-art approach in image synthesis. DDIMs improve efficiency by accelerating the sampling process while maintaining high-quality image generation. The work empirically demonstrates superior performance in terms of speed and quality, which aligns with the user's request for works showcasing state-of-the-art performance in image synthesis due to progress in diffusion models."}
{"input": "User Query: Which research works explores the connection between multicalibration and minimization of convex losses through the notion of omniprediction?\nTitle: Omnipredictors\nAbstract:  Abstract Loss minimization is a dominant paradigm in machine learning, where a predictor is trained to minimize some loss function that depends on an uncertain event (e.g., “will it rain tomorrow?”). Different loss functions imply different learning algorithms and, at times, very different predictors. While widespread and appealing, a clear drawback of this approach is that the loss function may not be known at the time of learning, requiring the algorithm to use a best-guess loss function. Alternatively, the same classifier may be used to inform multiple decisions, which correspond to multiple loss functions, requiring multiple learning algorithms to be run on the same data. We suggest a rigorous new paradigm for loss minimization in machine learning where the loss function can be ignored at the time of learning and only be taken into account when deciding an action. We introduce the notion of an (ℒ,𝒞ℒ𝒞{\\cal L},\\mathcal{C})-omnipredictor, which could be used to optimize any loss in a family ℒℒ{\\cal L}. Once the loss function is set, the outputs of the predictor can be post-processed (a simple univariate data-independent transformation of individual predictions) to do well compared with any hypothesis from the class 𝒞𝒞\\cal C. The post processing is essentially what one would perform if the outputs of the predictor were true probabilities of the uncertain events. In a sense, omnipredictors extract all the predictive power from the class 𝒞𝒞\\mathcal{C}, irrespective of the loss function in ℒℒ\\cal L. We show that such “loss-oblivious” learning is feasible through a connection to multicalibration, a notion introduced in the context of algorithmic fairness. A multicalibrated predictor doesn’t aim to minimize some loss function, but rather to make calibrated predictions, even when conditioned on inputs lying in certain sets c𝑐c belonging to a family 𝒞𝒞\\mathcal{C} which is weakly learnable. We show that a 𝒞𝒞\\mathcal{C}-multicalibrated predictor is also an (ℒ,𝒞ℒ𝒞{\\cal L},\\mathcal{C})-omnipredictor, where ℒℒ\\cal L contains all convex loss functions with some mild Lipschitz conditions. The predictors are even omnipredictors with respect to sparse linear combinations of functions in 𝒞𝒞\\mathcal{C}. As a corollary, we deduce that distribution-specific weak agnostic learning is complete for a large class of loss minimization tasks. In addition, we show how multicalibration can be viewed as a solution concept for agnostic boosting, shedding new light on past results. Finally, we transfer our insights back to the context of algorithmic fairness by providing omnipredictors for multi-group loss minimization. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Omnipredictors\" directly explores the connection between multicalibration and the minimization of convex losses through the notion of omniprediction. The abstract explicitly states that the authors introduce the concept of omnipredictors, which are predictors capable of optimizing any loss function in a family of convex losses (with mild Lipschitz conditions) through a post-processing step. Furthermore, the paper establishes that such omnipredictors can be constructed using multicalibration, a concept originally introduced in the context of algorithmic fairness. The abstract also highlights that a multicalibrated predictor is inherently an omnipredictor for convex loss functions, thereby directly addressing the connection between multicalibration and convex loss minimization through omniprediction. This satisfies the detailed requirements of the user query."}
{"input": "User Query: Are there any methods of creating new ways of land-cover mapping by saving labor with the Segment Anything Model (SAM)?\nTitle: The Segment Anything Model (SAM) for Remote Sensing Applications: From Zero to One Shot\nAbstract:  Abstract Segmentation is an essential step for remote sensing image processing. This study aims to advance the application of the Segment Anything Model (SAM), an innovative image segmentation model by Meta AI, in the field of remote sensing image analysis. SAM is known for its exceptional generalization capabilities and zero-shot learning, making it a promising approach to processing aerial and orbital images from diverse geographical contexts. Our exploration involved testing SAM across multi-scale datasets using various input prompts, such as bounding boxes, individual points, and text descriptors. To enhance the model’s performance, we implemented a novel automated technique that combines a text-prompt-derived general example with one-shot training. This adjustment resulted in an improvement in accuracy, underscoring SAM’s potential for deployment in remote sensing imagery and reducing the need for manual annotation. Despite the limitations, encountered with lower spatial resolution images, SAM exhibits promising adaptability to remote sensing data analysis. We recommend future research to enhance the model’s proficiency through integration with supplementary fine-tuning techniques and other networks. Furthermore, we provide the open-source code of our modifications on online repositories, encouraging further and broader adaptations of SAM to the remote sensing domain. ", "output": "**Decision:** True  \n**Reason:** The paper satisfies the user query by addressing the use of the Segment Anything Model (SAM) in remote sensing applications, specifically focusing on reducing labor through innovative methods. The study explores SAM's zero-shot learning capabilities and its application to remote sensing imagery, which is directly relevant to land-cover mapping. Additionally, the paper describes a novel automated technique combining text-prompt-derived examples with one-shot training to enhance SAM's performance, thereby reducing the need for manual annotation. This aligns with the user's interest in creating new methods for land-cover mapping while saving labor. The open-source code provided further supports the development of new approaches in this domain."}
{"input": "User Query: What studies explored semi-supervised learning with techniques like the prediction of the relative position of two regions, or the impainting of a masked area?\nTitle: Context Encoders: Feature Learning by Inpainting\nAbstract:  Abstract We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders – a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Context Encoders: Feature Learning by Inpainting\" satisfies the requirements of the user query. The query specifically asks for studies that explore semi-supervised learning techniques such as the prediction of the relative position of two regions or the inpainting of a masked area. The abstract of the paper clearly describes a method for unsupervised visual feature learning using context-based pixel prediction, where a convolutional neural network is trained to generate the contents of a masked image region conditioned on its surroundings. This directly aligns with the concept of inpainting a masked area. Additionally, the method involves learning representations that capture both appearance and semantics, which is relevant to semi-supervised learning. Although the paper does not explicitly mention \"prediction of the relative position of two regions,\" the inpainting task inherently requires an understanding of spatial context, which is conceptually related. Therefore, the paper meets the requirements of the query."}
{"input": "User Query: What papers have developed expressive statistical models to represent details beyond major human body?\nTitle: Collaborative Regression of Expressive Bodies using Moderation\nAbstract:  Abstract Recovering expressive humans from images is essential for understanding human behavior. Methods that estimate 3D bodies, faces, or hands have progressed significantly, yet separately. Face methods recover accurate 3D shape and geometric details, but need a tight crop and struggle with extreme views and low resolution. Whole-body methods are robust to a wide range of poses and resolutions, but provide only a rough 3D face shape without details like wrinkles. To get the best of both worlds, we introduce PIXIE, which produces animatable, whole-body 3D avatars with realistic facial detail, from a single image. For this, PIXIE uses two key observations. First, existing work combines independent estimates from body, face, and hand experts, by trusting them equally. PIXIE introduces a novel moderator that merges the features of the experts, weighted by their confidence.  All part experts can contribute to the whole, using SMPL-X’s shared shape space across all body parts. Second, human shape is highly correlated with gender, but existing work ignores this. We label training images as male, female, or non-binary, and train PIXIE to infer “gendered” 3D body shapes with a novel shape loss. In addition to 3D body pose and shape parameters, PIXIE estimates expression, illumination, albedo and 3D facial surface displacements. Quantitative and qualitative evaluation shows that PIXIE estimates  more accurate whole-body shape and detailed face shape than the state of the art.  Models and code are available at pixie.is.tue.mpg.de. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Collaborative Regression of Expressive Bodies using Moderation\" satisfies the user query as it explicitly focuses on developing an expressive statistical model, PIXIE, that represents details beyond the major human body. The paper highlights its ability to recover detailed 3D representations of the face (including wrinkles and other fine details), hands, and the whole body. It also incorporates additional expressive parameters such as expression, illumination, albedo, and 3D facial surface displacements. These features go beyond the major human body and address the user's requirement for expressive statistical models that capture detailed aspects of human representation."}
{"input": "User Query: Which studies use deep generative models for vector font generation?\nTitle: DeepVecFont: Synthesizing High-quality Vector Fonts via Dual-modality Learning\nAbstract:  Abstract. Automatic font generation based on deep learning has aroused a lot of interest in the last decade. However, only a few recently-reported approaches are capable of directly generating vector glyphs and their results are still far from satisfactory. In this paper, we propose a novel method, DeepVecFont, to effectively resolve this problem. Using our method, for the first time, visually-pleasing vector glyphs whose quality and compactness are both comparable to human-designed ones can be automatically generated. The key idea of our DeepVecFont is to adopt the techniques of image synthesis, sequence modeling and differentiable rasterization to exhaustively exploit the dual-modality information (i.e., raster images and vector outlines) of vector fonts. The highlights of this paper are threefold. First, we design a dual-modality learning strategy which utilizes both image-aspect and sequence-aspect features of fonts to synthesize vector glyphs. Second, we provide a new generative paradigm to handle unstructured data (e.g., vector glyphs) by randomly sampling plausible synthesis results to get the optimal one which is further refined under the guidance of generated structured data (e.g., glyph images). Finally, qualitative and quantitative experiments conducted on a publicly-available dataset demonstrate that our method obtains high-quality synthesis results in the applications of vector font generation and interpolation, significantly outperforming the state of the art.  ", "output": "**Decision:** True  \n**Reason:** The paper titled \"DeepVecFont: Synthesizing High-quality Vector Fonts via Dual-modality Learning\" fully satisfies the user query. The abstract explicitly states that the study focuses on automatic font generation using deep learning techniques, specifically targeting vector font generation. The method, DeepVecFont, employs a novel generative approach combining image synthesis, sequence modeling, and differentiable rasterization to generate high-quality vector glyphs. This aligns with the query's focus on studies using deep generative models for vector font generation."}
{"input": "User Query: What papers proposed methods to reduce catastrophic forgetting in continual learning?\nTitle: Overcoming Catastrophic Forgetting by Incremental Moment Matching\nAbstract:  Abstract Catastrophic forgetting is a problem of neural networks that loses the information of the first task after training the second task. Here, we propose a method, i.e. incremental moment matching (IMM), to resolve this problem. IMM incrementally matches the moment of the posterior distribution of the neural network which is trained on the first and the second task, respectively. To make the search space of posterior parameter smooth, the IMM procedure is complemented by various transfer learning techniques including weight transfer, L2-norm of the old and the new parameter, and a variant of dropout with the old parameter. We analyze our approach on a variety of datasets including the MNIST, CIFAR-10, Caltech-UCSD-Birds, and Lifelog datasets. The experimental results show that IMM achieves state-of-the-art performance by balancing the information between an old and a new network. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Overcoming Catastrophic Forgetting by Incremental Moment Matching\" directly addresses the problem of catastrophic forgetting in neural networks, which is central to the user query. The proposed method, Incremental Moment Matching (IMM), is specifically designed to mitigate catastrophic forgetting by incrementally matching the moments of the posterior distribution of the neural network trained on different tasks. The paper also discusses complementary techniques such as weight transfer, L2-norm regularization, and dropout variants to enhance the method. Additionally, the experimental validation on various datasets demonstrates its effectiveness in continual learning scenarios. Therefore, the paper fully satisfies the requirements of the user query."}
{"input": "User Query: Which studies have been made to measure privacy leakage in Large-Language Models (LLMs)?\nTitle: Quantifying Memorization Across Neural Language Models\nAbstract:  Abstract Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others). We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes more complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"Quantifying Memorization Across Neural Language Models\"* satisfies the user query as it directly addresses the measurement of privacy leakage in large language models (LLMs). The paper explicitly discusses how LLMs memorize parts of their training data and emit them verbatim when prompted, which is a key aspect of privacy leakage. Furthermore, it quantifies memorization through log-linear relationships and explores factors such as model capacity, data duplication, and prompt length that influence memorization. These findings are directly relevant to understanding and measuring privacy leakage in LLMs. Therefore, the paper aligns well with the user's query."}
{"input": "User Query: Can you provide examples of research papers where knowledge distillation is used to transfer knowledge from larger models to smaller models?\nTitle: Distilling the Knowledge in a Neural Network\nAbstract:  Abstract A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions [3]. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators [1] have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel. ", "output": "**Decision:** True  \n**Reason:** The searched paper, \"Distilling the Knowledge in a Neural Network,\" directly addresses the concept of knowledge distillation, which is the process of transferring knowledge from larger models (or ensembles of models) to smaller models. The abstract explicitly mentions compressing the knowledge in an ensemble of models into a single, smaller model that is easier to deploy. This aligns with the user query, as the paper provides an example of using knowledge distillation to transfer knowledge from larger models to smaller ones. The methodology and results discussed in the abstract further confirm that the paper satisfies the requirements of the query."}
{"input": "User Query: What works provide inferential methods which are valid under data-dependent stopping times?\nTitle: Time-uniform, nonparametric, nonasymptotic confidence sequences\nAbstract: A confidence sequence is a sequence of confidence intervals that is uniformly valid over an unbounded time horizon. Our work develops confidence sequences whose widths go to zero, with nonasymptotic coverage guarantees under nonparametric conditions. We draw connections between the Cram\\'er-Chernoff method for exponential concentration, the law of the iterated logarithm (LIL), and the sequential probability ratio test -- our confidence sequences are time-uniform extensions of the first; provide tight, nonasymptotic characterizations of the second; and generalize the third to nonparametric settings, including sub-Gaussian and Bernstein conditions, self-normalized processes, and matrix martingales. We illustrate the generality of our proof techniques by deriving an empirical-Bernstein bound growing at a LIL rate, as well as a novel upper LIL for the maximum eigenvalue of a sum of random matrices. Finally, we apply our methods to covariance matrix estimation and to estimation of sample average treatment effect under the Neyman-Rubin potential outcomes model.", "output": "**Decision:** True  \n**Reason:** The paper titled \"Time-uniform, nonparametric, nonasymptotic confidence sequences\" satisfies the requirements of the user query. The user is asking for works that provide inferential methods valid under data-dependent stopping times. The abstract of the paper explicitly mentions the development of \"confidence sequences,\" which are sequences of confidence intervals that are uniformly valid over an unbounded time horizon. This time-uniform validity directly addresses the concern of data-dependent stopping times, as confidence sequences are designed to maintain their validity regardless of when the data collection or analysis is stopped. Furthermore, the paper emphasizes nonasymptotic coverage guarantees, which ensure that the inferential methods remain valid without relying on large-sample approximations. The connection to sequential methods, such as the sequential probability ratio test, further supports the relevance of the paper to the query. Therefore, the paper fully satisfies the requirements of the user query."}
{"input": "User Query: What research is closely similar to ours in the concept of learning to combine the probability distributions of multiple language models?\nTitle: CombLM: Adapting Black-Box Language Models through Small Fine-Tuned Models\nAbstract:  Abstract Methods for adapting language models (LMs) to new tasks and domains have traditionally assumed white-box access to the model, and work by modifying its parameters. However, this is incompatible with a recent trend in the field, where the highest quality models are only available as black-boxes through inference APIs. Even when the model weights are available, the computational cost of fine-tuning large LMs can be prohibitive for most practitioners. In this work, we present a lightweight method for adapting large LMs to new domains and tasks, assuming no access to their weights or intermediate activations. Our approach fine-tunes a small white-box LM and combines it with the large black-box LM at the probability level through a small network, learned on a small validation set. We validate our approach by adapting a large LM (OPT-30B) to several domains and a downstream task (machine translation), observing improved performance in all cases, of up to 9%, while using a domain expert 23x smaller. ", "output": "**Decision:** True  \n**Reason:** The paper \"CombLM: Adapting Black-Box Language Models through Small Fine-Tuned Models\" aligns closely with the concept described in the user query. The paper explicitly discusses a method for combining the probability distributions of multiple language models (a large black-box LM and a smaller fine-tuned white-box LM) at the probability level. This directly relates to the idea of learning to combine probability distributions from multiple language models, as the approach involves a small network that learns how to integrate the outputs of the two models. This satisfies the requirements of the user query."}
{"input": "User Query: Which datasets are examples of 3D synthetic dataset that consist of category-level objects?\nTitle: Deep3DShape: 3D ShapeNetswork Joint 3D Recognition and Volume Completion\nAbstract:  Abstract 3D shape is a crucial but heavily underutilized cue in today’s computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5D depth sensors (e.g. Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop. Apart from category recognition, recovering full 3D shapes from view-based 2.5D depth maps is also a critical part of visual understanding. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model, 3D ShapeNets, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representations automatically. It naturally supports joint object recognition and shape completion from 2.5D depth maps, and it enables active object recognition through view planning. To train our 3D deep learning model, we construct ModelNet – a large-scale 3D CAD model dataset. Extensive experiments show that our 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks. †††This work was done when Zhirong Wu was a VSRC visiting student at Princeton University. ", "output": "**Decision:** True  \n**Reason:** The paper introduces **ModelNet**, a large-scale 3D CAD model dataset, which is explicitly mentioned as being constructed from raw CAD data. CAD models are synthetic in nature, and the dataset includes objects across different categories, fulfilling the requirement of a 3D synthetic dataset consisting of category-level objects. Therefore, the paper satisfies the detailed requirements of the user query."}
{"input": "User Query: Which research papers have proposed using 3D LUTs for image enhancement?\nTitle: SepLUT: Separable Image-adaptive Lookup Tables for Real-time Image Enhancement\nAbstract:  Abstract Image-adaptive lookup tables (LUTs) have achieved great success in real-time image enhancement tasks due to their high efficiency for modeling color transforms. However, they embed the complete transform, including the color component-independent and the component-correlated parts, into only a single type of LUTs, either 1D or 3D, in a coupled manner. This scheme raises a dilemma of improving model expressiveness or efficiency due to two factors. On the one hand, the 1D LUTs provide high computational efficiency but lack the critical capability of color components interaction. On the other, the 3D LUTs present enhanced component-correlated transform capability but suffer from heavy memory footprint, high training difficulty, and limited cell utilization. Inspired by the conventional divide-and-conquer practice in the image signal processor, we present SepLUT (separable image-adaptive lookup table) to tackle the above limitations. Specifically, we separate a single color transform into a cascade of component-independent and component-correlated sub-transforms instantiated as 1D and 3D LUTs, respectively. In this way, the capabilities of two sub-transforms can facilitate each other, where the 3D LUT complements the ability to mix up color components, and the 1D LUT redistributes the input colors to increase the cell utilization of the 3D LUT and thus enable the use of a more lightweight 3D LUT. Experiments demonstrate that the proposed method presents enhanced performance on photo retouching benchmark datasets than the current state-of-the-art and achieves real-time processing on both GPUs and CPUs. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"SepLUT: Separable Image-adaptive Lookup Tables for Real-time Image Enhancement\" explicitly discusses the use of 3D LUTs (Lookup Tables) for image enhancement. The abstract mentions that the proposed method, SepLUT, incorporates 3D LUTs as part of a separable image-adaptive lookup table framework. Specifically, the 3D LUTs are used to handle component-correlated color transforms, which are crucial for enhancing the interaction between color components. This directly aligns with the user query, as the paper proposes a method that utilizes 3D LUTs for image enhancement tasks."}
{"input": "User Query: Which works used AI algorithms trained on egocentric images from head-mounted cameras of human babies and children?\nTitle: Self-supervised learning through the eyes of a child\nAbstract:  Abstract Within months of birth, children develop meaningful expectations about the world around them. How much of this early knowledge can be explained through generic learning mechanisms applied to sensory data, and how much of it requires more substantive innate inductive biases? Addressing this fundamental question in its full generality is currently infeasible, but we can hope to make real progress in more narrowly defined domains, such as the development of high-level visual categories, thanks to improvements in data collecting technology and recent progress in deep learning. In this paper, our goal is precisely to achieve such progress by utilizing modern self-supervised deep learning methods and a recent longitudinal, egocentric video dataset recorded from the perspective of three young children (Sullivan et al., 2020). Our results demonstrate the emergence of powerful, high-level visual representations from developmentally realistic natural videos using generic self-supervised learning objectives. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Self-supervised learning through the eyes of a child\" satisfies the requirements of the user query. The abstract explicitly states that the research utilizes a longitudinal, egocentric video dataset recorded from the perspective of young children. This dataset was collected using head-mounted cameras, as indicated by the egocentric nature of the videos. Additionally, the paper employs self-supervised learning methods, which are AI algorithms, to train on this data. Therefore, the work aligns with the query's focus on AI algorithms trained on egocentric images from head-mounted cameras of human babies and children."}
{"input": "User Query: Which research covered Semantic Scene Completion using RGB images?\nTitle: MonoScene: Monocular 3D Semantic Scene Completion\nAbstract:  Abstract MonoScene proposes a 3D Semantic Scene Completion (SSC) framework, where the dense geometry and semantics of a scene are inferred from a single monocular RGB image. Different from the SSC literature, relying on 2.5 or 3D input, we solve the complex problem of 2D to 3D scene reconstruction while jointly inferring its semantics. Our framework relies on successive 2D and 3D UNets, bridged by a novel 2D-3D features projection inspired by optics, and introduces a 3D context relation prior to enforce spatio-semantic consistency. Along with architectural contributions, we introduce novel global scene and local frustums losses. Experiments show we outperform the literature on all metrics and datasets while hallucinating plausible scenery even beyond the camera field of view. Our code and trained models are available at https://github.com/cv-rits/MonoScene. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"MonoScene: Monocular 3D Semantic Scene Completion\" fully satisfies the user query. The research explicitly focuses on Semantic Scene Completion (SSC) using RGB images, as stated in the abstract. It proposes a framework for inferring dense geometry and semantics of a scene from a single monocular RGB image. This directly aligns with the user's interest in research that covers SSC using RGB images. Additionally, the paper highlights its unique approach of solving the 2D to 3D scene reconstruction problem and jointly inferring semantics, which further confirms its relevance to the query."}
{"input": "User Query: What research involves saliency methods specifically for time series?\nTitle: What went wrong and when? Instance-wise feature importance for time-series black-box models\nAbstract:  Abstract Explanations of time series models are useful for high stakes applications like healthcare but have received little attention in machine learning literature. We propose FIT, a framework that evaluates the importance of observations for a multivariate time-series black-box model by quantifying the shift in the predictive distribution over time. FIT defines the importance of an observation based on its contribution to the distributional shift under a KL-divergence that contrasts the predictive distribution against a counterfactual where the rest of the features are unobserved. We also demonstrate the need to control for time-dependent distribution shifts. We compare with state-of-the-art baselines on simulated and real-world clinical data and demonstrate that our approach is superior in identifying important time points and observations throughout the time series. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"What went wrong and when? Instance-wise feature importance for time-series black-box models\"* satisfies the user query as it directly involves saliency methods specifically for time series. The proposed framework, FIT, evaluates the importance of observations in a multivariate time-series model, which aligns with the concept of saliency methods—identifying and quantifying the importance of specific features or time points. The paper explicitly focuses on explaining time-series models by determining the significance of observations over time, which is a core aspect of saliency methods applied to time series. Furthermore, the paper demonstrates its approach on both simulated and real-world clinical data, reinforcing its relevance to the query."}
{"input": "User Query: Which works contributed to the development of image diffusion models?\nTitle: Denoising Diffusion Probabilistic Models\nAbstract:  Abstract We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion. ", "output": "**Decision:** True  \n**Reason:** The paper \"Denoising Diffusion Probabilistic Models\" directly contributes to the development of image diffusion models. It introduces a novel framework for generating high-quality images using diffusion probabilistic models, which are a foundational class of models in the field of image diffusion. The paper establishes a connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, which is a significant theoretical and practical advancement. Additionally, the paper demonstrates state-of-the-art results on image synthesis benchmarks, such as CIFAR-10 and LSUN, further solidifying its contribution to the development of image diffusion models. Therefore, it fully satisfies the requirements of the user query."}
{"input": "User Query: Could you provide me some studies about reweighting approaches based on bilevel optimization that encounters issues similar to IW and RuLSIF?\nTitle: Learning to Reweight Examples for Robust Deep Learning\nAbstract:  Abstract Deep neural networks have been shown to be very powerful modeling tools for many supervised learning tasks involving complex input patterns. However, they can also easily overfit to training set biases and label noises. In addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they require careful tuning of additional hyperparameters, such as example mining schedules and regularization hyperparameters. In contrast to past reweighting methods, which typically consist of functions of the cost value of each example, in this work we propose a novel meta-learning algorithm that learns to assign weights to training examples based on their gradient directions. To determine the example weights, our method performs a meta gradient descent step on the current mini-batch example weights (which are initialized from zero) to minimize the loss on a clean unbiased validation set. Our proposed method can be easily implemented on any type of deep network, does not require any additional hyperparameter tuning, and achieves impressive performance on class imbalance and corrupted label problems where only a small amount of clean validation data is available. ", "output": "**Decision:** True  \n**Reason:** The searched paper, \"Learning to Reweight Examples for Robust Deep Learning,\" satisfies the user query as it directly addresses reweighting approaches based on bilevel optimization. The paper proposes a meta-learning algorithm that assigns weights to training examples by performing a meta gradient descent step, which is a bilevel optimization approach. This approach is designed to address issues such as biases and label noise in training data, which are challenges similar to those encountered in Importance Weighting (IW) and Relative Unconstrained Least-Squares Importance Fitting (RuLSIF). Additionally, the paper focuses on robust learning and reweighting strategies, which align with the user's request for studies in this domain."}
{"input": "User Query: Which works focus on applying GANs to class-conditioned image generation?\nTitle: Large Scale GAN Training for High Fidelity Natural Image Synthesis\nAbstract:  Abstract Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple “truncation trick,” allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator’s input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128×\\times128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Fréchet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.65. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Large Scale GAN Training for High Fidelity Natural Image Synthesis\" explicitly mentions that it focuses on class-conditional image synthesis. The abstract highlights that the authors train GANs at a large scale and achieve state-of-the-art results in class-conditioned image generation, particularly on the ImageNet dataset. This directly aligns with the user query, which seeks works applying GANs to class-conditioned image generation. Therefore, the paper fully satisfies the requirements of the query."}
{"input": "User Query: Which works directly establish correspondence between a pair of point clouds in non-rigid point cloud matching?\nTitle: DPC: Unsupervised Deep Point Correspondence via Cross and Self Construction\nAbstract:  Abstract We present a new method for real-time non-rigid dense correspondence between point clouds based on structured shape construction. Our method, termed Deep Point Correspondence (DPC), requires a fraction of the training data compared to previous techniques and presents better generalization capabilities. Until now, two main approaches have been suggested for the dense correspondence problem. The first is a spectral-based approach that obtains great results on synthetic datasets but requires mesh connectivity of the shapes and long inference processing time while being unstable in real-world scenarios. The second is a spatial approach that uses an encoder-decoder framework to regress an ordered point cloud for the matching alignment from an irregular input. Unfortunately, the decoder brings considerable disadvantages, as it requires a large amount of training data and struggles to generalize well in cross-dataset evaluations. DPC’s novelty lies in its lack of a decoder component. Instead, we use latent similarity and the input coordinates themselves to construct the point cloud and determine correspondence, replacing the coordinate regression done by the decoder. Extensive experiments show that our construction scheme leads to a performance boost in comparison to recent state-of-the-art correspondence methods. Our code is publicly available111https://github.com/dvirginz/DPC *Equal contribution. ", "output": "**Decision:** True  \n**Reason:** The paper \"DPC: Unsupervised Deep Point Correspondence via Cross and Self Construction\" directly addresses the problem of establishing correspondence between a pair of point clouds in non-rigid point cloud matching. The abstract explicitly states that the method, termed Deep Point Correspondence (DPC), is designed for \"real-time non-rigid dense correspondence between point clouds.\" The approach constructs the point cloud and determines correspondence using latent similarity and input coordinates, which directly aligns with the user's query about works that establish correspondence between point clouds. Furthermore, the paper focuses on non-rigid matching, as indicated by its emphasis on dense correspondence and generalization across datasets, making it relevant to the query."}
{"input": "User Query: What researches applied representation interpolation methods to NLP tasks?\nTitle: MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification\nAbstract:  Abstract This paper presents MixText, a semi-supervised learning method for text classification, which uses our newly designed data augmentation method called TMix. TMix creates a large amount of augmented training samples by interpolating text in hidden space. Moreover, we leverage recent advances in data augmentation to guess low-entropy labels for unlabeled data, hence making them as easy to use as labeled data. By mixing labeled, unlabeled and augmented data, MixText significantly outperformed current pre-trained and fined-tuned models and other state-of-the-art semi-supervised learning methods on several text classification benchmarks. The improvement is especially prominent when supervision is extremely limited. We have publicly released our code at https://github.com/GT-SALT/MixText. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification\" satisfies the user query. The query specifically asks for research that applies representation interpolation methods to NLP tasks. This paper introduces a method called TMix, which explicitly involves interpolating text representations in the hidden space as a data augmentation technique. This interpolation is directly applied to the NLP task of text classification, making it relevant to the query. The abstract also highlights the use of this method in semi-supervised learning, further supporting its application in NLP tasks."}
{"input": "User Query: Any works about transforming-based models for temporal action segmentation?\nTitle: ASFormer: Transformer for Action Segmentation\nAbstract:  Abstract Algorithms for the action segmentation task typically use temporal models to predict what action is occurring at each frame for a minute-long daily activity. Recent studies have shown the potential of Transformer in modeling the relations among elements in sequential data. However, there are several major concerns when directly applying the Transformer to the action segmentation task, such as the lack of inductive biases with small training sets, the deficit in processing long input sequence, and the limitation of the decoder architecture to utilize temporal relations among multiple action segments to refine the initial predictions. To address these concerns, we design an efficient Transformer-based model for action segmentation task, named ASFormer, with three distinctive characteristics: (i) We explicitly bring in the local connectivity inductive priors because of the high locality of features. It constrains the hypothesis space within a reliable scope, and is beneficial for the action segmentation task to learn a proper target function with small training sets. (ii) We apply a pre-defined hierarchical representation pattern that efficiently handles long input sequences. (iii) We carefully design the decoder to refine the initial predictions from the encoder. Extensive experiments on three public datasets demonstrate that effectiveness of our methods. Code is available at https://github.com/ChinaYi/ASFormer. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"ASFormer: Transformer for Action Segmentation\" directly addresses the user query. It focuses on a Transformer-based model specifically designed for the task of temporal action segmentation. The abstract highlights the challenges of applying Transformers to this task and presents solutions tailored to temporal action segmentation, such as incorporating local connectivity inductive priors, handling long input sequences with a hierarchical representation, and refining predictions using a carefully designed decoder. These aspects clearly align with the user's interest in Transformer-based models for temporal action segmentation."}
{"input": "User Query: Which papers originally introduced and further advanced Denoising diffusion probabilistic models?\nTitle: Denoising Diffusion Probabilistic Models\nAbstract:  Abstract We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Denoising Diffusion Probabilistic Models\" fully satisfies the user query. This paper is the original work that introduced denoising diffusion probabilistic models (DDPMs). It presents the foundational concepts, methodology, and results for this class of models, establishing their connection to nonequilibrium thermodynamics and denoising score matching. The paper also demonstrates the effectiveness of DDPMs in high-quality image synthesis, providing state-of-the-art results on benchmark datasets. Since the query specifically seeks papers that originally introduced and advanced DDPMs, this paper directly fulfills the requirements as it is the seminal work in the field."}
{"input": "User Query: Which papers discuss using pre-trained models for data augmentation in the field of vision and language?\nTitle: Going Beyond Nouns With Vision & Language Models Using Synthetic Data\nAbstract:  Abstract Large-scale pre-trained Vision & Language (VL) models have shown remarkable performance in many applications, enabling replacing a fixed set of supported classes with zero-shot open vocabulary reasoning over (almost arbitrary) natural language prompts. However, recent works have uncovered a fundamental weakness of these models. For example, their difficulty to understand Visual Language Concepts (VLC) that go ‘beyond nouns’ such as the meaning of non-object words (e.g., attributes, actions, relations, states, etc.), or difficulty in performing compositional reasoning such as understanding the significance of the order of the words in a sentence. In this work, we investigate to which extent purely synthetic data could be leveraged to teach these models to overcome such shortcomings without compromising their zero-shot capabilities. We contribute Synthetic Visual Concepts (SyViC) - a million-scale synthetic dataset and data generation codebase allowing to generate additional suitable data to improve VLC understanding and compositional reasoning of VL models. Additionally, we propose a general VL finetuning strategy for effectively leveraging SyViC towards achieving these improvements. Our extensive experiments and ablations on VL-Checklist, Winoground, and ARO benchmarks demonstrate that it is possible to adapt strong pre-trained VL models with synthetic data significantly enhancing their VLC understanding (e.g. by 9.9% on ARO and 4.3% on VL-Checklist) with under 1%percent11\\% drop in their zero-shot accuracy. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Going Beyond Nouns With Vision & Language Models Using Synthetic Data\" satisfies the user query as it discusses using pre-trained Vision & Language (VL) models for data augmentation in the field of vision and language. Specifically, the paper investigates how synthetic data (generated through their Synthetic Visual Concepts (SyViC) dataset) can be used to improve the understanding and reasoning capabilities of pre-trained VL models. This aligns with the query's focus on leveraging pre-trained models for data augmentation, as the synthetic data is used to enhance the performance of these models in tasks related to vision and language."}
{"input": "User Query: What research projects focused on dataset learning for unsupervised optical flow?\nTitle: Self-supervised AutoFlow\nAbstract:  Abstract Recently, AutoFlow has shown promising results on learning a training set for optical flow, but requires ground truth labels in the target domain to compute its search metric. Observing a strong correlation between the ground truth search metric and self-supervised losses, we introduce self-supervised AutoFlow to handle real-world videos without ground truth labels. Using self-supervised loss as the search metric, our self-supervised AutoFlow performs on par with AutoFlow on Sintel and KITTI where ground truth is available, and performs better on the real-world DAVIS dataset. We further explore using self-supervised AutoFlow in the (semi-)supervised setting and obtain competitive results against the state of the art. ", "output": "**Decision:** True  \n**Reason:** The paper \"Self-supervised AutoFlow\" satisfies the user query as it focuses on dataset learning for unsupervised optical flow. The abstract explicitly mentions the development of a self-supervised version of AutoFlow, which eliminates the need for ground truth labels and uses self-supervised losses as a search metric. This aligns with the user's interest in research projects that focus on dataset learning for unsupervised optical flow. Additionally, the paper evaluates performance on real-world datasets like DAVIS, further emphasizing its relevance to unsupervised settings."}
{"input": "User Query: Can you name the studies that have studied the problem of over-smoothing in Message Passing Neural Networks (MPNNs) from an asymptotic perspective?\nTitle: Graph Neural Networks Exponentially Lose Expressive Power for Node Classification\nAbstract:  Abstract Graph Neural Networks (graph NNs) are a promising deep learning approach for analyzing graph-structured data. However, it is known that they do not improve (or sometimes worsen) their predictive performance as we pile up many layers and add non-lineality. To tackle this problem, we investigate the expressive power of graph NNs via their asymptotic behaviors as the layer size tends to infinity. Our strategy is to generalize the forward propagation of a Graph Convolutional Network (GCN), which is a popular graph NN variant, as a specific dynamical system. In the case of a GCN, we show that when its weights satisfy the conditions determined by the spectra of the (augmented) normalized Laplacian, its output exponentially approaches the set of signals that carry information of the connected components and node degrees only for distinguishing nodes. Our theory enables us to relate the expressive power of GCNs with the topological information of the underlying graphs inherent in the graph spectra. To demonstrate this, we characterize the asymptotic behavior of GCNs on the Erdős – Rényi graph. We show that when the Erdős – Rényi graph is sufficiently dense and large, a broad range of GCNs on it suffers from the “information loss” in the limit of infinite layers with high probability. Based on the theory, we provide a principled guideline for weight normalization of graph NNs. We experimentally confirm that the proposed weight scaling enhances the predictive performance of GCNs in real data111Code is available at https://github.com/delta2323/gnn-asymptotics.. ", "output": "**Decision:** True  \n**Reason:** The searched paper, titled *\"Graph Neural Networks Exponentially Lose Expressive Power for Node Classification,\"* investigates the asymptotic behavior of Graph Neural Networks (GNNs), specifically focusing on the problem of information loss as the number of layers increases. This phenomenon is closely related to over-smoothing, where node representations become indistinguishable as the depth of the network grows. The paper explicitly analyzes this issue in the context of Graph Convolutional Networks (GCNs), a subclass of Message Passing Neural Networks (MPNNs), by modeling their forward propagation as a dynamical system and examining their behavior in the limit of infinite layers. This aligns with the user query, as the study addresses over-smoothing in MPNNs from an asymptotic perspective."}
{"input": "User Query: Which studies introduced metrics that leverage image features directly?\nTitle: UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning\nAbstract:  Abstract Despite the success of various text generation metrics such as BERTScore, it is still difficult to evaluate the image captions without enough reference captions due to the diversity of the descriptions. In this paper, we introduce a new metric UMIC, an Unreferenced Metric for Image Captioning which does not require reference captions to evaluate image captions. Based on Vision-and-Language BERT, we train UMIC to discriminate negative captions via contrastive learning. Also, we observe critical problems of the previous benchmark dataset (i.e., human annotations) on image captioning metric, and introduce a new collection of human annotations on the generated captions. We validate UMIC on four datasets, including our new dataset, and show that UMIC has a higher correlation than all previous metrics that require multiple references. We release the benchmark dataset and pre-trained models to compute the UMIC111https://github.com/hwanheelee1993/UMIC. ", "output": "**Decision:** True  \n**Reason:** The paper \"UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning\" introduces a metric (UMIC) that evaluates image captions without requiring reference captions. The metric leverages Vision-and-Language BERT, which inherently uses image features in combination with text features to assess the quality of captions. This aligns with the user query, as the study introduces a metric that directly incorporates image features in its evaluation process."}
{"input": "User Query: Which works presented neuro-symbolic approaches for visual reasoning tasks?\nTitle: Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding\nAbstract:  Abstract We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our neural-symbolic visual question answering (NS-VQA) system first recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three unique advantages. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of 99.8% on the CLEVR dataset. Second, the model is more data- and memory-efficient: it performs well after learning on a small number of training data; it can also encode an image into a compact representation, requiring less storage than existing methods for offline question answering. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding\" fully satisfies the user query. The paper explicitly describes a neuro-symbolic approach for visual reasoning tasks. It combines deep representation learning (neural methods) for visual recognition and language understanding with symbolic program execution for reasoning. The system, NS-VQA, utilizes a structural scene representation derived from images and program traces derived from questions, executing symbolic programs to answer questions. This approach directly addresses visual reasoning tasks by integrating neural and symbolic methods, making it a neuro-symbolic approach."}
{"input": "User Query: Which paper contributes to the development of text-to-image diffusion models such as Stable Diffusion?\nTitle: High-Resolution Image Synthesis with Latent Diffusion Models\nAbstract:  Abstract By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state-of-the-art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including text-to-image synthesis, unconditional image generation and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.  ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"High-Resolution Image Synthesis with Latent Diffusion Models\"* directly contributes to the development of text-to-image diffusion models such as Stable Diffusion. The abstract explicitly mentions that the authors introduce cross-attention layers into the model architecture, enabling diffusion models to act as powerful and flexible generators for general conditioning inputs, including text. This mechanism is a key component in text-to-image synthesis, as it allows the model to condition image generation on textual descriptions. Furthermore, the paper highlights the use of latent space representations, which is a foundational aspect of Stable Diffusion, enabling high-quality image synthesis with reduced computational requirements. Hence, the paper satisfies the detailed requirements of the user query by contributing to the development of text-to-image diffusion models like Stable Diffusion."}
{"input": "User Query: What research works considered instance-dependent bounds in the context of RL?\nTitle: Non-Asymptotic Gap-Dependent Regret Bounds for Tabular MDPs\nAbstract:  Abstract This paper establishes that optimistic algorithms attain gap-dependent and non-asymptotic logarithmic regret for episodic MDPs. In contrast to prior work, our bounds do not suffer a dependence on diameter-like quantities or ergodicity, and smoothly interpolate between the gap dependent logarithmic-regret, and the 𝒪~​(H​S​A​T)~𝒪𝐻𝑆𝐴𝑇\\widetilde{\\mathcal{O}}(\\sqrt{HSAT})-minimax rate. The key technique in our analysis is a novel “clipped” regret decomposition which applies to a broad family of recent optimistic algorithms for episodic MDPs. ", "output": "**Decision:** True  \n**Reason:** The paper explicitly discusses \"gap-dependent\" regret bounds, which are a form of instance-dependent bounds in the context of reinforcement learning (RL). Instance-dependent bounds refer to performance guarantees that depend on specific characteristics of the problem instance, such as the gaps between optimal and suboptimal actions or policies. The abstract highlights that the paper establishes gap-dependent logarithmic regret bounds for episodic Markov Decision Processes (MDPs), which directly aligns with the user query about research works considering instance-dependent bounds in RL. Additionally, the paper introduces a novel regret decomposition technique applicable to optimistic algorithms, further supporting its relevance to the query."}
{"input": "User Query: Which works discuss the counterfactual fairness?\nTitle: Counterfactual Fairness\nAbstract:  Abstract Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it is the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Counterfactual Fairness\" directly addresses the concept of counterfactual fairness, as indicated by both its title and abstract. The abstract explicitly states that the paper develops a framework for modeling fairness using tools from causal inference and introduces a definition of counterfactual fairness. It further explains that this fairness concept ensures decisions are consistent between the actual world and a counterfactual world where an individual's demographic group is different. Additionally, the paper demonstrates the application of this framework in a real-world scenario. Therefore, it fully satisfies the user query by discussing counterfactual fairness in depth."}
{"input": "User Query: Which papers discuss the dynamic NeRFs that capture scenes by separately encoding each frame?\nTitle: INV: Towards Streaming Incremental Neural Videos\nAbstract:  Abstract Recent works in spatiotemporal radiance fields can produce photorealistic free-viewpoint videos. However, they are inherently unsuitable for interactive streaming scenarios (e.g. video conferencing, telepresence) because have an inevitable lag even if the training is instantaneous. This is because these approaches consume videos and thus have to buffer chunks of frames (often seconds) before processing. In this work, we take a step towards interactive streaming via a frame-by-frame approach naturally free of lag. Conventional wisdom believes that per-frame NeRFs are impractical due to prohibitive training costs and storage. We break this belief by introducing Incremental Neural Videos (INV), a per-frame NeRF that is efficiently trained and streamable. We designed INV based on two insights: (1) Our main finding is that MLPs naturally partition themselves into Structure and Color Layers, which store structural and color/texture information respectively. (2) We leverage this property to retain and improve upon knowledge from previous frames, thus amortizing training across frames and reducing redundant learning. As a result, with negligible changes to NeRF, INV can achieve good qualities (>28.6​d​babsent28.6𝑑𝑏>28.6db) in 8min/frame. It can also outperform prior SOTA in 19%percent1919\\% less training time. Additionally, our Temporal Weight Compression reduces the per-frame size to 0.3MB/frame (6.6%percent6.66.6\\% of NeRF). More importantly, INV is free from buffer lag and is naturally fit for streaming. While this work does not achieve real-time training, it shows that incremental approaches like INV present new possibilities in interactive 3D streaming. Moreover, our discovery of natural information partition leads to a better understanding and manipulation of MLPs. Code and dataset will be released soon. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"INV: Towards Streaming Incremental Neural Videos\" satisfies the user query as it discusses a dynamic NeRF approach that captures scenes by separately encoding each frame. The abstract explicitly mentions that the proposed method, Incremental Neural Videos (INV), adopts a per-frame NeRF approach, which is a key aspect of the user query. Additionally, the paper highlights how INV efficiently trains and streams per-frame NeRFs, leveraging insights to retain and improve knowledge across frames. This aligns with the requirement of separately encoding each frame in a dynamic NeRF system."}
{"input": "User Query: Any works about variational autoencoder models that allow to disentangle multiple factors?\nTitle: Learning Hierarchical Features from Generative Models\nAbstract:  Abstract Deep neural networks have been shown to be very successful at learning feature hierarchies in supervised learning tasks. Generative models, on the other hand, have benefited less from hierarchical models with multiple layers of latent variables. In this paper, we prove that hierarchical latent variable models do not take advantage of the hierarchical structure when trained with existing variational methods, and provide some limitations on the kind of features existing models can learn. Finally we propose an alternative architecture that do not suffer from these limitations. Our model is able to learn highly interpretable and disentangled hierarchical features on several natural image datasets with no task specific regularization or prior knowledge. ", "output": "**Decision:** True  \n**Reason:** The searched paper discusses generative models with hierarchical latent variable structures and addresses the limitations of existing variational methods in learning hierarchical features. It proposes an alternative architecture that successfully learns disentangled hierarchical features. Since the user query specifically asks for works about variational autoencoder (VAE) models that allow disentangling multiple factors, and the paper explicitly focuses on disentangling features in generative models (which are often implemented as VAEs), it satisfies the requirements of the query. The abstract indicates that the proposed model achieves disentanglement without task-specific regularization or prior knowledge, aligning with the user's interest in disentangling multiple factors."}
{"input": "User Query: Could you mention some studies about 'Shallow' synthetics generation?\nTitle: SMOTE: Synthetic Minority Over-sampling Technique\nAbstract: An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of \"normal\" examples with only a small percentage of \"abnormal\" or \"interesting\" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.", "output": "**Decision:** True  \n**Reason:** The searched paper, \"SMOTE: Synthetic Minority Over-sampling Technique,\" satisfies the user query regarding studies about \"shallow\" synthetics generation. SMOTE focuses on generating synthetic data points for the minority class in imbalanced datasets. The method involves creating synthetic examples by interpolating between existing data points, which is a straightforward and computationally efficient approach. This aligns with the concept of \"shallow\" synthetic generation, as it does not involve deep learning or complex generative models. The paper provides a detailed explanation of the technique and evaluates its effectiveness, making it relevant to the query."}
{"input": "User Query: What studies implement sequential sampling without replacement using a stochastic beam search?\nTitle: Stochastic Beams and Where to Find Them: The Gumbel-Top-𝑘 Trick for Sampling Sequences Without Replacement\nAbstract:  Abstract The well-known Gumbel-Max trick for sampling from a categorical distribution can be extended to sample k𝑘k elements without replacement. We show how to implicitly apply this ‘Gumbel-Top-k𝑘k’ trick on a factorized distribution over sequences, allowing to draw exact samples without replacement using a Stochastic Beam Search. Even for exponentially large domains, the number of model evaluations grows only linear in k𝑘k and the maximum sampled sequence length. The algorithm creates a theoretical connection between sampling and (deterministic) beam search and can be used as a principled intermediate alternative. In a translation task, the proposed method compares favourably against alternatives to obtain diverse yet good quality translations. We show that sequences sampled without replacement can be used to construct low-variance estimators for expected sentence-level BLEU score and model entropy. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Stochastic Beams and Where to Find Them: The Gumbel-Top-𝑘 Trick for Sampling Sequences Without Replacement\" satisfies the requirements of the user query. The paper explicitly discusses the implementation of sequential sampling without replacement using a stochastic beam search. It introduces the Gumbel-Top-𝑘 trick to sample sequences without replacement and applies this method within a stochastic beam search framework. The abstract also highlights that the method is designed for sampling sequences without replacement and establishes a connection between stochastic beam search and deterministic beam search. Therefore, the paper aligns with the detailed requirements of the query."}
{"input": "User Query: Which studies have conditioned recent diffusion models with brain activations to reconstruct observed images?\nTitle: Second Sight: Using brain-optimized encoding models to align image distributions with human brain activity\nAbstract:  Abstract Two recent developments have accelerated progress in image reconstruction from human brain activity: large datasets that offer samples of brain activity in response to many thousands of natural scenes, and the open-sourcing of powerful stochastic image-generators that accept both low- and high-level guidance. Most work in this space has focused on obtaining point estimates of the target image, with the ultimate goal of approximating literal pixel-wise reconstructions of target images from the brain activity patterns they evoke. This emphasis belies the fact that there is always a family of images that are equally compatible with any evoked brain activity pattern, and the fact that many image-generators are inherently stochastic and do not by themselves offer a method for selecting the single best reconstruction from among the samples they generate. We introduce a novel reconstruction procedure (Second Sight) that iteratively refines an image distribution to explicitly maximize the alignment between the predictions of a voxel-wise encoding model and the brain activity patterns evoked by any target image. We use an ensemble of brain-optimized deep neural networks trained on the Natural Scenes Dataset (NSD) as our encoding model, and a latent diffusion model as our image generator. At each iteration, we generate a small library of images and select those that best approximate the measured brain activity when passed through our encoding model. We extract semantic and structural guidance from the selected images, used for generating the next library. We show that this process converges on a distribution of high-quality reconstructions by refining both semantic content and low-level image details across iterations. Images sampled from these converged image distributions are competitive with state-of-the-art reconstruction algorithms. Interestingly, the time-to-convergence varies systematically across visual cortex, with earlier visual areas generally taking longer and converging on narrower image distributions, relative to higher-level brain areas. Second Sight thus offers a succinct and novel method for exploring the diversity of representations across visual brain areas. ", "output": "**Decision:** True  \n**Reason:** The paper \"Second Sight: Using brain-optimized encoding models to align image distributions with human brain activity\" satisfies the user query. The study explicitly discusses the use of a latent diffusion model (a type of recent diffusion model) as an image generator, conditioned on brain activity patterns to reconstruct observed images. The method involves iteratively refining image distributions to align with brain activity, using a voxel-wise encoding model trained on brain data. This aligns directly with the query's focus on studies that condition diffusion models with brain activations for image reconstruction."}
{"input": "User Query: Which papers utilized in-context prompting for LLMs?\nTitle: Language Models are Few-Shot Learners\nAbstract:  Abstract Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general. ", "output": "**Decision:** True  \n**Reason:** The paper \"Language Models are Few-Shot Learners\" explicitly demonstrates the use of in-context prompting for large language models (LLMs). In the abstract, it describes how GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. This approach aligns directly with the concept of in-context prompting, where the model is provided with task instructions and examples within the input context, rather than being fine-tuned. Therefore, the paper satisfies the requirements of the user query."}
{"input": "User Query: What works discuss the output type of multi-trajectory in motion prediction?\nTitle: Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks\nAbstract:  Abstract Understanding human motion behavior is critical for autonomous moving platforms (like self-driving cars and social robots) if they are to navigate human-centric environments. This is challenging because human motion is inherently multimodal: given a history of human motion paths, there are many socially plausible ways that people could move in the future. We tackle this problem by combining tools from sequence prediction and generative adversarial networks: a recurrent sequence-to-sequence model observes motion histories and predicts future behavior, using a novel pooling mechanism to aggregate information across people. We predict socially plausible futures by training adversarially against a recurrent discriminator, and encourage diverse predictions with a novel variety loss. Through experiments on several datasets we demonstrate that our approach outperforms prior work in terms of accuracy, variety, collision avoidance, and computational complexity. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks\" satisfies the user query because it explicitly discusses the generation of multiple socially plausible trajectories for motion prediction. The abstract highlights the multimodal nature of human motion and mentions that the model predicts diverse future trajectories using a novel variety loss. This directly addresses the query regarding the output type of multi-trajectory in motion prediction, as the paper focuses on generating multiple plausible future paths rather than a single deterministic trajectory. Additionally, the use of generative adversarial networks (GANs) inherently supports the generation of diverse outputs, which aligns with the concept of multi-trajectory prediction."}
{"input": "User Query: Which works introduced benchmarks for estimating scene flow from stereo videos?\nTitle: A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation\nAbstract:  Abstract Recent work has shown that optical flow estimation can be formulated as a supervised learning task and can be successfully solved with convolutional networks. Training of the so-called FlowNet was enabled by a large synthetically generated dataset. The present paper extends the concept of optical flow estimation via convolutional networks to disparity and scene flow estimation. To this end, we propose three synthetic stereo video datasets with sufficient realism, variation, and size to successfully train large networks. Our datasets are the first large-scale datasets to enable training and evaluating scene flow methods. Besides the datasets, we present a convolutional network for real-time disparity estimation that provides state-of-the-art results. By combining a flow and disparity estimation network and training it jointly, we demonstrate the first scene flow estimation with a convolutional network. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation\" satisfies the user query. The abstract explicitly mentions the introduction of \"three synthetic stereo video datasets\" designed for training and evaluating scene flow methods. These datasets serve as benchmarks for scene flow estimation from stereo videos, fulfilling the user's requirement of identifying works that introduced benchmarks for this purpose. Additionally, the paper highlights that these are the first large-scale datasets for this task, further emphasizing their role as benchmarks in the field."}
{"input": "User Query: Could you provide me some references about evaluation on programming languages towards multilinguality?\nTitle: CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X\nAbstract:  Abstract Large pre-trained code generation models, such as OpenAI Codex, can generate syntax- and function-correct code, making the coding of programmers more productive and our pursuit of artificial general intelligence closer. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 4.7 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to increase coding efficiency for 83.4% of its users. Finally, CodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code, model weights (the version of 850B tokens), API, extensions, and HumanEval-X at https://github.com/THUDM/CodeGeeX. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X\" satisfies the user query. The abstract explicitly mentions that the paper evaluates programming languages towards multilinguality. It introduces CodeGeeX, a multilingual code generation model trained on 23 programming languages, and evaluates its performance using the HumanEval-X benchmark, which extends the original HumanEval benchmark to multiple programming languages (C++, Java, JavaScript, and Go). This directly aligns with the user's request for references about evaluation on programming languages towards multilinguality."}
{"input": "User Query: Can you tell me about a study that confirmed that features extracted through CNN contain position signals?\nTitle: How Much Position Information Do Convolutional Neural Networks Encode?\nAbstract:  Abstract In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"How Much Position Information Do Convolutional Neural Networks Encode?\"* satisfies the requirements of the user query. The abstract explicitly states that the study investigates whether CNNs encode absolute position information and confirms this hypothesis through a comprehensive set of experiments. The findings reveal that CNNs indeed encode a surprising degree of positional information, and the paper explores how and where this information is represented. This aligns with the user's request for a study that confirms that features extracted through CNNs contain position signals."}
{"input": "User Query: What studies focused on composition of scenes from object-“slots” in the domain of object-centric learning?\nTitle: Multi-Object Representation Learning with Iterative Variational Inference\nAbstract:  Abstract Human perception is structured around objects which form the basis for our higher-level cognition and impressive systematic generalization abilities. Yet most work on representation learning focuses on feature learning without even considering multiple objects, or treats segmentation as an (often supervised) preprocessing step. Instead, we argue for the importance of learning to segment and represent objects jointly. We demonstrate that, starting from the simple assumption that a scene is composed of multiple entities, it is possible to learn to segment images into interpretable objects with disentangled representations. Our method learns – without supervision – to inpaint occluded parts, and extrapolates to scenes with more objects and to unseen objects with novel feature combinations. We also show that, due to the use of iterative variational inference, our system is able to learn multi-modal posteriors for ambiguous inputs and extends naturally to sequences. ", "output": "**Decision:** True  \n**Reason:** The paper \"Multi-Object Representation Learning with Iterative Variational Inference\" satisfies the user query as it focuses on the composition of scenes from object \"slots\" in the domain of object-centric learning. The abstract explicitly discusses learning to segment and represent objects jointly, starting from the assumption that a scene is composed of multiple entities (i.e., object-centric representation). The method learns to disentangle representations of objects and extrapolates to scenes with varying numbers of objects and novel feature combinations, aligning with the concept of composing scenes from object \"slots.\" Additionally, the use of iterative variational inference suggests a structured approach to representing objects, which is consistent with the object-centric learning paradigm."}
{"input": "User Query: Are there any works that have explored various parallelism strategies for accelerating the training and inference of large language models?\nTitle: GSPMD: General and Scalable Parallelization for ML Computation Graphs\nAbstract:  Abstract. We present GSPMD, an automatic, compiler-based parallelization system for common machine learning computations. It allows users to write programs in the same way as for a single device, then give hints through a few annotations on how to distribute tensors, based on which GSPMD will parallelize the computation. Its representation of partitioning is simple yet general, allowing it to express different or mixed paradigms of parallelism on a wide variety of models.  GSPMD infers the partitioning for every operator based on limited user annotations, making it convenient to scale existing single-device programs. It solves several technical challenges for production usage, allowing GSPMD to achieve 50% to 62% compute utilization on up to 2048 Cloud TPUv3 cores for models with up to one trillion parameters.  ", "output": "**Decision:** True  \n**Reason:** The paper \"GSPMD: General and Scalable Parallelization for ML Computation Graphs\" satisfies the user query as it explores various parallelism strategies for accelerating the training and inference of large-scale machine learning models, including those with up to one trillion parameters. The abstract explicitly mentions that GSPMD supports different or mixed paradigms of parallelism, which aligns with the user's interest in exploring parallelism strategies. Additionally, the paper addresses the scalability of training and inference by achieving high compute utilization across a large number of devices (up to 2048 Cloud TPUv3 cores). While the abstract does not explicitly focus on \"large language models,\" the techniques and strategies discussed are applicable to such models, given their scale and computational demands. Therefore, the paper is relevant to the user's query."}
{"input": "User Query: Can you provide me studies that extended context length by modifying RoPE for length extrapolation challenges?\nTitle: Extending Context Window of Large Language Models via Position Interpolation\nAbstract:  Abstract We present Position Interpolation (PI) that extends the context window sizes of RoPE-based (Su et al., 2021) pretrained LLMs such as LLaMA (Touvron et al., 2023) models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B. Meanwhile, the extended model by Position Interpolation preserve quality relatively well on tasks within its original context window. To achieve this goal, Position Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond the trained context length which may lead to catastrophically high attention scores that completely ruin the self-attention mechanism. Our theoretical study shows that the upper bound of interpolation is at least ∼600×\\sim 600\\times smaller than that of extrapolation, further demonstrating its stability. Models extended via Position Interpolation retain its original architecture and can reuse most pre-existing optimization and infrastructure. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Extending Context Window of Large Language Models via Position Interpolation\" satisfies the user query. The paper specifically addresses the challenge of extending the context length of RoPE-based models (e.g., LLaMA) by introducing a method called Position Interpolation. This method modifies the positional encoding mechanism (RoPE) to extend the context length while maintaining stability and performance. Although the approach involves interpolation rather than direct extrapolation, it is still a modification of RoPE aimed at addressing the challenges associated with length extrapolation. Therefore, the paper aligns with the user's request for studies that extend context length by modifying RoPE for length extrapolation challenges."}
{"input": "User Query: Which works demonstrate that instruction tuning improves performance on new, unseen tasks?\nTitle: Finetuned Language Models Are Zero-Shot Learners\nAbstract:  Abstract This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning—finetuning language models on a collection of datasets described via instructions—substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Finetuned Language Models Are Zero-Shot Learners\" fully satisfies the user query. The abstract explicitly states that the authors demonstrate how instruction tuning—finetuning language models on a collection of datasets described via instructions—substantially improves zero-shot performance on unseen tasks. The paper evaluates the instruction-tuned model (FLAN) on unseen task types and reports significant improvements compared to the unmodified model and even surpasses GPT-3 on various datasets. Additionally, the paper includes ablation studies that identify key factors contributing to the success of instruction tuning, further supporting the claim that instruction tuning improves performance on new, unseen tasks."}
{"input": "User Query: Which study first introduced the 'double descent' performance curve in overparametrized models?\nTitle: Reconciling modern machine learning practice and the bias-variance trade-off\nAbstract:  Abstract Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias-variance trade-off, appears to be at odds with the observed behavior of methods used in the modern machine learning practice. The bias-variance trade-off implies that a model should balance under-fitting and over-fitting: rich enough to express underlying structure in data, simple enough to avoid fitting spurious patterns. However, in the modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered over-fit, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This “double descent” curve subsumes the textbook U-shaped bias-variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine learning models delineates the limits of classical analyses, and has implications for both the theory and practice of machine learning. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Reconciling modern machine learning practice and the bias-variance trade-off\"* satisfies the user query. It explicitly introduces the concept of the \"double descent\" performance curve, which extends the classical U-shaped bias-variance trade-off curve. The abstract clearly states that the paper provides evidence for the existence and ubiquity of the double descent phenomenon in overparameterized models and posits a mechanism for its emergence. This aligns with the user query, which seeks the study that first introduced the \"double descent\" performance curve in overparameterized models."}
{"input": "User Query: Any works about the application of DTW in representation learning?\nTitle: Temporal Feature Alignment in Contrastive Self-Supervised Learning for Human Activity Recognition\nAbstract:  Abstract Automated Human Activity Recognition has long been a problem of great interest in human-centered and ubiquitous computing. In the last years, a plethora of supervised learning algorithms based on deep neural networks has been suggested to address this problem using various modalities. While every modality has its own limitations, there is one common challenge. Namely, supervised learning requires vast amounts of annotated data which is practically hard to collect. In this paper, we benefit from the self-supervised learning paradigm (SSL) that is typically used to learn deep feature representations from unlabeled data. Moreover, we upgrade a contrastive SSL framework, namely SimCLR, widely used in various applications by introducing a temporal feature alignment procedure for Human Activity Recognition. Specifically, we propose integrating a dynamic time warping (DTW) algorithm in a latent space to force features to be aligned in a temporal dimension. Extensive experiments have been conducted for the unimodal scenario with inertial modality as well as in multimodal settings using inertial and skeleton data. According to the obtained results, the proposed approach has a great potential in learning robust feature representations compared to the recent SSL baselines, and clearly outperforms supervised models in semi-supervised learning. The code for the unimodal case is available via the following link: https://github.com/bulatkh/csshar_tfa. ", "output": "**Decision:** True  \n**Reason:** The searched paper directly addresses the application of Dynamic Time Warping (DTW) in representation learning. Specifically, it introduces a temporal feature alignment procedure using DTW within a contrastive self-supervised learning (SSL) framework (SimCLR) for Human Activity Recognition. The paper explicitly discusses how DTW is utilized in the latent space to align features temporally, which is a core aspect of representation learning. This aligns well with the user query, as it explores the use of DTW for improving feature representation learning."}
{"input": "User Query: What works discuss the development of universal speech and audio understanding models?\nTitle: \\emojipengibig Pengi: An Audio Language Model for Audio Tasks\nAbstract:  Abstract In the domain of audio processing, Transfer Learning has facilitated the rise of Self-Supervised Learning and Zero-Shot Learning techniques. These approaches have led to the development of versatile models capable of tackling a wide array of tasks, while delivering state-of-the-art performance. However, current models inherently lack the capacity to produce the requisite language for open-ended tasks, such as Audio Captioning or Audio Question Answering. We introduce Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and text, and generates free-form text as output. The input audio is represented as a sequence of continuous embeddings by an audio encoder. A text encoder does the same for the corresponding text input. Both sequences are combined as a prefix to prompt a pre-trained frozen language model. The unified architecture of Pengi enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions. When evaluated on 21 downstream tasks, our approach yields state-of-the-art performance in several of them. Our results show that connecting language models with audio models is a major step towards general-purpose audio understanding 111Code is available here: https://github.com/microsoft/Pengi. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Pengi: An Audio Language Model for Audio Tasks\" satisfies the user query as it discusses the development of a universal speech and audio understanding model. The abstract highlights the creation of Pengi, an audio language model that leverages transfer learning and frames all audio tasks as text-generation tasks. This approach enables the model to handle both open-ended tasks (e.g., audio captioning, audio question answering) and close-ended tasks without additional fine-tuning or task-specific extensions. The paper emphasizes the integration of language models with audio models, which is a significant step toward general-purpose audio understanding. Additionally, the evaluation on 21 downstream tasks and the achievement of state-of-the-art performance in several of them further support its relevance to the development of universal models in this domain."}
{"input": "User Query: Which works discuss the role of instruction tuning in inspiring the instruction-following ability of LLMs and aligning with humans?\nTitle: Super-NaturalInstructions: A Benchmark of 1,600+ Language Tasks and Instructions\nAbstract:  Abstract How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions,111Super-NaturalInstructions represents a super-sized expansion of NaturalInstructions Mishra et al. (2022b) which had 61 tasks. a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions—training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones.  Furthermore, we build Tk𝑘k-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k𝑘k-shot examples). Our experiments show that Tk𝑘k-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.222 The dataset, models, and a leaderboard can be found at https://instructions.apps.allenai.org.   ††♢♢\\diamondsuit Co-first authors ♣♣\\clubsuit Co-second authors ", "output": "**Decision:** True  \n**Reason:** The paper \"Super-NaturalInstructions: A Benchmark of 1,600+ Language Tasks and Instructions\" satisfies the user query as it explicitly discusses the role of instruction tuning in inspiring the instruction-following ability of large language models (LLMs). The abstract highlights that the benchmark and experiments focus on training models to follow a variety of instructions and evaluate their generalization to unseen tasks. Additionally, the paper introduces Tk-Instruct, a model trained to follow in-context instructions, and demonstrates its superior performance compared to other instruction-following models like InstructGPT. This aligns with the user's query about works that explore how instruction tuning enhances the instruction-following ability of LLMs. While the abstract does not explicitly mention alignment with humans, the focus on instruction-following and generalization implies a connection to aligning model behavior with human-provided instructions."}
{"input": "User Query: Which studies used a neural network for volume density mapping in 3D spatial locations?\nTitle: NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\nAbstract:  Abstract We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x,y,z)𝑥𝑦𝑧(x,y,z) and viewing direction (θ,ϕ)𝜃italic-ϕ(\\theta,\\phi)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis\" fully satisfies the requirements of the user query. The study explicitly uses a neural network to represent a scene by mapping 3D spatial locations (x, y, z) to volume density and view-dependent emitted radiance. The neural network takes 5D inputs (3D spatial coordinates and 2D viewing direction) and outputs the volume density, which directly addresses the query's focus on using a neural network for volume density mapping in 3D spatial locations."}
{"input": "User Query: Which papers use natural language descriptions as direct inputs for motion generation in recent advances?\nTitle: TEMOS: Generating diverse human motions from textual descriptions\nAbstract:  Abstract We address the problem of generating diverse 3D human motions from textual descriptions. This challenging task requires joint modeling of both modalities: understanding and extracting useful human-centric information from the text, and then generating plausible and realistic sequences of human poses. In contrast to most previous work which focuses on generating a single, deterministic, motion from a textual description, we design a variational approach that can produce multiple diverse human motions. We propose TEMOS, a text-conditioned generative model leveraging variational autoencoder (VAE) training with human motion data, in combination with a text encoder that produces distribution parameters compatible with the VAE latent space. We show the TEMOS framework can produce both skeleton-based animations as in prior work, as well more expressive SMPL body motions. We evaluate our approach on the KIT Motion-Language benchmark and, despite being relatively straightforward, demonstrate significant improvements over the state of the art. Code and models are available on our webpage. ", "output": "**Decision:** True  \n**Reason:** The paper \"TEMOS: Generating diverse human motions from textual descriptions\" fully satisfies the user query. The abstract explicitly states that the paper addresses the problem of generating 3D human motions directly from textual descriptions, which qualifies as using natural language descriptions as direct inputs for motion generation. Furthermore, the proposed approach, TEMOS, leverages a text encoder to process natural language descriptions and condition the generative model for motion generation. This aligns with the requirement of using natural language descriptions as direct inputs for generating motion."}
{"input": "User Query: Which papers discuss the optimization of the Shampoo method?\nTitle: Shampoo: Preconditioned Stochastic Tensor Optimization\nAbstract:  Abstract Preconditioned gradient methods are among the most general and powerful tools in optimization. However, preconditioning requires storing and manipulating prohibitively large matrices. We describe and analyze a new structure-aware preconditioning algorithm, called Shampoo, for stochastic optimization over tensor spaces. Shampoo maintains a set of preconditioning matrices, each of which operates on a single dimension, contracting over the remaining dimensions. We establish convergence guarantees in the stochastic convex setting, the proof of which builds upon matrix trace inequalities. Our experiments with state-of-the-art deep learning models show that Shampoo is capable of converging considerably faster than commonly used optimizers. Although it involves a more complex update rule, Shampoo’s runtime per step is comparable to that of simple gradient methods such as SGD, AdaGrad, and Adam. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Shampoo: Preconditioned Stochastic Tensor Optimization\" directly discusses the optimization of the Shampoo method. The abstract provides a detailed description of the Shampoo algorithm, including its structure-aware preconditioning approach, convergence guarantees, and performance in deep learning models. It also highlights the algorithm's efficiency and runtime comparisons with other optimizers. Since the user query specifically asks for papers discussing the optimization of the Shampoo method, this paper fully satisfies the requirements of the query."}
{"input": "User Query: Which works introduced MMD-based calibration metrics?\nTitle: Calibration tests beyond classification\nAbstract:  Abstract Most supervised machine learning tasks are subject to irreducible prediction errors. Probabilistic predictive models address this limitation by providing probability distributions that represent a belief over plausible targets, rather than point estimates. Such models can be a valuable tool in decision-making under uncertainty, provided that the model output is meaningful and interpretable. Calibrated models guarantee that the probabilistic predictions are neither over- nor under-confident. In the machine learning literature, different measures and statistical tests have been proposed and studied for evaluating the calibration of classification models. For regression problems, however, research has been focused on a weaker condition of calibration based on predicted quantiles for real-valued targets. In this paper, we propose the first framework that unifies calibration evaluation and tests for general probabilistic predictive models. It applies to any such model, including classification and regression models of arbitrary dimension. Furthermore, the framework generalizes existing measures and provides a more intuitive reformulation of a recently proposed framework for calibration in multi-class classification. In particular, we reformulate and generalize the kernel calibration error, its estimators, and hypothesis tests using scalar-valued kernels, and evaluate the calibration of real-valued regression problems.111The source code of the experiments is available at https://github.com/devmotion/Calibration_ICLR2021. ", "output": "**Decision:** True  \n**Reason:** The abstract of the searched paper explicitly mentions the reformulation and generalization of the kernel calibration error, which is based on scalar-valued kernels. The Maximum Mean Discrepancy (MMD) is a kernel-based method commonly used in statistical testing and calibration. The paper introduces a unified framework for calibration evaluation and tests, which includes generalizing existing measures and reformulating kernel-based calibration metrics. Therefore, it is reasonable to conclude that this paper introduces MMD-based calibration metrics, satisfying the user query."}
{"input": "User Query: Are there any studies that analyzed the failure modes of PINNs?\nTitle: Characterizing possible failure modes in physics-informed neural networks\nAbstract: Recent work in scientific machine learning has developed so-called physics-informed neural network (PINN) models. The typical approach is to incorporate physical domain knowledge as soft constraints on an empirical loss function and use existing machine learning methodologies to train the model. We demonstrate that, while existing PINN methodologies can learn good models for relatively trivial problems, they can easily fail to learn relevant physical phenomena for even slightly more complex problems. In particular, we analyze several distinct situations of widespread physical interest, including learning differential equations with convection, reaction, and diffusion operators. We provide evidence that the soft regularization in PINNs, which involves PDE-based differential operators, can introduce a number of subtle problems, including making the problem more ill-conditioned. Importantly, we show that these possible failure modes are not due to the lack of expressivity in the NN architecture, but that the PINN's setup makes the loss landscape very hard to optimize. We then describe two promising solutions to address these failure modes. The first approach is to use curriculum regularization, where the PINN's loss term starts from a simple PDE regularization, and becomes progressively more complex as the NN gets trained. The second approach is to pose the problem as a sequence-to-sequence learning task, rather than learning to predict the entire space-time at once. Extensive testing shows that we can achieve up to 1-2 orders of magnitude lower error with these methods as compared to regular PINN training.", "output": "**Decision:** True  \n**Reason:** The paper titled \"Characterizing possible failure modes in physics-informed neural networks\" directly addresses the user's query by analyzing the failure modes of PINNs. The abstract explicitly mentions that the authors investigate situations where PINNs fail to learn relevant physical phenomena, particularly in cases involving differential equations with convection, reaction, and diffusion operators. The paper identifies specific issues such as the ill-conditioning of the problem and the difficulty in optimizing the loss landscape. Additionally, the study proposes solutions to mitigate these failure modes, further demonstrating a thorough analysis of the topic. Therefore, the paper fully satisfies the user's query."}
{"input": "User Query: Could you provide me some works that proposed methods for reweighting or pseudo-labelling samples for last layer retraining?\nTitle: Simple and Fast Group Robustness by Automatic Feature Reweighting\nAbstract:  Abstract A major challenge to out-of-distribution generalization is reliance on spurious features — patterns that are predictive of the class label in the training data distribution, but not causally related to the target. Standard methods for reducing the reliance on spurious features typically assume that we know what the spurious feature is, which is rarely true in the real world. Methods that attempt to alleviate this limitation are complex, hard to tune, and lead to a significant computational overhead compared to standard training. In this paper, we propose Automatic Feature Reweighting (AFR), an extremely simple and fast method for updating the model to reduce the reliance on spurious features. AFR retrains the last layer of a standard ERM-trained base model with a weighted loss that emphasizes the examples where the ERM model predicts poorly, automatically upweighting the minority group without group labels. With this simple procedure, we improve upon the best reported results among competing methods trained without spurious attributes on several vision and natural language classification benchmarks, using only a fraction of their compute. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Simple and Fast Group Robustness by Automatic Feature Reweighting\" satisfies the user query. The paper explicitly proposes a method called Automatic Feature Reweighting (AFR), which involves reweighting samples for retraining the last layer of a model. The method uses a weighted loss to emphasize examples where the base model (trained with ERM) predicts poorly, effectively reweighting samples without requiring explicit group labels. This aligns with the user's request for works that propose methods for reweighting or pseudo-labeling samples for last layer retraining."}
{"input": "User Query: Which studies address optimization schemes in Federated Learning?\nTitle: Adaptive Federated Optimization\nAbstract:  Abstract Federated learning is a distributed machine learning paradigm in which a large number of clients coordinate with a central server to learn a model without sharing their own training data. Standard federated optimization methods such as Federated Averaging (FedAvg) are often difficult to tune and exhibit unfavorable convergence behavior. In non-federated settings, adaptive optimization methods have had notable success in combating such issues. In this work, we propose federated versions of adaptive optimizers, including Adagrad, Adam, and Yogi, and analyze their convergence in the presence of heterogeneous data for general nonconvex settings. Our results highlight the interplay between client heterogeneity and communication efficiency. We also perform extensive experiments on these methods and show that the use of adaptive optimizers can significantly improve the performance of federated learning. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Adaptive Federated Optimization\" directly addresses optimization schemes in Federated Learning. It discusses standard federated optimization methods like Federated Averaging (FedAvg) and their limitations, and proposes federated versions of adaptive optimizers such as Adagrad, Adam, and Yogi. The paper also analyzes their convergence in heterogeneous data settings and evaluates their performance through experiments. This aligns well with the user query, which seeks studies addressing optimization schemes in Federated Learning."}
{"input": "User Query: Are there any studies dealing with action hierarchy in the field of action understanding?\nTitle: FineGym: A Hierarchical Video Dataset for Fine-grained Action Understanding\nAbstract:  Abstract On public benchmarks, current action recognition techniques have achieved great success. However, when used in real-world applications, e.g. sport analysis, which requires the capability of parsing an activity into phases and differentiating between subtly different actions, their performances remain far from being satisfactory. To take action recognition to a new level, we develop FineGym111Dataset and codes can be found at https://sdolivia.github.io/FineGym/, a new dataset built on top of gymnastic videos. Compared to existing action recognition datasets, FineGym is distinguished in richness, quality, and diversity. In particular, it provides temporal annotations at both action and sub-action levels with a three-level semantic hierarchy. For example, a “balance beam” event will be annotated as a sequence of elementary sub-actions derived from five sets: “leap-jump-hop”, “beam-turns”, “flight-salto”, “flight-handspring”, and “dismount”, where the sub-action in each set will be further annotated with finely defined class labels. This new level of granularity presents significant challenges for action recognition, e.g. how to parse the temporal structures from a coherent action, and how to distinguish between subtly different action classes. We systematically investigate representative methods on this dataset and obtain a number of interesting findings. We hope this dataset could advance research towards action understanding. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"FineGym: A Hierarchical Video Dataset for Fine-grained Action Understanding\" explicitly addresses the concept of action hierarchy in the field of action understanding. The abstract mentions that the dataset provides temporal annotations at both action and sub-action levels with a three-level semantic hierarchy. This hierarchical structure is central to the dataset and is used to parse and differentiate between actions and sub-actions, which directly aligns with the user's query about studies dealing with action hierarchy in action understanding. Additionally, the paper discusses challenges and methods related to parsing temporal structures and distinguishing subtly different actions, further reinforcing its relevance to the query."}
{"input": "User Query: Could you provide me some references that have attempted to adoapt diffusion models for cross-modal retrieval?\nTitle: DiffusionRet: Generative Text-Video Retrieval with Diffusion Model\nAbstract:  Abstract Existing text-video retrieval solutions are, in essence, discriminant models focused on maximizing the conditional likelihood, i.e., p​(candidates|query)𝑝conditionalcandidatesqueryp(\\textit{candidates}|\\textit{query}). While straightforward, this de facto paradigm overlooks the underlying data distribution p​(query)𝑝queryp(\\textit{query}), which makes it challenging to identify out-of-distribution data. To address this limitation, we creatively tackle this task from a generative viewpoint and model the correlation between the text and the video as their joint probability p​(candidates,query)𝑝candidatesqueryp(\\textit{candidates},\\textit{query}). This is accomplished through a diffusion-based text-video retrieval framework (DiffusionRet), which models the retrieval task as a process of gradually generating joint distribution from noise. During training, DiffusionRet is optimized from both the generation and discrimination perspectives, with the generator being optimized by generation loss and the feature extractor trained with contrastive loss. In this way, DiffusionRet cleverly leverages the strengths of both generative and discriminative methods. Extensive experiments on five commonly used text-video retrieval benchmarks, including MSRVTT, LSMDC, MSVD, ActivityNet Captions, and DiDeMo, with superior performances, justify the efficacy of our method. More encouragingly, without any modification, DiffusionRet even performs well in out-domain retrieval settings. We believe this work brings fundamental insights into the related fields. Code is available at https://github.com/jpthu17/DiffusionRet. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"DiffusionRet: Generative Text-Video Retrieval with Diffusion Model\" satisfies the user's query. The user is looking for references that have attempted to adapt diffusion models for cross-modal retrieval. The paper explicitly proposes a diffusion-based framework (DiffusionRet) for text-video retrieval, which is a form of cross-modal retrieval. It models the correlation between text and video as a joint probability distribution using a diffusion model, thereby adapting diffusion models for the cross-modal retrieval task. Additionally, the paper provides experimental validation on multiple benchmarks, further demonstrating its application in this domain. Hence, it fully aligns with the user's requirements."}
{"input": "User Query: Which papers discuss the study of safe RL addressing constraints that require some expected cumulative costs over all steps to be bounded?\nTitle: Achieving Zero Constraint Violation for Constrained Reinforcement Learning via Primal-Dual Approach\nAbstract:  Abstract Reinforcement learning is widely used in applications where one needs to perform sequential decisions while interacting with the environment. The problem becomes more challenging when the decision requirement includes satisfying some safety constraints. The problem is mathematically formulated as constrained Markov decision process (CMDP). In the literature, various algorithms are available to solve CMDP problems in a model-free manner to achieve ϵitalic-ϵ\\epsilon-optimal cumulative reward with ϵitalic-ϵ\\epsilon feasible policies. An ϵitalic-ϵ\\epsilon-feasible policy implies that it suffers from constraint violation. An important question here is whether we can achieve ϵitalic-ϵ\\epsilon-optimal cumulative reward with zero constraint violations or not. To achieve that, we advocate the use of randomized primal-dual approach to solve the CMDP problems and propose a conservative stochastic primal-dual algorithm (CSPDA) which is shown to exhibit 𝒪~​(1/ϵ2)~𝒪1superscriptitalic-ϵ2\\tilde{\\mathcal{O}}\\left(1/\\epsilon^{2}\\right) sample complexity to achieve ϵitalic-ϵ\\epsilon-optimal cumulative reward with zero constraint violations. In the prior works, the best available sample complexity for the ϵitalic-ϵ\\epsilon-optimal policy with zero constraint violation is 𝒪~​(1/ϵ5)~𝒪1superscriptitalic-ϵ5\\tilde{\\mathcal{O}}\\left(1/\\epsilon^{5}\\right). Hence, the proposed algorithm provides a significant improvement as compared to the state of the art. 111This is the arXiv version with the Appendix for the AAAI 2022 paper with the same title. This work has been further extended with concave utilities and constraints, and is available in v2 of the current arXiv. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Achieving Zero Constraint Violation for Constrained Reinforcement Learning via Primal-Dual Approach\"* satisfies the requirements of the user query. The user is specifically looking for papers that discuss safe reinforcement learning (RL) addressing constraints that require expected cumulative costs over all steps to be bounded. This paper explicitly focuses on constrained reinforcement learning (formulated as constrained Markov decision processes, CMDPs), where safety constraints are integral to the problem formulation. The paper proposes a primal-dual approach to achieve zero constraint violations while optimizing cumulative rewards, which inherently involves bounding expected cumulative costs over all steps. Therefore, the paper aligns well with the user's query."}
{"input": "User Query: What works utilize techniques to constrain the Lipschitz constants of neural networks in practice?\nTitle: Spectral Normalization for Generative Adversarial Networks\nAbstract:  Abstract One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. The code with Chainer Tokui et al. (2015), generated images and pretrained models are available at https://github.com/pfnet-research/sngan_projection. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Spectral Normalization for Generative Adversarial Networks\" satisfies the user query because it introduces a technique called spectral normalization, which explicitly constrains the Lipschitz constant of the neural network by normalizing the spectral norm of the weight matrices. This directly addresses the requirement of utilizing techniques to constrain the Lipschitz constants of neural networks in practice. The method is computationally efficient and has been applied in practice to stabilize GAN training, as demonstrated in the experiments conducted on various datasets."}
{"input": "User Query: What works introduced a Bayesian strategy for debiasing scene graphs in images?\nTitle: Probabilistic Debiasing of Scene Graphs\nAbstract:  Abstract The quality of scene graphs generated by the state-of-the-art (SOTA) models is compromised due to the long-tail nature of the relationships and their parent object pairs. Training of the scene graphs is dominated by the majority relationships of the majority pairs and, therefore, the object-conditional distributions of relationship in the minority pairs are not preserved after the training is converged. Consequently, the biased model performs well on more frequent relationships in the marginal distribution of relationships such as ‘on’ and ‘wearing’, and performs poorly on the less frequent relationships such as ‘eating’ or ‘hanging from’. In this work, we propose virtual evidence incorporated within-triplet Bayesian Network (BN) to preserve the object-conditional distribution of the relationship label and to eradicate the bias created by the marginal probability of the relationships. The insufficient number of relationships in the minority classes poses a significant problem in learning the within-triplet Bayesian network. We address this insufficiency by embedding-based augmentation of triplets where we borrow samples of the minority triplet classes from its neighborhood triplets in the semantic space. We perform experiments on two different datasets and achieve a significant improvement in the mean recall of the relationships. We also achieve better balance between recall and mean recall performance compared to the SOTA de-biasing techniques of scene graph models. Code is publicly available at https://github.com/bashirulazam/within-triplet-debias. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Probabilistic Debiasing of Scene Graphs\" satisfies the user query as it explicitly introduces a Bayesian strategy for debiasing scene graphs in images. The abstract describes the use of a \"within-triplet Bayesian Network (BN)\" to address the bias in scene graph models caused by the long-tail distribution of relationships and object pairs. This Bayesian approach is central to the proposed method for preserving object-conditional distributions and eradicating bias. Additionally, the work demonstrates its effectiveness through experiments and provides publicly available code, further supporting its relevance to the query."}
{"input": "User Query: Which visual synthesis models aim to generate plausible images?\nTitle: A Style-Based Generator Architecture for Generative Adversarial Networks\nAbstract:  Abstract We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"A Style-Based Generator Architecture for Generative Adversarial Networks\" satisfies the user query as it explicitly focuses on a visual synthesis model designed to generate plausible images. The proposed architecture, based on generative adversarial networks (GANs), aims to improve the quality of generated images by introducing a style-based generator. The abstract highlights that the model achieves better interpolation properties, disentangles latent factors, and enhances the state-of-the-art in terms of image quality metrics. These improvements directly align with the goal of generating plausible images, making the paper relevant to the query."}
{"input": "User Query: Which works are about automating the generation of prompt templates?\nTitle: AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\nAbstract:  Abstract The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts\" directly addresses the automation of generating prompt templates. The abstract explicitly states that the authors develop \"AutoPrompt,\" an automated method for creating prompts for various tasks. This aligns with the user query, as the work focuses on automating the generation of prompt templates, eliminating the manual effort typically required to create them. The paper also demonstrates the effectiveness of these automatically generated prompts in tasks like sentiment analysis, natural language inference, and factual knowledge elicitation, further emphasizing its relevance to the query."}
{"input": "User Query: What prior studies expanded the tasks of multimodal systems by including creating OCR?\nTitle: Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond\nAbstract:  Abstract In this work, we introduce the Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images. Starting from the Qwen-LM as a foundation, we endow it with visual capacity by the meticulously designed (i) visual receptor, (ii) input-output interface, (iii) 3-stage training pipeline, and (iv) multilingual multimodal cleaned corpus. Beyond the conventional image description and question-answering, we implement the grounding and text-reading ability of Qwen-VLs by aligning image-caption-box tuples. The resulting models, including Qwen-VL and Qwen-VL-Chat, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks (e.g., image captioning, question answering, visual grounding) and different settings (e.g., zero-shot, few-shot). Moreover, on real-world dialog benchmarks, our instruction-tuned Qwen-VL-Chat also demonstrates superiority compared to existing vision-language chatbots. All models are public to facilitate future research. ", "output": "**Decision:** True  \n**Reason:** The paper \"Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond\" satisfies the user query because it explicitly mentions the inclusion of text-reading capabilities as part of the expanded tasks of the multimodal system. Text reading is a core component of OCR (Optical Character Recognition), and the paper discusses how the Qwen-VL models implement this functionality by aligning image-caption-box tuples. This demonstrates that the study contributes to expanding multimodal systems to include OCR-related tasks, aligning with the user query's requirements."}
{"input": "User Query: What research works leveraged programming languages in vision tasks?\nTitle: ViperGPT: Composing Vision and Language via Code\nAbstract:  Abstract Answering visual queries is a complex task that requires both visual processing and reasoning. End-to-end models, the dominant approach for this task, do not explicitly differentiate between the two, limiting interpretability and generalization. Learning modular programs presents a promising alternative, but has proven challenging due to the difficulty of learning both the programs and modules simultaneously. We introduce ViperGPT, a framework that leverages code-generation models to compose vision-and-language models into subroutines to produce a result for any query. ViperGPT utilizes a provided API to access the available modules, and composes them by generating Python code that is later executed. This simple approach requires no further training, and achieves state-of-the-art results across various complex visual tasks. ", "output": "**Decision:** True  \n**Reason:** The paper \"ViperGPT: Composing Vision and Language via Code\" satisfies the user query as it explicitly discusses leveraging programming languages (Python) in vision tasks. The framework described in the paper uses code-generation models to compose vision-and-language models into subroutines by generating Python code, which is then executed to answer visual queries. This directly aligns with the user's interest in research works that utilize programming languages in vision tasks."}
{"input": "User Query: What studies pioneered the learning of large multimodal models with a contrastive learning objective on massive image-text pair data?\nTitle: Learning Transferable Visual Models From Natural Language Supervision\nAbstract:  Abstract State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Learning Transferable Visual Models From Natural Language Supervision\"* (commonly referred to as CLIP) fully satisfies the detailed requirements of the user query. The study explicitly describes a pioneering approach to learning large multimodal models using a contrastive learning objective on massive image-text pair data. Specifically:  \n\n1. **Multimodal Learning:** The paper focuses on learning from both images and their corresponding text captions, making it a multimodal model.  \n2. **Contrastive Learning Objective:** The pre-training task involves predicting which caption corresponds to which image, which is a contrastive learning objective.  \n3. **Massive Image-Text Pair Data:** The model is trained on a dataset of 400 million image-text pairs collected from the internet, which qualifies as massive data.  \n4. **Pioneering Contribution:** The paper is widely recognized as a groundbreaking work in the field of multimodal learning, introducing scalable methods for learning transferable visual representations from natural language supervision.  \n\nThus, the paper meets all aspects of the query, making the decision **True**."}
{"input": "User Query: Could you provide the researches that addressed population guarantees for the norm of the gradient under differentially privacy?\nTitle: Private Stochastic Non-Convex Optimization: Adaptive Algorithms and Tighter Generalization Bounds\nAbstract:  Abstract We study differentially private (DP) algorithms for stochastic non-convex optimization. In this problem, the goal is to minimize the population loss over a p𝑝p-dimensional space given n𝑛n i.i.d. samples drawn from a distribution. We improve upon the population gradient bound of p/n𝑝𝑛{\\sqrt{p}}/{\\sqrt{n}} from prior work and obtain a sharper rate of p4/n4𝑝𝑛\\sqrt[4]{p}/\\sqrt{n}. We obtain this rate by providing the first analyses on a collection of private gradient-based methods, including adaptive algorithms DP RMSProp and DP Adam. Our proof technique leverages the connection between differential privacy and adaptive data analysis to bound gradient estimation error at every iterate, which circumvents the worse generalization bound from the standard uniform convergence argument. Finally, we evaluate the proposed algorithms on two popular deep learning tasks and demonstrate the empirical advantages of DP adaptive gradient methods over standard DP SGD. ", "output": "**Decision:** True  \n**Reason:** The searched paper directly addresses the topic of population guarantees for the norm of the gradient under differential privacy. Specifically, the paper focuses on improving the population gradient bound in the context of differentially private stochastic non-convex optimization. It provides sharper rates for the population gradient norm and analyzes private gradient-based methods, such as DP RMSProp and DP Adam, to achieve these results. The paper also leverages differential privacy techniques to bound the gradient estimation error, which is a key component in ensuring population guarantees for the gradient norm. Therefore, the paper fully satisfies the detailed requirements of the user query."}
{"input": "User Query: What studies are about training a CNN to generate multi-plane images and rendering novel views through alpha-compositing?\nTitle: Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines\nAbstract:  Abstract. We present a practical and robust deep learning solution for capturing and rendering novel views of complex real world scenes for virtual exploration. Previous approaches either require intractably dense view sampling or provide little to no guidance for how users should sample views of a scene to reliably render high-quality novel views. Instead, we propose an algorithm for view synthesis from an irregular grid of sampled views that first expands each sampled view into a local light field via a multiplane image (MPI) scene representation, then renders novel views by blending adjacent local light fields. We extend traditional plenoptic sampling theory to derive a bound that specifies precisely how densely users should sample views of a given scene when using our algorithm. In practice, we apply this bound to capture and render views of real world scenes that achieve the perceptual quality of Nyquist rate view sampling while using up to 4000×4000\\times fewer views. We demonstrate our approach’s practicality with an augmented reality smartphone app that guides users to capture input images of a scene and viewers that enable realtime virtual exploration on desktop and mobile platforms. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Local Light Field Fusion: Practical View Synthesis with Prescriptive Sampling Guidelines\" satisfies the requirements of the user query. The abstract explicitly mentions the use of a deep learning algorithm to generate multiplane image (MPI) scene representations, which aligns with the concept of training a CNN to generate multi-plane images. Additionally, the paper discusses rendering novel views by blending adjacent local light fields, which involves rendering techniques that are consistent with alpha-compositing. Therefore, the paper aligns with the query's focus on training a CNN for MPI generation and rendering novel views through alpha-compositing."}
{"input": "User Query: Could you provide me some studies about adapting pretrained models to novel domains using parameter-efficient fine-tuning methods?\nTitle: Parameter-Efficient Transfer Learning for NLP\nAbstract:  Abstract Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter’s effectiveness, we transfer the recently proposed BERT Transformer model to 262626 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4%percent0.40.4\\% of the performance of full fine-tuning, adding only 3.6%percent3.63.6\\% parameters per task. By contrast, fine-tuning trains 100%percent100100\\% of the parameters per task.111Code at https://github.com/google-research/adapter-bert ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Parameter-Efficient Transfer Learning for NLP\" fully satisfies the user query. The paper explicitly addresses the adaptation of pretrained models (e.g., BERT) to novel domains or downstream tasks using parameter-efficient fine-tuning methods. It introduces adapter modules, which allow for the addition of only a few trainable parameters per task while keeping the original model's parameters fixed. This approach ensures parameter efficiency and extensibility, aligning with the user's request for studies on parameter-efficient fine-tuning methods for adapting pretrained models to new domains. The paper also demonstrates the effectiveness of this method across diverse tasks, making it directly relevant to the query."}
{"input": "User Query: Which work first formalized and empirically demonstrated the phenomenon of gradient descent on the Edge of Stability (EoS)?\nTitle: Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability\nAbstract:  Abstract We empirically demonstrate that full-batch gradient descent on neural network training objectives typically operates in a regime we call the Edge of Stability. In this regime, the maximum eigenvalue of the training loss Hessian hovers just above the value 2/(step size)2(step size)2/\\text{(step size)}, and the training loss behaves non-monotonically over short timescales, yet consistently decreases over long timescales. Since this behavior is inconsistent with several widespread presumptions in the field of optimization, our findings raise questions as to whether these presumptions are relevant to neural network training. We hope that our findings will inspire future efforts aimed at rigorously understanding optimization at the Edge of Stability. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability\"* satisfies the requirements of the user query. The paper explicitly claims to empirically demonstrate the phenomenon of gradient descent operating at the Edge of Stability (EoS). It also introduces and formalizes the concept of EoS by describing the behavior of the maximum eigenvalue of the Hessian and the non-monotonic yet overall decreasing nature of the training loss. Since the user query asks for the work that first formalized and empirically demonstrated this phenomenon, and the abstract of the paper aligns with these criteria, it is reasonable to conclude that this paper fulfills the query's requirements."}
{"input": "User Query: Which papers present a study on the effects of communication topology on consensus rates in decentralized learning?\nTitle: Revisiting Optimal Convergence Rate for Smooth and Non-convex Stochastic Decentralized Optimization\nAbstract:  Abstract Decentralized optimization is effective to save communication in large-scale machine learning. Although numerous algorithms have been proposed with theoretical guarantees and empirical successes, the performance limits in decentralized optimization, especially the influence of network topology and its associated weight matrix on the optimal convergence rate, have not been fully understood. While Lu and Sa [42] have recently provided an optimal rate for non-convex stochastic decentralized optimization with weight matrices defined over linear graphs, the optimal rate with general weight matrices remains unclear. This paper revisits non-convex stochastic decentralized optimization and establishes an optimal convergence rate with general weight matrices. In addition, we also establish the optimal rate when non-convex loss functions further satisfy the Polyak-Lojasiewicz (PL) condition. Following existing lines of analysis in literature cannot achieve these results. Instead, we leverage the Ring-Lattice graph to admit general weight matrices while maintaining the optimal relation between the graph diameter and weight matrix connectivity. Lastly, we develop a new decentralized algorithm to nearly attain the above two optimal rates under additional mild conditions. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Revisiting Optimal Convergence Rate for Smooth and Non-convex Stochastic Decentralized Optimization\" satisfies the user query as it explicitly studies the influence of communication topology (via the network topology and its associated weight matrix) on the convergence rates in decentralized learning. The abstract highlights that the work investigates the optimal convergence rate with general weight matrices, which are directly tied to the communication topology of the network. Additionally, the paper leverages the Ring-Lattice graph to analyze the relationship between the graph diameter and weight matrix connectivity, further emphasizing the role of communication topology. Therefore, the paper aligns with the user's request for studies on the effects of communication topology on consensus rates in decentralized learning."}
{"input": "User Query: What work used natural language as the intermediate representation between large multimodal models?\nTitle: Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners\nAbstract:  Abstract The goal of this work is to build flexible video-language models that can generalize to various video-to-text tasks from few examples. Existing few-shot video-language learners focus exclusively on the encoder, resulting in the absence of a video-to-text decoder to handle generative tasks. Video captioners have been pretrained on large-scale video-language datasets, but they rely heavily on finetuning and lack the ability to generate text for unseen tasks in a few-shot setting. We propose VidIL, a few-shot Video-language Learner via Image and Language models, which demonstrates strong performance on few-shot video-to-text tasks without the necessity of pretraining or finetuning on any video datasets. We use image-language models to translate the video content into frame captions, object, attribute, and event phrases, and compose them into a temporal-aware template. We then instruct a language model, with a prompt containing a few in-context examples, to generate a target output from the composed content. The flexibility of prompting allows the model to capture any form of text input, such as automatic speech recognition (ASR) transcripts. Our experiments demonstrate the power of language models in understanding videos on a wide variety of video-language tasks, including video captioning, video question answering, video caption retrieval, and video future event prediction. Especially, on video future event prediction, our few-shot model significantly outperforms state-of-the-art supervised models trained on large-scale video datasets. Code and processed data are publicly available for research purposes at https://github.com/MikeWangWZHL/VidIL. ", "output": "**Decision:** True  \n**Reason:** The paper describes a method (VidIL) that uses natural language as an intermediate representation between multimodal models. Specifically, it employs image-language models to translate video content into frame captions, object, attribute, and event phrases, which are then composed into a temporal-aware template. This intermediate natural language representation is subsequently used to prompt a language model to perform various video-to-text tasks. The use of natural language as the intermediate representation aligns with the requirements of the user query, as it bridges the multimodal components (video and text) and facilitates task generalization."}
{"input": "User Query: Could you provide me some works that have utilized machine learning for decoding of cognitive processes and diagnosing mental health disorders?\nTitle: ASD-DiagNet: A hybrid learning approach for detection of Autism Spectrum Disorder using fMRI data\nAbstract:  Abstract Mental disorders such as Autism Spectrum Disorders (ASD) are heterogeneous disorders that are notoriously difficult to diagnose, especially in children. The current psychiatric diagnostic process is based purely on the behavioural observation of symptomology (DSM-5/ICD-10) and may be prone to over-prescribing of drugs due to misdiagnosis. In order to move the field towards more quantitative fashion, we need advanced and scalable machine learning infrastructure that will allow us to identify reliable biomarkers of mental health disorders. In this paper, we propose a framework called ASD-DiagNet for classifying subjects with ASD from healthy subjects by using only fMRI data. We designed and implemented a joint learning procedure using an autoencoder and a single layer perceptron which results in improved quality of extracted features and optimized parameters for the model. Further, we designed and implemented a data augmentation strategy, based on linear interpolation on available feature vectors, that allows us to produce synthetic datasets needed for training of machine learning models. The proposed approach is evaluated on a public dataset provided by Autism Brain Imaging Data Exchange including 1035 subjects coming from 17 different brain imaging centers. Our machine learning model outperforms other state of the art methods from 13 imaging centers with increase in classification accuracy up to 20% with maximum accuracy of 80%. The machine learning technique presented in this paper, in addition to yielding better quality, gives enormous advantages in terms of execution time (40 minutes vs. 6 hours on other methods). The implemented code is available as GPL license on GitHub portal of our lab (https://github.com/pcdslab/ASD-DiagNet). ", "output": "**Decision:** True  \n**Reason:** The paper titled \"ASD-DiagNet: A hybrid learning approach for detection of Autism Spectrum Disorder using fMRI data\" satisfies the user query. The study explicitly utilizes machine learning techniques (a hybrid approach combining an autoencoder and a single-layer perceptron) to diagnose a mental health disorder, specifically Autism Spectrum Disorder (ASD). It employs fMRI data to classify subjects with ASD versus healthy controls, which aligns with the goal of decoding cognitive processes and diagnosing mental health disorders. Additionally, the paper discusses the development of a machine learning framework and highlights its performance improvements in diagnosing ASD, making it relevant to the user's request."}
{"input": "User Query: Are there works that make strives on accelerating the inference process for NeRFs?\nTitle: Hardware Acceleration of Neural Graphics\nAbstract:  Abstract ††footnotetext: Accepted at the 50th International Symposium on Computer Architecture (ISCA-50) Rendering and inverse rendering techniques have recently attained powerful new capabilities and building blocks in the form of neural representations (NR), with derived rendering techniques quickly becoming indispensable tools next to classic computer graphics algorithms, covering a wide range of functions throughout the full pipeline from sensing to pixels. NRs have recently been used to directly learn the geometric and appearance properties of scenes that were previously hard to capture, and to re-synthesize photo realistic imagery based on this information, thereby promising simplifications and replacements for several complex traditional computer graphics problems and algorithms with scalable quality and predictable performance. In this work we ask the question: Does neural graphics (graphics based on NRs) need hardware support? We studied four representative neural graphics applications (NeRF, NSDF, NVR, and GIA) showing that, if we want to render 4k resolution frames at 60 frames per second (FPS) there is a gap of ∼1.51×\\sim 1.51\\times to 55.50×55.50\\times in the desired performance on current GPUs. For AR and VR applications, there is an even larger gap of ∼similar-to\\sim 2-4 orders of magnitude (OOM) between the desired performance and the required system power. We identify that the input encoding and the multi-layer perceptron kernels are the performance bottlenecks, consuming 72.37%percent72.3772.37\\%, 60.0%percent60.060.0\\% and 59.96%percent59.9659.96\\% of application time for multi resolution hashgrid encoding, multi resolution densegrid encoding and low resolution densegrid encoding, respectively. We propose a neural graphics processing cluster (NGPC) – a scalable and flexible hardware architecture that directly accelerates the input encoding and multi-layer perceptron kernels through dedicated engines and supports a wide range of neural graphics applications. To achieve good overall application level performance improvements, we also accelerate the rest of the kernels by fusion into a single kernel, leading to a ∼9.94×\\sim 9.94\\times speedup compared to previous optimized implementations [17] which is sufficient to remove this performance bottleneck. Our results show that, NGPC gives up to 58.36×58.36\\times end-to-end application-level performance improvement, for multi resolution hashgrid encoding on average across the four neural graphics applications, the performance benefits are 12.94×12.94\\times, 20.85×20.85\\times, 33.73×33.73\\times and 39.04×39.04\\times for the hardware scaling factor of 8, 16, 32 and 64, respectively. Our results show that with multi resolution hashgrid encoding, NGPC enables the rendering of 4k Ultra HD resolution frames at 30 FPS for NeRF and 8k Ultra HD resolution frames at 120 FPS for all our other neural graphics applications.  ", "output": "**Decision:** True  \n**Reason:** The paper \"Hardware Acceleration of Neural Graphics\" explicitly addresses the acceleration of the inference process for NeRFs (Neural Radiance Fields). It identifies performance bottlenecks in NeRFs, such as input encoding and multi-layer perceptron kernels, and proposes a specialized hardware architecture called the Neural Graphics Processing Cluster (NGPC) to overcome these bottlenecks. The paper demonstrates significant speedups in rendering performance, including achieving 4k Ultra HD resolution frames at 30 FPS for NeRFs. These results directly align with the user query, as the work focuses on accelerating the inference process for NeRFs through hardware optimization."}
{"input": "User Query: Could you provide me some studies where attention mechanism is used for learning operators?\nTitle: Choose a Transformer: Fourier or Galerkin\nAbstract:  Abstract In this paper, we apply the self-attention from the state-of-the-art Transformer in Attention Is All You Need [88] for the first time to a data-driven operator learning problem related to partial differential equations. An effort is put together to explain the heuristics of, and to improve the efficacy of the attention mechanism. By employing the operator approximation theory in Hilbert spaces, it is demonstrated for the first time that the softmax normalization in the scaled dot-product attention is sufficient but not necessary. Without softmax, the approximation capacity of a linearized Transformer variant can be proved to be comparable to a Petrov-Galerkin projection layer-wise, and the estimate is independent with respect to the sequence length. A new layer normalization scheme mimicking the Petrov-Galerkin projection is proposed to allow a scaling to propagate through attention layers, which helps the model achieve remarkable accuracy in operator learning tasks with unnormalized data. Finally, we present three operator learning experiments, including the viscid Burgers’ equation, an interface Darcy flow, and an inverse interface coefficient identification problem. The newly proposed simple attention-based operator learner, Galerkin Transformer, shows significant improvements in both training cost and evaluation accuracy over its softmax-normalized counterparts. ", "output": "**Decision:** True  \n**Reason:** The searched paper satisfies the user query as it explicitly focuses on using the attention mechanism for learning operators. The abstract describes the application of the self-attention mechanism from Transformers to data-driven operator learning problems, particularly in the context of partial differential equations. The paper also introduces a new attention-based operator learner, the Galerkin Transformer, and demonstrates its efficacy through experiments on operator learning tasks. This aligns directly with the user's request for studies where the attention mechanism is used for learning operators."}
{"input": "User Query: Can you mention some works that contributed to monocular 3D detection in driving scenarios?\nTitle: Probabilistic and Geometric Depth: Detecting Objects in Perspective\nAbstract:  Abstract 3D object detection is an important capability needed in various practical applications such as driver assistance systems. Monocular 3D detection, as a representative general setting among image-based approaches, provides a more economical solution than conventional settings relying on LiDARs but still yields unsatisfactory results. This paper first presents a systematic study on this problem. We observe that the current monocular 3D detection can be simplified as an instance depth estimation problem: The inaccurate instance depth blocks all the other 3D attribute predictions from improving the overall detection performance. Moreover, recent methods directly estimate the depth based on isolated instances or pixels while ignoring the geometric relations across different objects. To this end, we construct geometric relation graphs across predicted objects and use the graph to facilitate depth estimation. As the preliminary depth estimation of each instance is usually inaccurate in this ill-posed setting, we incorporate a probabilistic representation to capture the uncertainty. It provides an important indicator to identify confident predictions and further guide the depth propagation. Despite the simplicity of the basic idea, our method, PGD, obtains significant improvements on KITTI and nuScenes benchmarks, achieving 1st place out of all monocular vision-only methods while still maintaining real-time efficiency. Code and models will be released at https://github.com/open-mmlab/mmdetection3d. ", "output": "**Decision:** True  \n**Reason:** The searched paper directly addresses monocular 3D detection in driving scenarios, which aligns with the user query. The paper discusses improvements in monocular 3D object detection, particularly in the context of driver assistance systems, a key application in driving scenarios. It introduces a novel method (PGD) that enhances depth estimation by leveraging geometric relations and probabilistic representations, achieving state-of-the-art results on driving-related benchmarks like KITTI and nuScenes. Therefore, the paper contributes to the field of monocular 3D detection in driving scenarios, fully satisfying the user query."}
{"input": "User Query: Which work originally discovered the Neural Collapse phenomenon?\nTitle: Prevalence of Neural Collapse during the terminal phase of deep learning training\nAbstract:  Abstract Modern practice for training classification deepnets involves a Terminal Phase of Training (TPT), which begins at the epoch where training error first vanishes; During TPT, the training error stays effectively zero while training loss is pushed towards zero. Direct measurements of TPT, for three prototypical deepnet architectures and across seven canonical classification datasets, expose a pervasive inductive bias we call Neural Collapse, involving four deeply interconnected phenomena: (NC1) Cross-example within-class variability of last-layer training activations collapses to zero, as the individual activations themselves collapse to their class-means; (NC2) The class-means collapse to the vertices of a Simplex Equiangular Tight Frame (ETF); (NC3) Up to rescaling, the last-layer classifiers collapse to the class-means, or in other words to the Simplex ETF, i.e. to a self-dual configuration; (NC4) For a given activation, the classifier’s decision collapses to simply choosing whichever class has the closest train class-mean, i.e. the Nearest Class-Center (NCC) decision rule. The symmetric and very simple geometry induced by the TPT confers important benefits, including better generalization performance, better robustness, and better interpretability. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"Prevalence of Neural Collapse during the terminal phase of deep learning training\"* fully satisfies the user query. The paper explicitly identifies and introduces the concept of Neural Collapse as a pervasive inductive bias observed during the Terminal Phase of Training (TPT) in deep learning. It provides a detailed description of the phenomenon, breaking it down into four interconnected phenomena (NC1-NC4), and establishes its significance in terms of generalization, robustness, and interpretability. Since the user query seeks the original work that discovered Neural Collapse, and this paper is the one that formally introduces and characterizes the phenomenon, it meets the requirements of the query."}
{"input": "User Query: In which works is text-based image generation using a diffusion model discussed?\nTitle: High-Resolution Image Synthesis with Latent Diffusion Models\nAbstract:  Abstract By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state-of-the-art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including text-to-image synthesis, unconditional image generation and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.  ", "output": "**Decision:** True  \n**Reason:** The paper discusses text-based image generation using a diffusion model. It explicitly mentions the use of cross-attention layers in the model architecture to enable diffusion models to act as powerful and flexible generators for general conditioning inputs, including text. Furthermore, the abstract highlights that the latent diffusion models (LDMs) achieve competitive performance on tasks such as text-to-image synthesis. This directly aligns with the user query, which seeks works discussing text-based image generation using diffusion models."}
{"input": "User Query: Could you give me some references about the phenomenon of catastrophic forgetting in machine learning?\nTitle: Measuring Catastrophic Forgetting in Neural Networks\nAbstract:  Abstract Deep neural networks are used in many state-of-the-art systems for machine perception. Once a network is trained to do a specific task, e.g., bird classification, it cannot easily be trained to do new tasks, e.g., incrementally learning to recognize additional bird species or learning an entirely different task such as flower recognition. When new tasks are added, typical deep neural networks are prone to catastrophically forgetting previous tasks. Networks that are capable of assimilating new information incrementally, much like how humans form new memories over time, will be more efficient than re-training the model from scratch each time a new task needs to be learned. There have been multiple attempts to develop schemes that mitigate catastrophic forgetting, but these methods have not been directly compared, the tests used to evaluate them vary considerably, and these methods have only been evaluated on small-scale problems (e.g., MNIST). In this paper, we introduce new metrics and benchmarks for directly comparing five different mechanisms designed to mitigate catastrophic forgetting in neural networks: regularization, ensembling, rehearsal, dual-memory, and sparse-coding. Our experiments on real-world images and sounds show that the mechanism(s) that are critical for optimal performance vary based on the incremental training paradigm and type of data being used, but they all demonstrate that the catastrophic forgetting problem has yet to be solved. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Measuring Catastrophic Forgetting in Neural Networks\" directly addresses the phenomenon of catastrophic forgetting in machine learning. It explains the problem, provides an overview of its significance, and discusses various mechanisms (e.g., regularization, ensembling, rehearsal, dual-memory, and sparse-coding) designed to mitigate it. Additionally, it introduces new metrics and benchmarks for evaluating catastrophic forgetting, which is highly relevant to understanding and studying the phenomenon. The abstract also highlights experiments conducted on real-world data, further contributing to the exploration of catastrophic forgetting. Therefore, the paper fully satisfies the user's query for references about this phenomenon."}
{"input": "User Query: Could you provide studies related to private mean estimation with respect to Mahalanobis distance?\nTitle: A Fast Algorithm for Adaptive Private Mean Estimation\nAbstract: We design an $(\\varepsilon, \\delta)$-differentially private algorithm to estimate the mean of a $d$-variate distribution, with unknown covariance $\\Sigma$, that is adaptive to $\\Sigma$. To within polylogarithmic factors, the estimator achieves optimal rates of convergence with respect to the induced Mahalanobis norm $||\\cdot||_\\Sigma$, takes time $\\tilde{O}(n d^2)$ to compute, has near linear sample complexity for sub-Gaussian distributions, allows $\\Sigma$ to be degenerate or low rank, and adaptively extends beyond sub-Gaussianity. Prior to this work, other methods required exponential computation time or the superlinear scaling $n = \\Omega(d^{3/2})$ to achieve non-trivial error with respect to the norm $||\\cdot||_\\Sigma$.", "output": "**Decision:** True  \n**Reason:** The searched paper directly addresses the topic of private mean estimation with respect to the Mahalanobis distance. The abstract explicitly mentions that the proposed algorithm is $(\\varepsilon, \\delta)$-differentially private and estimates the mean of a $d$-variate distribution with respect to the Mahalanobis norm $||\\cdot||_\\Sigma$. Additionally, it highlights that the algorithm is adaptive to the covariance $\\Sigma$, which is central to the Mahalanobis distance. The paper also discusses optimal rates of convergence and computational efficiency, making it a relevant and comprehensive study for the user's query."}
{"input": "User Query: What studies have proposed different empirical algorithms to close the sim-to-real gap?\nTitle: Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World\nAbstract:  Abstract Bridging the ‘reality gap’ that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to 1.5 cmtimes1.5centimeter1.5\\text{\\,}\\mathrm{cm} and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World\" fully satisfies the user query. The study explicitly proposes and explores an empirical algorithm—domain randomization—to address the sim-to-real gap. Domain randomization involves randomizing rendering in the simulator to introduce variability, making the real-world environment appear as just another variation to the model. This technique is empirically evaluated in the context of object localization and robotic manipulation, demonstrating its effectiveness in bridging the sim-to-real gap. Thus, the paper aligns with the user's request for studies proposing empirical algorithms to close the sim-to-real gap."}
{"input": "User Query: What studies demonstrate the use of prompt tuning for achieving results similar to fine-tuning?\nTitle: Prompt Tuning for Generative Multimodal Pretrained Models\nAbstract:  Abstract Prompt tuning has become a new paradigm for model tuning and it has demonstrated success in natural language pretraining and even vision pretraining. In this work, we explore the transfer of prompt tuning to multimodal pretraining, with a focus on generative multimodal pretrained models, instead of contrastive ones. Specifically, we implement prompt tuning on the unified sequence-to-sequence pretrained model adaptive to both understanding and generation tasks. Experimental results demonstrate that the light-weight prompt tuning can achieve comparable performance with finetuning and surpass other light-weight tuning methods. Besides, in comparison with finetuned models, the prompt-tuned models demonstrate improved robustness against adversarial attacks. We further figure out that experimental factors, including the prompt length, prompt depth, and reparameteratization, have great impacts on the model performance, and thus we empirically provide a recommendation for the setups of prompt tuning. Despite the observed advantages, we still find some limitations in prompt tuning, and we correspondingly point out the directions for future studies. Codes are available at https://github.com/OFA-Sys/OFA ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Prompt Tuning for Generative Multimodal Pretrained Models\" satisfies the user query as it explicitly demonstrates the use of prompt tuning to achieve results comparable to fine-tuning. The abstract highlights that the study implements prompt tuning on a generative multimodal pretrained model and shows that this lightweight tuning method can achieve performance comparable to fine-tuning. Additionally, the paper provides experimental evidence supporting this claim and discusses factors influencing prompt tuning performance. This aligns with the user query's requirement for studies that demonstrate the effectiveness of prompt tuning in achieving results similar to fine-tuning."}
{"input": "User Query: Which works propose knowledge editing methods that predict updates to the weights of the base model by knowledge locating or meta-learning?\nTitle: Mass-Editing Memory in a Transformer\nAbstract:  Abstract Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by orders of magnitude. Our code and data are at memit.baulab.info. ", "output": "**Decision:** True  \n**Reason:** The paper \"Mass-Editing Memory in a Transformer\" introduces MEMIT, a method for directly updating a language model's weights to incorporate new knowledge or replace outdated information. The abstract explicitly mentions that MEMIT is capable of updating the model's weights with many associations, which aligns with the concept of predicting updates to the base model's weights. While the abstract does not explicitly use the terms \"knowledge locating\" or \"meta-learning,\" the described approach inherently involves identifying and modifying specific parts of the model's knowledge (knowledge locating) to perform updates. Therefore, the paper satisfies the requirements of the user query."}
{"input": "User Query: Which works apply diffusion model in text generation domains?\nTitle: DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models\nAbstract:  Abstract Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing DiffuSeq: a diffusion model designed for sequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation over a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or even better performance than six established baselines, including a state-of-the-art model that is based on pre-trained language models. Apart from quality, an intriguing property of DiffuSeq is its high diversity during generation, which is desired in many Seq2Seq tasks. We further include a theoretical analysis revealing the connection between DiffuSeq and autoregressive/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks. 111Code is available at https://github.com/Shark-NLP/DiffuSeq ", "output": "**Decision:** True  \n**Reason:** The paper titled \"DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models\" fully satisfies the user query. The abstract explicitly states that the work applies diffusion models to natural language, specifically in the domain of text generation. The authors propose \"DiffuSeq,\" a diffusion model designed for sequence-to-sequence (Seq2Seq) text generation tasks, which directly aligns with the query's focus on diffusion models in text generation domains. Additionally, the paper discusses the adaptation of diffusion models to natural language, evaluates their performance on Seq2Seq tasks, and highlights their potential for conditional language generation, further confirming its relevance to the query."}
{"input": "User Query: What work first showed that diffusion models outperform GANs in image synthesis?\nTitle: Diffusion Models Beat GANs on Image Synthesis\nAbstract:  Abstract We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128×\\times128, 4.59 on ImageNet 256×\\times256, and 7.72 on ImageNet 512×\\times512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256×\\times256 and 3.85 on ImageNet 512×\\times512. We release our code at https://github.com/openai/guided-diffusion. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Diffusion Models Beat GANs on Image Synthesis\" directly addresses the user query. It explicitly demonstrates that diffusion models can outperform GANs in image synthesis, as indicated by the title and supported by the abstract. The abstract highlights that diffusion models achieve superior image sample quality compared to state-of-the-art generative models (which include GANs) and provides specific metrics (e.g., FID scores) to substantiate this claim. Additionally, the paper discusses improvements in both unconditional and conditional image synthesis, further confirming its relevance to the query. Therefore, this paper satisfies the requirements of the user query."}
{"input": "User Query: Could you list some studies that used neural networks to parameterize the Q-function?\nTitle: Off-Policy Deep Reinforcement Learning without Exploration\nAbstract:  Abstract Many practical applications of reinforcement learning constrain agents to learn from a fixed batch of data which has already been gathered, without offering further possibility for data collection. In this paper, we demonstrate that due to errors introduced by extrapolation, standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, are incapable of learning without data correlated to the distribution under the current policy, making them ineffective for this fixed batch setting. We introduce a novel class of off-policy algorithms, batch-constrained reinforcement learning, which restricts the action space in order to force the agent towards behaving close to on-policy with respect to a subset of the given data. We present the first continuous control deep reinforcement learning algorithm which can learn effectively from arbitrary, fixed batch data, and empirically demonstrate the quality of its behavior in several tasks. ", "output": "**Decision:** True  \n**Reason:** The searched paper discusses reinforcement learning algorithms, specifically focusing on off-policy deep reinforcement learning. It mentions standard algorithms like DQN (Deep Q-Networks) and DDPG (Deep Deterministic Policy Gradient), both of which are well-known for using neural networks to parameterize the Q-function. The paper also introduces a novel class of algorithms for batch-constrained reinforcement learning, which is likely to involve neural networks for Q-function approximation, as it builds upon existing deep reinforcement learning methods. Therefore, the paper satisfies the user's query requirements."}
{"input": "User Query: Could you provide me some examples of research that focused on transferring knowledge from CLIP models to OOD scenarios?\nTitle: A Sentence Speaks a Thousand Images: Domain Generalization through Distilling CLIP with Language Guidance\nAbstract:  Abstract Domain generalization studies the problem of training a model with samples from several domains (or distributions) and then testing the model with samples from a new, unseen domain. In this paper, we propose a novel approach for domain generalization that leverages recent advances in large vision-language models, specifically a CLIP teacher model, to train a smaller model that generalizes to unseen domains. The key technical contribution is a new type of regularization that requires the student’s learned image representations to be close to the teacher’s learned text representations obtained from encoding the corresponding text descriptions of images. We introduce two designs of the loss function, absolute and relative distance, which provide specific guidance on how the training process of the student model should be regularized. We evaluate our proposed method, dubbed  RISE (Regularized Invariance with Semantic Embeddings), on various benchmark datasets, and show that it outperforms several state-of-the-art domain generalization methods. To our knowledge, our work is the first to leverage knowledge distillation using a large vision-language model for domain generalization. By incorporating text-based information,  RISE improves the generalization capability of machine learning models. ", "output": "**Decision:** True  \n**Reason:** The searched paper aligns with the user query as it focuses on transferring knowledge from CLIP models to out-of-distribution (OOD) scenarios, specifically in the context of domain generalization. The paper proposes a method (RISE) that leverages a CLIP teacher model to train a smaller student model, enabling it to generalize to unseen domains. This directly addresses the concept of transferring knowledge from CLIP to OOD scenarios. Additionally, the paper highlights the use of CLIP's vision-language capabilities and introduces novel regularization techniques to improve generalization, further satisfying the requirements of the query."}
{"input": "User Query: Which works improve zero-shot transfer by utilizing GPT-3 to generate rich context descriptions?\nTitle: Visual Classification via Description from Large Language Models\nAbstract:  Abstract Vision-language models (VLMs) such as CLIP have shown promising performance on a variety of recognition tasks using the standard zero-shot classification procedure – computing similarity between the query image and the embedded words for each category. By only using the category name, they neglect to make use of the rich context of additional information that language affords. The procedure gives no intermediate understanding of why a category is chosen, and furthermore provides no mechanism for adjusting the criteria used towards this decision. We present an alternative framework for classification with VLMs, which we call classification by description. We ask VLMs to check for descriptive features rather than broad categories: to find a tiger, look for its stripes; its claws; and more. By basing decisions on these descriptors, we can provide additional cues that encourage using the features we want to be used. In the process, we can get a clear idea of what features the model uses to construct its decision; it gains some level of inherent explainability. We query large language models (e.g., GPT-3) for these descriptors to obtain them in a scalable way. Extensive experiments show our framework has numerous advantages past interpretability. We show improvements in accuracy on ImageNet across distribution shifts; demonstrate the ability to adapt VLMs to recognize concepts unseen during training; and illustrate how descriptors can be edited to effectively mitigate bias compared to the baseline. ", "output": "**Decision:** True  \n**Reason:** The paper \"Visual Classification via Description from Large Language Models\" satisfies the user query as it specifically addresses improving zero-shot transfer by utilizing GPT-3 to generate rich context descriptions. The abstract clearly states that the authors use GPT-3 to generate descriptive features (e.g., \"to find a tiger, look for its stripes; its claws; and more\") rather than relying solely on category names, which enhances the richness of the context. This approach directly contributes to improving zero-shot transfer by leveraging the descriptive capabilities of GPT-3. Additionally, the paper demonstrates improvements in accuracy on ImageNet across distribution shifts and the ability to adapt to unseen concepts, which aligns with the goal of enhancing zero-shot transfer."}
{"input": "User Query: Which work proposed a novel approach to model dynamic scenes at a large scale using a scene flow field representation?\nTitle: Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes\nAbstract:  Abstract We present a method to perform novel view and time synthesis of dynamic scenes, requiring only a monocular video with known camera poses as input. To do this, we introduce Neural Scene Flow Fields, a new representation that models the dynamic scene as a time-variant continuous function of appearance, geometry, and 3D scene motion. Our representation is optimized through a neural network to fit the observed input views. We show that our representation can be used for varieties of in-the-wild scenes, including thin structures, view-dependent effects, and complex degrees of motion. We conduct a number of experiments that demonstrate our approach significantly outperforms recent monocular view synthesis methods, and show qualitative results of space-time view synthesis on a variety of real-world videos. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Neural Scene Flow Fields for Space-Time View Synthesis of Dynamic Scenes\" fully satisfies the requirements of the user query. The paper explicitly proposes a novel approach to model dynamic scenes using a \"Neural Scene Flow Fields\" representation. This representation is described as a time-variant continuous function that captures appearance, geometry, and 3D scene motion, which aligns with the concept of a \"scene flow field representation.\" Furthermore, the method is designed to handle dynamic scenes at a large scale, as evidenced by its application to \"varieties of in-the-wild scenes\" and its ability to model complex motion and structures. Thus, the paper matches the query's focus on a novel approach for modeling dynamic scenes using a scene flow field representation."}
{"input": "User Query: Which papers studied the use of reinforcement learning from human feedback (RLHF) for aligning language models with human values?\nTitle: Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\nAbstract:  Abstract We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\" directly addresses the use of reinforcement learning from human feedback (RLHF) to align language models with human values. The abstract explicitly mentions the application of RLHF to fine-tune language models to act as \"helpful and harmless assistants,\" which aligns with the goal of aligning models with human values. Additionally, the paper discusses preference modeling, iterated training with human feedback, and the robustness of RLHF training, all of which are relevant to the query. Therefore, the paper fully satisfies the requirements of the user's query."}
{"input": "User Query: Which papers involve RETROfitting and Memorizing Transformer for adding new capabilities to existing LMs?\nTitle: Memorizing Transformers\nAbstract:  Abstract Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights. We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate k𝑘kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. On benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Memorizing Transformers\" satisfies the detailed requirements of the user query. The user is looking for papers that involve RETROfitting and Memorizing Transformers for adding new capabilities to existing language models (LMs). While the paper does not explicitly mention RETROfitting, it directly discusses the concept of Memorizing Transformers, which is a core part of the query. The paper describes a method to extend language models with the ability to memorize internal representations of past inputs using a non-differentiable memory mechanism. This approach allows the model to acquire new knowledge at inference time without retraining or fine-tuning, effectively adding new capabilities to existing LMs. The experiments demonstrate the model's ability to handle new data and tasks, such as using newly defined functions and theorems during test time, aligning with the user's interest in adding new capabilities to LMs."}
{"input": "User Query: What research works extended the use of Swin Transformer for 3D images?\nTitle: Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images\nAbstract:  Abstract Semantic segmentation of brain tumors is a fundamental medical image analysis task involving multiple MRI imaging modalities that can assist clinicians in diagnosing the patient and successively studying the progression of the malignant entity. In recent years, Fully Convolutional Neural Networks (FCNNs) approaches have become the de facto standard for 3D medical image segmentation. The popular “U-shaped” network architecture has achieved state-of-the-art performance benchmarks on different 2D and 3D semantic segmentation tasks and across various imaging modalities. However, due to the limited kernel size of convolution layers in FCNNs, their performance of modeling long-range information is sub-optimal, and this can lead to deficiencies in the segmentation of tumors with variable sizes. On the other hand, transformer models have demonstrated excellent capabilities in capturing such long-range information in multiple domains, including natural language processing and computer vision. Inspired by the success of vision transformers and their variants, we propose a novel segmentation model termed Swin UNEt TRansformers (Swin UNETR). Specifically, the task of 3D brain tumor semantic segmentation is reformulated as a sequence to sequence prediction problem wherein multi-modal input data is projected into a 1D sequence of embedding and used as an input to a hierarchical Swin transformer as the encoder. The swin transformer encoder extracts features at five different resolutions by utilizing shifted windows for computing self-attention and is connected to an FCNN-based decoder at each resolution via skip connections. We have participated in BraTS 2021 segmentation challenge, and our proposed model ranks among the top-performing approaches in the validation phase.  Code: https://monai.io/research/swin-unetr ", "output": "**Decision:** True  \n**Reason:** The paper \"Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images\" satisfies the user query as it specifically extends the use of Swin Transformers to 3D images. The paper reformulates the task of 3D brain tumor semantic segmentation as a sequence-to-sequence prediction problem and employs a Swin Transformer as the encoder for feature extraction. The Swin Transformer is adapted to process 3D medical images (multi-modal MRI data) by projecting them into a 1D sequence of embeddings and utilizing hierarchical feature extraction. Furthermore, the model incorporates a 3D segmentation framework by connecting the Swin Transformer encoder to an FCNN-based decoder via skip connections. This work directly contributes to extending the application of Swin Transformers to 3D image tasks, aligning with the requirements of the user query."}
{"input": "User Query: Which works are about generalizing over linear function approximations in reinforcement learning?\nTitle: Bilinear Classes: A Structural Framework for Provable Generalization in RL\nAbstract:  Abstract This work introduces Bilinear Classes, a new structural framework, which permit generalization in reinforcement learning in a wide variety of settings through the use of function approximation. The framework incorporates nearly all existing models in which a polynomial sample complexity is achievable, and, notably, also includes new models, such as the Linear Q∗/V∗superscript𝑄superscript𝑉Q^{*}/V^{*} model in which both the optimal Q𝑄Q-function and the optimal V𝑉V-function are linear in some known feature space. Our main result provides an RL algorithm which has polynomial sample complexity for Bilinear Classes; notably, this sample complexity is stated in terms of a reduction to the generalization error of an underlying supervised learning sub-problem. These bounds nearly match the best known sample complexity bounds for existing models. Furthermore, this framework also extends to the infinite dimensional (RKHS) setting: for the the Linear Q∗/V∗superscript𝑄superscript𝑉Q^{*}/V^{*} model, linear MDPs, and linear mixture MDPs, we provide sample complexities that have no explicit dependence on the explicit feature dimension (which could be infinite), but instead depends only on information theoretic quantities. ", "output": "**Decision:** True  \n**Reason:** The paper \"Bilinear Classes: A Structural Framework for Provable Generalization in RL\" satisfies the user query. It specifically addresses generalization in reinforcement learning (RL) using function approximation. The abstract explicitly mentions the inclusion of models such as the \"Linear Q*/V* model,\" where both the optimal Q-function and V-function are linear in a known feature space. This directly aligns with the query's focus on generalizing over linear function approximations in RL. Additionally, the paper discusses sample complexity bounds and generalization in the context of linear models, further supporting its relevance to the query."}
{"input": "User Query: What works mentioned learning the reverse covariance as part of the training-based methods?\nTitle: Estimating the Optimal Covariance with Imperfect Mean in Diffusion Probabilistic Models\nAbstract:  Abstract Diffusion probabilistic models (DPMs) are a class of powerful deep generative models (DGMs). Despite their success, the iterative generation process over the full timesteps is much less efficient than other DGMs such as GANs. Thus, the generation performance on a subset of timesteps is crucial, which is greatly influenced by the covariance design in DPMs. In this work, we consider diagonal and full covariances to improve the expressive power of DPMs. We derive the optimal result for such covariances, and then correct it when the mean of DPMs is imperfect. Both the optimal and the corrected ones can be decomposed into terms of conditional expectations over functions of noise. Building upon it, we propose to estimate the optimal covariance and its correction given imperfect mean by learning these conditional expectations. Our method can be applied to DPMs with both discrete and continuous timesteps. We consider the diagonal covariance in our implementation for computational efficiency. For an efficient practical implementation, we adopt a parameter sharing scheme and a two-stage training process. Empirically, our method outperforms a wide variety of covariance design on likelihood results, and improves the sample quality especially on a small number of timesteps. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Estimating the Optimal Covariance with Imperfect Mean in Diffusion Probabilistic Models\" explicitly discusses learning the covariance in diffusion probabilistic models (DPMs) as part of its methodology. The abstract mentions that the authors derive the optimal covariance and propose a method to estimate it, even when the mean is imperfect. This estimation process involves learning conditional expectations, which is a training-based approach. Furthermore, the paper's focus on improving the covariance design and its correction aligns with the query about works that involve learning the reverse covariance as part of training-based methods. Therefore, the paper satisfies the detailed requirements of the user query."}
{"input": "User Query: Which works have developed Vision Transformers that used feature maps at multiple spatial scales?\nTitle: Multiscale Vision Transformers\nAbstract:  Abstract We present Multiscale Vision Transformers (MViT) for video and image recognition, by connecting the seminal idea of multiscale feature hierarchies with transformer models. Multiscale Transformers have several channel-resolution scale stages. Starting from the input resolution and a small channel dimension, the stages hierarchically expand the channel capacity while reducing the spatial resolution. This creates a multiscale pyramid of features with early layers operating at high spatial resolution to model simple low-level visual information, and deeper layers at spatially coarse, but complex, high-dimensional features. We evaluate this fundamental architectural prior for modeling the dense nature of visual signals for a variety of video recognition tasks where it outperforms concurrent vision transformers that rely on large scale external pre-training and are 5-10×\\times more costly in computation and parameters. We further remove the temporal dimension and apply our model for image classification where it outperforms prior work on vision transformers. Code is available at: https://github.com/facebookresearch/SlowFast. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Multiscale Vision Transformers\" explicitly describes the development of Vision Transformers that utilize feature maps at multiple spatial scales. The abstract mentions that the model creates a \"multiscale pyramid of features,\" where the architecture hierarchically processes visual information at different spatial resolutions. Early layers operate at high spatial resolution for low-level features, while deeper layers handle coarse, high-dimensional features. This aligns directly with the user query about Vision Transformers using feature maps at multiple spatial scales."}
{"input": "User Query: What papers contributes to specification-based methods where they sparsely select part of the foundation model parameters for adjustment and freeze other parameters?\nTitle: What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning\nAbstract:  Abstract Pretrained transformer-based language models have achieved state of the art across countless tasks in natural language processing. These models are highly expressive, comprising at least a hundred million parameters and a dozen layers. Recent evidence suggests that only a few of the final layers need to be fine-tuned for high quality on downstream tasks. Naturally, a subsequent research question is, “how many of the last layers do we need to fine-tune?” In this paper, we precisely answer this question. We examine two recent pretrained language models, BERT and RoBERTa, across standard tasks in textual entailment, semantic similarity, sentiment analysis, and linguistic acceptability. We vary the number of final layers that are fine-tuned, then study the resulting change in task-specific effectiveness. We show that only a fourth of the final layers need to be fine-tuned to achieve 90% of the original quality. Surprisingly, we also find that fine-tuning all layers does not always help. ", "output": "**Decision:** True  \n**Reason:** The paper \"What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning\" directly addresses the concept of sparsely selecting parts of a foundation model's parameters for adjustment while freezing others. The study investigates fine-tuning only a subset of the final layers of pretrained transformer-based models (e.g., BERT and RoBERTa) and evaluates the impact on downstream task performance. This aligns with the user's query about specification-based methods, as the paper explores the selective adjustment of parameters by freezing certain layers and fine-tuning others. The findings, such as achieving 90% of the original quality by fine-tuning only a fourth of the layers, contribute to the understanding and methodology of sparse parameter selection in foundation models."}
{"input": "User Query: What papers present BOBW algorithm with the data-dependent bound?\nTitle: Adversarially Robust Multi-Armed Bandit Algorithm with Variance-Dependent Regret Bounds\nAbstract:  Abstract This paper considers the multi-armed bandit (MAB) problem and provides a new best-of-both-worlds (BOBW) algorithm that works nearly optimally in both stochastic and adversarial settings. In stochastic settings, some existing BOBW algorithms achieve tight gap-dependent regret bounds of O​(∑i:Δi>0log⁡TΔi)𝑂subscript:𝑖subscriptΔ𝑖0𝑇subscriptΔ𝑖O(\\sum_{i:\\Delta_{i}>0}\\frac{\\log T}{\\Delta_{i}}) for suboptimality gap ΔisubscriptΔ𝑖\\Delta_{i} of arm i𝑖i and time horizon T𝑇T. As Audibert et al. (2007) have shown, however, that the performance can be improved in stochastic environments with low-variance arms. In fact, they have provided a stochastic MAB algorithm with gap-variance-dependent regret bounds of O​(∑i:Δi>0(σi2Δi+1)​log⁡T)𝑂subscript:𝑖subscriptΔ𝑖0superscriptsubscript𝜎𝑖2subscriptΔ𝑖1𝑇O(\\sum_{i:\\Delta_{i}>0}(\\frac{\\sigma_{i}^{2}}{\\Delta_{i}}+1)\\log T) for loss variance σi2superscriptsubscript𝜎𝑖2\\sigma_{i}^{2} of arm i𝑖i. In this paper, we propose the first BOBW algorithm with gap-variance-dependent bounds, showing that the variance information can be used even in the possibly adversarial environment. Further, the leading constant factor in our gap-variance dependent bound is only (almost) twice the value for the lower bound. Additionally, the proposed algorithm enjoys multiple data-dependent regret bounds in adversarial settings and works well in stochastic settings with adversarial corruptions. The proposed algorithm is based on the follow-the-regularized-leader method and employs adaptive learning rates that depend on the empirical prediction error of the loss, which leads to gap-variance-dependent regret bounds reflecting the variance of the arms. ", "output": "**Decision:** True  \n**Reason:** The searched paper satisfies the requirements of the user query. The paper explicitly presents a Best-of-Both-Worlds (BOBW) algorithm with data-dependent bounds. Specifically, it introduces a BOBW algorithm that achieves gap-variance-dependent regret bounds, which are data-dependent as they incorporate the variance of the arms (\\(\\sigma_i^2\\)) and the suboptimality gap (\\(\\Delta_i\\)). Additionally, the paper highlights that the algorithm works well in both stochastic and adversarial settings, aligning with the BOBW framework. Therefore, the paper fully addresses the query's requirements."}
{"input": "User Query: Which works studied MDPs under a weaker assumption known as low inherent Bellman error?\nTitle: Provably Efficient Reward-Agnostic Navigation with Linear Value Iteration\nAbstract:  Abstract There has been growing progress on theoretical analyses for provably efficient learning in MDPs with linear function approximation, but much of the existing work has made strong assumptions to enable exploration by conventional exploration frameworks. Typically these assumptions are stronger than what is needed to find good solutions in the batch setting. In this work, we show how under a more standard notion of low inherent Bellman error, typically employed in least-square value iteration-style algorithms, we can provide strong PAC guarantees on learning a near optimal value function provided that the linear space is sufficiently “explorable”. We present a computationally tractable algorithm for the reward-free setting and show how it can be used to learn a near optimal policy for any (linear) reward function, which is revealed only once learning has completed. If this reward function is also estimated from the samples gathered during pure exploration, our results also provide same-order PAC guarantees on the performance of the resulting policy for this setting. ", "output": "**Decision:** True  \n**Reason:** The searched paper explicitly discusses studying MDPs under the weaker assumption of low inherent Bellman error. The abstract mentions that the work provides theoretical analyses for learning in MDPs with linear function approximation under the assumption of low inherent Bellman error. This aligns with the user query, as the paper focuses on this specific assumption and its implications for learning and exploration in MDPs. Therefore, the paper satisfies the detailed requirements of the query."}
{"input": "User Query: What work incorporated the usage of Matryoshka Representations for adaptive representations in ANNS?\nTitle: Matryoshka Representation Learning\nAbstract:  Abstract Learned representations are a central component in modern ML systems, serving a multitude of downstream tasks. When training such representations, it is often the case that computational and statistical constraints for each downstream task are unknown. In this context, rigid fixed-capacity representations can be either over or under-accommodating to the task at hand. This leads us to ask: can we design a flexible representation that can adapt to multiple downstream tasks with varying computational resources? Our main contribution is  Matryoshka​Representation​LearningMatryoshkaRepresentationLearning{\\rm Matryoshka~{}Representation~{}Learning} (MRLMRL{\\rm MRL}) which encodes information at different granularities and allows a single embedding to adapt to the computational constraints of downstream tasks. MRLMRL{\\rm MRL} minimally modifies existing representation learning pipelines and imposes no additional cost during inference and deployment. MRLMRL{\\rm MRL} learns coarse-to-fine representations that are at least as accurate and rich as independently trained low-dimensional representations. The flexibility within the learned Matryoshka​RepresentationsMatryoshkaRepresentations{\\rm Matryoshka~{}Representations} offer: (a) up to 𝟏𝟒×\\mathbf{14}\\times smaller embedding size for ImageNet-1K classification at the same level of accuracy; (b) up to 𝟏𝟒×\\mathbf{14}\\times real-world speed-ups for large-scale retrieval on ImageNet-1K and 4K; and (c) up to 𝟐2\\mathbf{2}% accuracy improvements for long-tail few-shot classification, all while being as robust as the original representations. Finally, we show that MRLMRL{\\rm MRL} extends seamlessly to web-scale datasets (ImageNet, JFT) across various modalities – vision (ViT, ResNet), vision + language (ALIGN) and language (BERT). MRLMRL{\\rm MRL} code and pretrained models are open-sourced at https://github.com/RAIVNLab/MRL. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Matryoshka Representation Learning\" explicitly discusses the usage of Matryoshka Representations for adaptive representations. The abstract highlights that the proposed method, Matryoshka Representation Learning (MRL), encodes information at different granularities and allows a single embedding to adapt to varying computational constraints of downstream tasks. This aligns with the user's query regarding the incorporation of Matryoshka Representations for adaptive representations. Additionally, the paper demonstrates the application of these representations in various tasks, including large-scale retrieval and classification, which are relevant to adaptive representations in Artificial Neural Networks (ANNs). Thus, the paper fully satisfies the detailed requirements of the user query."}
{"input": "User Query: Which works explored re-calibrating batch normalization statistics to make deep neural networks robust to image corruptions?\nTitle: Evaluating Prediction-Time Batch Normalization for Robustness under Covariate Shift\nAbstract:  Abstract Covariate shift has been shown to sharply degrade both predictive accuracy and the calibration of uncertainty estimates for deep learning models. This is worrying, because covariate shift is prevalent in a wide range of real world deployment settings. However, in this paper, we note that frequently there exists the potential to access small unlabeled batches of the shifted data just before prediction time. This interesting observation enables a simple but surprisingly effective method which we call prediction-time batch normalization, which significantly improves model accuracy and calibration under covariate shift. Using this one line code change, we achieve state-of-the-art on recent covariate shift benchmarks and an mCE of 60.28% on the challenging ImageNet-C dataset; to our knowledge, this is the best result for any model that does not incorporate additional data augmentation or modification of the training pipeline. We show that prediction-time batch normalization provides complementary benefits to existing state-of-the-art approaches for improving robustness (e.g. deep ensembles) and combining the two further improves performance. Our findings are supported by detailed measurements of the effect of this strategy on model behavior across rigorous ablations on various dataset modalities. However, the method has mixed results when used alongside pre-training, and does not seem to perform as well under more natural types of dataset shift, and is therefore worthy of additional study. We include links to the data in our figures to improve reproducibility, including a Python notebooks that can be run to easily modify our analysis at this url. ", "output": "**Decision:** True  \n**Reason:** The searched paper, titled *\"Evaluating Prediction-Time Batch Normalization for Robustness under Covariate Shift,\"* directly addresses the concept of re-calibrating batch normalization statistics to improve the robustness of deep neural networks under covariate shift. The method proposed in the paper, termed \"prediction-time batch normalization,\" involves recalibrating batch normalization statistics using small, unlabeled batches of shifted data at prediction time. This approach is shown to significantly enhance model robustness and accuracy, particularly under image corruptions, as evidenced by the results on the challenging ImageNet-C dataset. Therefore, the paper satisfies the requirements of the user query by exploring a method to re-calibrate batch normalization statistics to make deep neural networks robust to image corruptions."}
{"input": "User Query: What works have addressed the task of annotation-free object segmentation?\nTitle: GENESIS-V2: Inferring Unordered Object Representations without Iterative Refinement\nAbstract:  Abstract Advances in unsupervised learning of object-representations have culminated in the development of a broad range of methods for unsupervised object segmentation and interpretable object-centric scene generation. These methods, however, are limited to simulated and real-world datasets with limited visual complexity. Moreover, object representations are often inferred using RNNs which do not scale well to large images or iterative refinement which avoids imposing an unnatural ordering on objects in an image but requires the a priori initialisation of a fixed number of object representations. In contrast to established paradigms, this work proposes an embedding-based approach in which embeddings of pixels are clustered in a differentiable fashion using a stochastic stick-breaking process. Similar to iterative refinement, this clustering procedure also leads to randomly ordered object representations, but without the need of initialising a fixed number of clusters a priori. This is used to develop a new model, genesis-v2, which can infer a variable number of object representations without using RNNs or iterative refinement. We show that genesis-v2 performs strongly in comparison to recent baselines in terms of unsupervised image segmentation and object-centric scene generation on established synthetic datasets as well as more complex real-world datasets. ", "output": "**Decision:** True  \n**Reason:** The paper \"GENESIS-V2: Inferring Unordered Object Representations without Iterative Refinement\" directly addresses the task of annotation-free object segmentation. The abstract explicitly states that the work focuses on \"unsupervised image segmentation,\" which inherently aligns with the concept of annotation-free segmentation since no labeled data is used. The method proposed, GENESIS-V2, employs a clustering-based approach to infer object representations without requiring annotations, making it relevant to the user query. Additionally, the paper highlights its application to both synthetic and real-world datasets, further supporting its relevance to the task of annotation-free object segmentation."}
{"input": "User Query: What work was used as a starting point for exploration and finetuning in Minecraft?\nTitle: Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos\nAbstract:  Abstract Pretraining on noisy, internet-scale datasets has been heavily studied as a technique for training models with broad, general capabilities for text, images, and other modalities.1, 2, 3, 4, 5, 6 However, for many sequential decision domains such as robotics, video games, and computer use, publicly available data does not contain the labels required to train behavioral priors in the same way. We extend the internet-scale pretraining paradigm to sequential decision domains through semi-supervised imitation learning wherein agents learn to act by watching online unlabeled videos. Specifically, we show that with a small amount of labeled data we can train an inverse dynamics model accurate enough to label a huge unlabeled source of online data – here, online videos of people playing Minecraft – from which we can then train a general behavioral prior. Despite using the native human interface (mouse and keyboard at 20Hz), we show that this behavioral prior has nontrivial zero-shot capabilities and that it can be fine-tuned, with both imitation learning and reinforcement learning, to hard-exploration tasks that are impossible to learn from scratch via reinforcement learning. For many tasks our models exhibit human-level performance, and we are the first to report computer agents that can craft diamond tools, which can take proficient humans upwards of 20 minutes (24,000 environment actions) of gameplay to accomplish. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos\" directly addresses the user's query. It describes a method where online videos of people playing Minecraft were used as a starting point for exploration and fine-tuning. The paper outlines how a small amount of labeled data was used to train an inverse dynamics model, which then labeled a large dataset of unlabeled Minecraft gameplay videos. This labeled dataset was subsequently used to train a behavioral prior, which was fine-tuned using imitation learning and reinforcement learning to achieve human-level performance in Minecraft. The paper explicitly focuses on Minecraft as the domain of exploration and fine-tuning, making it fully relevant to the user's query."}
{"input": "User Query: Could you cite a research paper that investigated deconfounding scores in the context of causal inference?\nTitle: Deconfounding Scores: Feature Representations for Causal Effect Estimation with Weak Overlap\nAbstract: A key condition for obtaining reliable estimates of the causal effect of a treatment is overlap (a.k.a. positivity): the distributions of the features used to perform causal adjustment cannot be too different in the treated and control groups. In cases where overlap is poor, causal effect estimators can become brittle, especially when they incorporate weighting. To address this problem, a number of proposals (including confounder selection or dimension reduction methods) incorporate feature representations to induce better overlap between the treated and control groups. A key concern in these proposals is that the representation may introduce confounding bias into the effect estimator. In this paper, we introduce deconfounding scores, which are feature representations that induce better overlap without biasing the target of estimation. We show that deconfounding scores satisfy a zero-covariance condition that is identifiable in observed data. As a proof of concept, we characterize a family of deconfounding scores in a simplified setting with Gaussian covariates, and show that in some simple simulations, these scores can be used to construct estimators with good finite-sample properties. In particular, we show that this technique could be an attractive alternative to standard regularizations that are often applied to IPW and balancing weights.", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Deconfounding Scores: Feature Representations for Causal Effect Estimation with Weak Overlap\" directly investigates deconfounding scores in the context of causal inference. The abstract explicitly discusses the concept of deconfounding scores as feature representations designed to improve overlap between treated and control groups while avoiding confounding bias. This aligns with the user's query, which specifically asks for a research paper that examines deconfounding scores in the context of causal inference. Therefore, the paper fully satisfies the requirements of the query."}
{"input": "User Query: Which papers examine the two-timescale/asymmetric gradient descent/ascent methods under the PŁcondition?\nTitle: Solving a Class of Non-Convex Min-Max Games Using Iterative First Order Methods\nAbstract:  Abstract Recent applications that arise in machine learning have surged significant interest in solving min-max saddle point games. This problem has been extensively studied in the convex-concave regime for which a global equilibrium solution can be computed efficiently. In this paper, we study the problem in the non-convex regime and show that an ε𝜀\\varepsilon–first order stationary point of the game can be computed when one of the player’s objective can be optimized to global optimality efficiently. In particular, we first consider the case where the objective of one of the players satisfies the Polyak-Łojasiewicz (PL) condition. For such a game, we show that a simple multi-step gradient descent-ascent algorithm finds an ε𝜀\\varepsilon–first order stationary point of the problem in 𝒪~​(ε−2)~𝒪superscript𝜀2\\widetilde{\\mathcal{O}}(\\varepsilon^{-2}) iterations. Then we show that our framework can also be applied to the case where the objective of the “max-player\" is concave. In this case, we propose a multi-step gradient descent-ascent algorithm that finds an ε𝜀\\varepsilon–first order stationary point of the game in 𝒪~​(ε−3.5)~𝒪superscript𝜀3.5\\widetilde{\\cal O}(\\varepsilon^{-3.5}) iterations, which is the best known rate in the literature. We applied our algorithm to a fair classification problem of Fashion-MNIST dataset and observed that the proposed algorithm results in smoother training and better generalization. ", "output": "**Decision:** True  \n**Reason:** The paper explicitly examines gradient descent-ascent methods under the Polyak-Łojasiewicz (PL) condition, as stated in the abstract. It focuses on solving non-convex min-max games and proposes a multi-step gradient descent-ascent algorithm for cases where one player's objective satisfies the PL condition. This aligns with the user's query about two-timescale/asymmetric gradient descent/ascent methods under the PL condition. While the abstract does not explicitly mention \"two-timescale\" or \"asymmetric\" methods, the use of gradient descent-ascent methods inherently involves an asymmetric approach, as the two players (min and max) are treated differently. Therefore, the paper satisfies the detailed requirements of the user query."}
{"input": "User Query: Could you name the research that employs techniques such as word lists, blocklists of URLs, and Google Safe Search for removing toxic and harmful content during data cleaning process?\nTitle: The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only\nAbstract:  Abstract Large language models are commonly trained on a mixture of filtered web data and curated “high-quality” corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it***Details about how to access Falcon LLM open source is available on falconllm.tii.ae. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only\"* satisfies the requirements of the user query. The paper discusses the creation of the RefinedWeb dataset, which involves extensive filtering and deduplication of web data to produce high-quality training data for language models. Techniques such as word lists, blocklists of URLs, and Google Safe Search are commonly employed in filtering processes to remove toxic and harmful content, and the abstract explicitly mentions the focus on \"properly filtered\" web data. While the abstract does not explicitly detail the exact filtering techniques used, the context strongly implies the use of such standard methods. Therefore, the paper aligns with the query's requirements."}
{"input": "User Query: Which works primarily focused on translating paragraphs or documents at once?\nTitle: A Survey on Document-level Neural Machine Translation: Methods and Evaluation\nAbstract:  Abstract Machine translation (MT) is an important task in natural language processing (NLP) as it automates the translation process and reduces the reliance on human translators. With the resurgence of neural networks, the translation quality surpasses that of the translations obtained using statistical techniques for most language-pairs. Up until a few years ago, almost all of the neural translation models translated sentences independently, without incorporating the wider document-context and inter-dependencies among the sentences. The aim of this survey paper is to highlight the major works that have been undertaken in the space of document-level machine translation after the neural revolution, so that researchers can recognise the current state and future directions of this field. We provide an organisation of the literature based on novelties in modelling and architectures as well as training and decoding strategies. In addition, we cover evaluation strategies that have been introduced to account for the improvements in document MT, including automatic metrics and discourse-targeted test sets. We conclude by presenting possible avenues for future exploration in this research field.  ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"A Survey on Document-level Neural Machine Translation: Methods and Evaluation\"* fully satisfies the user query. The abstract explicitly states that the survey focuses on document-level machine translation (MT), which inherently involves translating paragraphs or documents at once, as opposed to sentence-level translation. The paper highlights major works in this area, discussing advancements in modeling, architectures, training, and decoding strategies specific to document-level MT. Additionally, it addresses evaluation strategies tailored to improvements in document MT. Therefore, the paper aligns well with the user's interest in works that primarily focus on translating paragraphs or documents at once."}
{"input": "User Query: What studies use point-based 3D detectors in LiDAR-based 3D object detection?\nTitle: SASA: Semantics-Augmented Set Abstraction for Point-based 3D Object Detection\nAbstract:  Abstract Although point-based networks are demonstrated to be accurate for 3D point cloud modeling, they are still falling behind their voxel-based competitors in 3D detection. We observe that the prevailing set abstraction design for down-sampling points may maintain too much unimportant background information that can affect feature learning for detecting objects. To tackle this issue, we propose a novel set abstraction method named Semantics-Augmented Set Abstraction (SASA). Technically, we first add a binary segmentation module as the side output to help identify foreground points. Based on the estimated point-wise foreground scores, we then propose a semantics-guided point sampling algorithm to help retain more important foreground points during down-sampling. In practice, SASA shows to be effective in identifying valuable points related to foreground objects and improving feature learning for point-based 3D detection. Additionally, it is an easy-to-plug-in module and able to boost various point-based detectors, including single-stage and two-stage ones. Extensive experiments on the popular KITTI and nuScenes datasets validate the superiority of SASA, lifting point-based detection models to reach comparable performance to state-of-the-art voxel-based methods. Code will be available at https://github.com/blakechen97/SASA. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"SASA: Semantics-Augmented Set Abstraction for Point-based 3D Object Detection\" fully satisfies the user query. The abstract explicitly mentions that the study focuses on point-based 3D detectors for LiDAR-based 3D object detection. It discusses the use of point-based networks, introduces a novel set abstraction method (SASA) to improve the performance of point-based detectors, and evaluates its effectiveness on popular LiDAR-based datasets like KITTI and nuScenes. Therefore, the paper is directly relevant to the query."}
{"input": "User Query: Can you tell me the papers which propose chain-of-thoughts prompting?\nTitle: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\nAbstract:  Abstract We explore how generating a chain of thought—a series of intermediate reasoning steps—significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-of-thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\" directly proposes the concept of chain-of-thought prompting. The abstract explicitly describes the method, where a series of intermediate reasoning steps (chain of thought) are used as exemplars in prompting to improve reasoning abilities in large language models. The paper introduces the method, demonstrates its effectiveness through experiments, and highlights its significant empirical gains. Therefore, it fully satisfies the user's query about papers proposing chain-of-thought prompting."}
{"input": "User Query: What study accelerates the generative process of diffusion models by discarding Markovian assumption?\nTitle: Denoising Diffusion Implicit Models\nAbstract:  Abstract Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples 10×10\\times to 50×50\\times faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Denoising Diffusion Implicit Models\" (DDIMs) directly addresses the user query. The study explicitly focuses on accelerating the generative process of diffusion models by discarding the Markovian assumption. The abstract describes how the authors generalize denoising diffusion probabilistic models (DDPMs) by introducing non-Markovian diffusion processes, which lead to the same training objective but enable faster sampling. By discarding the Markovian assumption, the proposed DDIMs achieve significant acceleration in the generative process (10× to 50× faster) while maintaining high sample quality. This aligns with the user's query about accelerating diffusion models by removing the Markovian assumption."}
{"input": "User Query: Which studies have reported on retina models for increasing adversarial robustness of the systems?\nTitle: Foveation-based Mechanisms Alleviate Adversarial Examples\nAbstract:  Abstract We show that adversarial examples, i.e., the visually imperceptible perturbations that result in Convolutional Neural Networks (CNNs) fail, can be alleviated with a mechanism based on foveations—applying the CNN in different image regions. To see this, first, we report results in ImageNet that lead to a revision of the hypothesis that adversarial perturbations are a consequence of CNNs acting as a linear classifier: CNNs act locally linearly to changes in the image regions with objects recognized by the CNN, and in other regions the CNN may act non-linearly. Then, we corroborate that when the neural responses are linear, applying the foveation mechanism to the adversarial example tends to significantly reduce the effect of the perturbation. This is because, hypothetically, the CNNs for ImageNet are robust to changes of scale and translation of the object produced by the foveation, but this property does not generalize to transformations of the perturbation. As a result, the accuracy after a foveation is almost the same as the accuracy of the CNN without the adversarial perturbation, even if the adversarial perturbation is calculated taking into account a foveation. ", "output": "**Decision:** True  \n**Reason:** The paper discusses a mechanism based on foveation, which is directly inspired by the human visual system, particularly the retina's foveal region responsible for high-acuity vision. The study explores how applying foveation-based mechanisms can alleviate adversarial examples in Convolutional Neural Networks (CNNs). This aligns with the user query, as the foveation mechanism can be considered a retina-inspired model aimed at increasing adversarial robustness. The paper explicitly investigates how these mechanisms enhance the system's robustness against adversarial perturbations, satisfying the detailed requirements of the query."}
{"input": "User Query: Which studies utilized programming languages in the field of robotics?\nTitle: Code as Policies: Language Model Programs for Embodied Control\nAbstract:  Abstract Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2, 3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (“faster”) depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8% of problems on the HumanEval[1] benchmark. Code and videos are available at https://code-as-policies.github.io ", "output": "**Decision:** True  \n**Reason:** The paper \"Code as Policies: Language Model Programs for Embodied Control\" fully satisfies the user query. The study explicitly discusses the use of programming languages in the field of robotics. It focuses on leveraging large language models (LLMs) trained on code-completion to generate robot policy code. The generated code is used to control robots by processing perception outputs, parameterizing control primitives, and implementing policies for tasks like vision-based pick and place or trajectory-based control. The paper demonstrates the use of programming languages (e.g., Python) and libraries (e.g., NumPy, Shapely) in creating robot policies, which directly aligns with the query about studies utilizing programming languages in robotics."}
{"input": "User Query: What papers discuss the fairness of an objective different from clustering in relation to suodular maximization?\nTitle: Fairness in Streaming Submodular Maximization: Algorithms and Hardness\nAbstract:  Abstract Submodular maximization has become established as the method of choice for the task of selecting representative and diverse summaries of data. However, if datapoints have sensitive attributes such as gender or age, such machine learning algorithms, left unchecked, are known to exhibit bias: under- or over-representation of particular groups. This has made the design of fair machine learning algorithms increasingly important. In this work we address the question: Is it possible to create fair summaries for massive datasets? To this end, we develop the first streaming approximation algorithms for submodular maximization under fairness constraints, for both monotone and non-monotone functions. We validate our findings empirically on exemplar-based clustering, movie recommendation, DPP-based summarization, and maximum coverage in social networks, showing that fairness constraints do not significantly impact utility. ", "output": "**Decision:** True  \n**Reason:** The paper \"Fairness in Streaming Submodular Maximization: Algorithms and Hardness\" satisfies the user query as it discusses fairness in submodular maximization for objectives beyond clustering. While the abstract mentions exemplar-based clustering as one of the applications, it also explicitly explores other objectives such as movie recommendation, DPP-based summarization, and maximum coverage in social networks. These objectives are different from clustering, and the paper investigates fairness constraints in these contexts. Therefore, the paper aligns with the user's request for discussions on fairness in objectives other than clustering in relation to submodular maximization."}
{"input": "User Query: What are the most recent state-of-the-art HIC methods?\nTitle: Leveraging Class Hierarchies with Metric-Guided Prototype Learning\nAbstract:  Abstract In many classification tasks, the set of target classes can be organized into a hierarchy. This structure induces a semantic distance between classes, and can be summarized under the form of a cost matrix, which defines a finite metric on the class set. In this paper, we propose to model the hierarchical class structure by integrating this metric in the supervision of a prototypical network. Our method relies on jointly learning a feature-extracting network and a set of class prototypes whose relative arrangement in the embedding space follows an hierarchical metric. We show that this approach allows for a consistent improvement of the error rate weighted by the cost matrix when compared to traditional methods and other prototype-based strategies. Furthermore, when the induced metric contains insight on the data structure, our method improves the overall precision as well. Experiments on four different public datasets—from agricultural time series classification to depth image semantic segmentation—validate our approach. ", "output": "**Decision:** True  \n**Reason:** The paper \"Leveraging Class Hierarchies with Metric-Guided Prototype Learning\" satisfies the user query about the most recent state-of-the-art Hierarchical Image Classification (HIC) methods. The paper explicitly addresses the use of hierarchical class structures by integrating a hierarchical metric into the supervision of a prototypical network. This approach is relevant to HIC as it leverages class hierarchies to improve classification performance. Additionally, the paper demonstrates improvements in error rates weighted by the cost matrix and overall precision, which are key metrics for evaluating HIC methods. The experiments conducted on diverse datasets further validate the proposed method, showcasing its applicability and effectiveness. Thus, the paper aligns with the user's interest in recent advancements in state-of-the-art HIC methods."}
{"input": "User Query: Which research papers focus on generating human motion?\nTitle: GANimator: Neural Motion Synthesis from a Single Sequence\nAbstract:  Abstract. We present GANimator, a generative model that learns to synthesize novel motions from a single, short motion sequence. GANimator generates motions that resemble the core elements of the original motion, while simultaneously synthesizing novel and diverse movements. Existing data-driven techniques for motion synthesis require a large motion dataset which contains the desired and specific skeletal structure. By contrast, GANimator only requires training on a single motion sequence, enabling novel motion synthesis for a variety of skeletal structures e.g., bipeds, quadropeds, hexapeds, and more. Our framework contains a series of generative and adversarial neural networks, each responsible for generating motions in a specific frame rate. The framework progressively learns to synthesize motion from random noise, enabling hierarchical control over the generated motion content across varying levels of detail. We show a number of applications, including crowd simulation, key-frame editing, style transfer, and interactive control, which all learn from a single input sequence. Code and data for this paper are at https://peizhuoli.github.io/ganimator. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"GANimator: Neural Motion Synthesis from a Single Sequence\" focuses on generating novel motions using a generative model. While the abstract mentions that the method can handle various skeletal structures (e.g., bipeds, quadrupeds, hexapeds), it does not exclude human motion. In fact, the framework is applicable to human motion synthesis, as humans are a subset of bipeds. The paper's applications, such as crowd simulation and style transfer, are also commonly associated with human motion. Therefore, the paper satisfies the user query regarding research on generating human motion."}
{"input": "User Query: Are there any studies showing that a deep BNN is equivalent to a GP with a compositional kernel?\nTitle: Deep Neural Networks as Gaussian Processes\nAbstract:  Abstract It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks. ", "output": "**Decision:** True  \n**Reason:** The paper \"Deep Neural Networks as Gaussian Processes\" directly addresses the user's query by deriving the equivalence between infinitely wide deep neural networks (DNNs) and Gaussian Processes (GPs). It explicitly discusses how kernel functions that mimic multi-layer (deep) random neural networks can serve as covariance functions for GPs, enabling fully Bayesian predictions. This aligns with the concept of a compositional kernel, as the kernels in deep GPs are inherently compositional due to the hierarchical structure of the layers. The paper also provides experimental evidence and theoretical connections to support this equivalence, satisfying the detailed requirements of the user query."}
{"input": "User Query: Which studies use a two-time scale update rule (TTUR) to push GANs’ training process towards converging to a local Nash equilibrium?\nTitle: GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium\nAbstract:  Abstract Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the ‘Fréchet Inception Distance” (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium\"* fully satisfies the requirements of the user query. The abstract explicitly states that the study proposes a two-time scale update rule (TTUR) for training GANs, with separate learning rates for the discriminator and generator. Furthermore, it proves that this TTUR approach leads to convergence to a stationary local Nash equilibrium under mild assumptions, which directly aligns with the user's query about studies using TTUR to push GAN training towards local Nash equilibrium. The paper also discusses the theoretical foundation and experimental results supporting the effectiveness of TTUR in GAN training, further confirming its relevance to the query."}
{"input": "User Query: Could you provide me some works that applied transfer-learning-based methods in few-shot learning?\nTitle: On the Importance of Distractors for Few-Shot Classification\nAbstract:  Abstract Few-shot classification aims at classifying categories of a novel task by learning from just a few (typically, 1 to 5) labelled examples. An effective approach to few-shot classification involves a prior model trained on a large-sample base domain, which is then finetuned over the novel few-shot task to yield generalizable representations. However, task-specific finetuning is prone to overfitting due to the lack of enough training examples. To alleviate this issue, we propose a new finetuning approach based on contrastive learning that reuses unlabelled examples from the base domain in the form of distractors. Unlike the nature of unlabelled data used in prior works, distractors belong to classes that do not overlap with the novel categories. We demonstrate for the first time that inclusion of such distractors can significantly boost few-shot generalization. Our technical novelty includes a stochastic pairing of examples sharing the same category in the few-shot task and a weighting term that controls the relative influence of task-specific negatives and distractors. An important aspect of our finetuning objective is that it is agnostic to distractor labels and hence applicable to various base domain settings. Compared to state-of-the-art approaches, our method shows accuracy gains of up to 12%percent1212\\% in cross-domain and up to 5%percent55\\% in unsupervised prior-learning settings. Our code is available at https://github.com/quantacode/Contrastive-Finetuning.git ", "output": "**Decision:** True  \n**Reason:** The paper satisfies the user query as it discusses a transfer-learning-based method applied to few-shot learning. Specifically, the paper outlines a method where a prior model is trained on a large-sample base domain and then fine-tuned on a novel few-shot task. This approach is a clear example of transfer learning, as it leverages knowledge from a base domain to improve performance on a new task with limited data. The proposed method also addresses challenges like overfitting in few-shot learning using a novel contrastive learning-based fine-tuning approach with distractors. Thus, the paper aligns with the requirements of applying transfer-learning-based methods in few-shot learning."}
{"input": "User Query: What works proposed a two-stage framework for depth estimation that estimates affine-invariant depth and then upgrades it to metric depth?\nTitle: Learning to Recover 3D Scene Shape from a Single Image\nAbstract:  Abstract Despite significant progress in monocular depth estimation in the wild, recent state-of-the-art methods cannot be used to recover accurate 3D scene shape due to an unknown depth shift induced by shift-invariant reconstruction losses used in mixed-data depth prediction training, and possible unknown camera focal length. We investigate this problem in detail, and propose a two-stage framework that first predicts depth up to an unknown scale and shift from a single monocular image, and then use 3D point cloud encoders to predict the missing depth shift and focal length that allow us to recover a realistic 3D scene shape. In addition, we propose an image-level normalized regression loss and a normal-based geometry loss to enhance depth prediction models trained on mixed datasets. We test our depth model on nine unseen datasets and achieve state-of-the-art performance on zero-shot dataset generalization. Code is available at:   https://git.io/Depth ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"Learning to Recover 3D Scene Shape from a Single Image\"* satisfies the user query. The paper explicitly proposes a two-stage framework for depth estimation. In the first stage, it predicts depth up to an unknown scale and shift (affine-invariant depth). In the second stage, it upgrades this depth to metric depth by predicting the missing depth shift and focal length using 3D point cloud encoders. This aligns with the user's query about a framework that estimates affine-invariant depth and then upgrades it to metric depth."}
{"input": "User Query: Which papers introduced fully supervised segmentation models that require pixel-wise segmentation?\nTitle: Fully Convolutional Networks for Semantic Segmentation\nAbstract:  Abstract Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [19], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [4] to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image. ", "output": "**Decision:** True  \n**Reason:** The paper \"Fully Convolutional Networks for Semantic Segmentation\" fully satisfies the requirements of the user query. It introduces a fully supervised segmentation model that requires pixel-wise segmentation. The abstract explicitly states that the model is trained end-to-end, from pixels to pixels, for semantic segmentation tasks. This indicates that the model requires pixel-level annotations for training, which is a hallmark of fully supervised segmentation. Furthermore, the paper describes how the fully convolutional network architecture is adapted for spatially dense prediction tasks, which aligns with the need for pixel-wise segmentation. Therefore, the paper meets the criteria outlined in the user query."}
{"input": "User Query: What research projects apply 2D projection-based methods for LiDAR semantic segmentation?\nTitle: PolarNet: An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation\nAbstract:  Abstract The need for fine-grained perception in autonomous driving systems has resulted in recently increased research on online semantic segmentation of single-scan LiDAR. Despite the emerging datasets and technological advancements, it remains challenging due to three reasons: (1) the need for near-real-time latency with limited hardware; (2) uneven or even long-tailed distribution of LiDAR points across space; and (3) an increasing number of extremely fine-grained semantic classes. In an attempt to jointly tackle all the aforementioned challenges, we propose a new LiDAR -specific, nearest-neighbor-free segmentation algorithm --- PolarNet. Instead of using common spherical or bird’s-eye-view projection, our polar bird’s-eye-view representation balances the points across grid cells in a polar coordinate system, indirectly aligning a segmentation network’s attention with the long-tailed distribution of the points along the radial axis. We find that our encoding scheme greatly increases the mIoU in three drastically different segmentation datasets of real urban LiDAR single scans while retaining near real-time throughput. ", "output": "**Decision:** True  \n**Reason:** The paper \"PolarNet: An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation\" fully satisfies the user query. The research explicitly applies a 2D projection-based method for LiDAR semantic segmentation. It introduces a polar bird’s-eye-view (BEV) representation, which is a 2D grid-based projection in a polar coordinate system. This approach is used to address challenges in LiDAR semantic segmentation, such as uneven point distribution and real-time processing requirements. The method aligns with the query's focus on research projects utilizing 2D projection-based techniques for LiDAR semantic segmentation."}
{"input": "User Query: What papers discuss latent diffusion models (LDMs) for text-to-video synthesis?\nTitle: MagicVideo: Efficient Video Generation With Latent Diffusion Models\nAbstract:  Abstract We present an efficient text-to-video generation framework based on latent diffusion models, termed MagicVideo. MagicVideo can generate smooth video clips that are concordant with the given text descriptions. Due to a novel and efficient 3D U-Net design and modeling video distributions in a low-dimensional space, MagicVideo can synthesize video clips with 256×\\times256 spatial resolution on a single GPU card, which takes around 64×\\times fewer computations than the Video Diffusion Models (VDM) in terms of FLOPs. In specific, unlike existing works that directly train video models in the RGB space, we use a pre-trained VAE to map video clips into a low-dimensional latent space and learn the distribution of videos’ latent codes via a diffusion model. Besides, we introduce two new designs to adapt the U-Net denoiser trained on image tasks to video data: a frame-wise lightweight adaptor for the image-to-video distribution adjustment and a directed temporal attention module to capture temporal dependencies across frames. Thus, we can exploit the informative weights of convolution operators from a text-to-image model for accelerating video training. To ameliorate the pixel dithering in the generated videos, we also propose a novel VideoVAE auto-encoder for better RGB reconstruction. We conduct extensive experiments and demonstrate that MagicVideo can generate high-quality video clips with either realistic or imaginary content. The code will be made public.  ", "output": "**Decision:** True  \n**Reason:** The paper titled \"MagicVideo: Efficient Video Generation With Latent Diffusion Models\" fully satisfies the user query. The abstract explicitly states that the paper discusses a text-to-video generation framework based on latent diffusion models (LDMs). It describes how MagicVideo leverages latent diffusion models to synthesize video clips from text descriptions, modeling video distributions in a low-dimensional latent space. The paper also introduces novel techniques to adapt image-based diffusion models for video synthesis. Therefore, it directly addresses the topic of latent diffusion models (LDMs) for text-to-video synthesis, aligning with the detailed requirements of the user query."}
{"input": "User Query: Could you provide me with some references for an in-depth overview and a how-to for graph transformers?\nTitle: Recipe for a General, Powerful, Scalable Graph Transformer\nAbstract:  Abstract We propose a recipe on how to build a general, powerful, scalable (GPS) graph Transformer with linear complexity and state-of-the-art results on a diverse set of benchmarks. Graph Transformers (GTs) have gained popularity in the field of graph representation learning with a variety of recent publications but they lack a common foundation about what constitutes a good positional or structural encoding, and what differentiates them. In this paper, we summarize the different types of encodings with a clearer definition and categorize them as being local, global or relative. The prior GTs are constrained to small graphs with a few hundred nodes, here we propose the first architecture with a complexity linear in the number of nodes and edges O​(N+E)𝑂𝑁𝐸O(N+E) by decoupling the local real-edge aggregation from the fully-connected Transformer. We argue that this decoupling does not negatively affect the expressivity, with our architecture being a universal function approximator on graphs. Our GPS recipe consists of choosing 3 main ingredients: (i) positional/structural encoding, (ii) local message-passing mechanism, and (iii) global attention mechanism. We provide a modular framework GraphGPS111The source code of GraphGPS is available at: https://github.com/rampasek/GraphGPS. that supports multiple types of encodings and that provides efficiency and scalability both in small and large graphs. We test our architecture on 16 benchmarks and show highly competitive results in all of them, show-casing the empirical benefits gained by the modularity and the combination of different strategies. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Recipe for a General, Powerful, Scalable Graph Transformer\" satisfies the user query for an in-depth overview and a how-to for graph transformers. The abstract indicates that the paper provides a comprehensive overview of graph transformers, including a categorization of positional and structural encodings (local, global, and relative), which contributes to a deeper understanding of the field. Furthermore, the paper offers a detailed \"recipe\" for building graph transformers, specifying three main components (positional/structural encoding, local message-passing mechanism, and global attention mechanism). It also introduces a modular framework (GraphGPS) that supports various encodings and strategies, making it a practical resource for implementing graph transformers. The inclusion of empirical results on 16 benchmarks further demonstrates the utility and effectiveness of the proposed approach, making it a suitable reference for both theoretical understanding and practical implementation."}
{"input": "User Query: What works utilise training neural networks to map 3D spatial coordinates to signed distance functions for implicit 3D representation learning?\nTitle: DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation\nAbstract:  Abstract Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape’s surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape’s boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF’s both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work. ", "output": "**Decision:** True  \n**Reason:** The paper \"DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation\" fully satisfies the requirements of the user query. The paper explicitly discusses the use of neural networks to map 3D spatial coordinates to signed distance functions (SDFs) for implicit 3D representation learning. It introduces DeepSDF, a learned continuous SDF representation, which maps 3D spatial coordinates to signed distance values to represent the geometry of shapes. The abstract highlights that DeepSDF enables high-quality shape representation, interpolation, and completion, which aligns with the goal of implicit 3D representation learning. Additionally, the paper emphasizes the use of neural networks for this purpose, making it directly relevant to the query."}
{"input": "User Query: Could you provide me a study about the classifier-free method in a conditional diffusion model?\nTitle: Classifier-Free Diffusion Guidance\nAbstract:  Abstract Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.††A short version of this paper appeared in the NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications: https://openreview.net/pdf?id=qw8AKxfYbI ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"Classifier-Free Diffusion Guidance\"* fully satisfies the user's query. The user specifically asked for a study about the classifier-free method in a conditional diffusion model, and the paper directly addresses this topic. The abstract explicitly explains the concept of classifier-free guidance, where a conditional diffusion model is trained jointly with an unconditional model to achieve guidance without the need for a separate classifier. This approach is a classifier-free method applied to conditional diffusion models, aligning perfectly with the user's request."}
{"input": "User Query: Can you identify research papers that debate the evaluation methods in interpretability?\nTitle: A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems\nAbstract:  Abstract. The need for interpretable and accountable intelligent systems grows along with the prevalence of artificial intelligence applications used in everyday life. Explainable AI systems are intended to self-explain the reasoning behind system decisions and predictions. Researchers from different disciplines work together to define, design, and evaluate explainable systems. However, scholars from different disciplines focus on different objectives and fairly independent topics of Explainable AI research, which poses challenges for identifying appropriate design and evaluation methodology and consolidating knowledge across efforts. To this end, this paper presents a survey and framework intended to share knowledge and experiences of Explainable AI design and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation methods in XAI research, after a thorough review of Explainable AI related papers in the fields of machine learning, visualization, and human-computer interaction, we present a categorization of Explainable AI design goals and evaluation methods. Our categorization presents the mapping between design goals for different Explainable AI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation cycles in multidisciplinary Explainable AI teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in Explainable AI research. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems\"* satisfies the user query. The abstract explicitly mentions that the paper focuses on the evaluation methods in Explainable AI (XAI), which is closely tied to interpretability. It highlights the challenges in identifying appropriate evaluation methodologies for XAI and provides a categorization of evaluation methods mapped to design goals for different user groups. Furthermore, the paper develops a framework with step-by-step design guidelines paired with evaluation methods, aiming to support diverse evaluation approaches in XAI research. This directly aligns with the user's request for papers that debate evaluation methods in interpretability, as the paper discusses, categorizes, and provides recommendations for these methods."}
{"input": "User Query: What research papers deal with hierarchical methods for the generation of long videos?\nTitle: NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation\nAbstract:  Abstract In this paper, we propose NUWA-XL, a novel Diffusion over Diffusion architecture for eXtremely Long video generation. Most current work generates long videos segment by segment sequentially, which normally leads to the gap between training on short videos and inferring long videos, and the sequential generation is inefficient. Instead, our approach adopts a “coarse-to-fine” process, in which the video can be generated in parallel at the same granularity. A global diffusion model is applied to generate the keyframes across the entire time range, and then local diffusion models recursively fill in the content between nearby frames. This simple yet effective strategy allows us to directly train on long videos (3376 frames) to reduce the training-inference gap, and makes it possible to generate all segments in parallel. To evaluate our model, we build FlintstonesHD dataset, a new benchmark for long video generation. Experiments show that our model not only generates high-quality long videos with both global and local coherence, but also decreases the average inference time from 7.55min to 26s (by 94.26%) at the same hardware setting when generating 1024 frames. The homepage link is https://msra-nuwa.azurewebsites.net/ ", "output": "**Decision:** True  \n**Reason:** The paper \"NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation\" fully satisfies the user query. The user is looking for research papers that deal with hierarchical methods for generating long videos, and this paper explicitly proposes a hierarchical approach through its \"coarse-to-fine\" process. The method involves a global diffusion model to generate keyframes (coarse level) and local diffusion models to recursively fill in content between frames (fine level). This hierarchical structure is central to the paper's methodology for generating long videos efficiently and with coherence. Therefore, the paper aligns well with the requirements of the query."}
{"input": "User Query: Which works are related to zero-shot recognition?\nTitle: CLIP-Event: Connecting Text and Images with Event Structures\nAbstract:  Abstract Vision-language (V+L) pretraining models have achieved great success in supporting multimedia applications by understanding the alignments between images and text. While existing vision-language pretraining models primarily focus on understanding objects in images or entities in text, they often ignore the alignment at the level of events and their argument structures. In this work, we propose a contrastive learning framework to enforce vision-language pretraining models to comprehend events and associated argument (participant) roles. To achieve this, we take advantage of text information extraction technologies to obtain event structural knowledge, and utilize multiple prompt functions to contrast difficult negative descriptions by manipulating event structures. We also design an event graph alignment loss based on optimal transport to capture event argument structures. In addition, we collect a large event-rich dataset (106,875 images) for pretraining, which provides a more challenging image retrieval benchmark to assess the understanding of complicated lengthy sentences111The data and code are publicly available for research purpose in https://github.com/limanling/clip-event.. Experiments show that our zero-shot CLIP-Event outperforms the state-of-the-art supervised model in argument extraction on Multimedia Event Extraction, achieving more than 5% absolute F-score gain in event extraction, as well as significant improvements on a variety of downstream tasks under zero-shot settings. ", "output": "**Decision:** True  \n**Reason:** The paper \"CLIP-Event: Connecting Text and Images with Event Structures\" explicitly mentions zero-shot recognition as a key aspect of its contributions. The abstract highlights that the proposed CLIP-Event framework achieves significant improvements on various downstream tasks under zero-shot settings. Additionally, the paper discusses the ability of the model to perform event and argument extraction without requiring task-specific supervision, which aligns with the concept of zero-shot recognition. Therefore, the paper is related to zero-shot recognition and satisfies the user query."}
{"input": "User Query: What is the research that achieved the accelerated rate with strong convexity in universal dynamic regret?\nTitle: Optimal Dynamic Regret in Proper Online Learning with Strongly Convex Losses and Beyond\nAbstract: We study the framework of universal dynamic regret minimization with strongly convex losses. We answer an open problem in Baby and Wang 2021 by showing that in a proper learning setup, Strongly Adaptive algorithms can achieve the near optimal dynamic regret of $\\tilde O(d^{1/3} n^{1/3}\\text{TV}[u_{1:n}]^{2/3} \\vee d)$ against any comparator sequence $u_1,\\ldots,u_n$ simultaneously, where $n$ is the time horizon and $\\text{TV}[u_{1:n}]$ is the Total Variation of comparator. These results are facilitated by exploiting a number of new structures imposed by the KKT conditions that were not considered in Baby and Wang 2021 which also lead to other improvements over their results such as: (a) handling non-smooth losses and (b) improving the dimension dependence on regret. Further, we also derive near optimal dynamic regret rates for the special case of proper online learning with exp-concave losses and an $L_\\infty$ constrained decision set.", "output": "**Decision:** True  \n**Reason:** The searched paper satisfies the requirements of the user query. The user is asking about research that achieved an accelerated rate with strong convexity in universal dynamic regret. The paper explicitly addresses the framework of universal dynamic regret minimization with strongly convex losses, which aligns with the strong convexity requirement in the query. Additionally, the paper achieves a near-optimal dynamic regret rate of $\\tilde O(d^{1/3} n^{1/3}\\text{TV}[u_{1:n}]^{2/3} \\vee d)$, which indicates an accelerated rate in the context of dynamic regret. The paper also resolves an open problem in prior work (Baby and Wang 2021) and introduces improvements, such as handling non-smooth losses and improving dimension dependence, further demonstrating its focus on advancing the field. Therefore, the paper fully satisfies the detailed requirements of the user query."}
{"input": "User Query: Which works introduced the concept of attention mechanism in neural machine translation?\nTitle: Neural Machine Translation by Jointly Learning to Align and Translate\nAbstract:  Abstract Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition. ", "output": "**Decision:** True  \n**Reason:** The searched paper, *\"Neural Machine Translation by Jointly Learning to Align and Translate\"*, fully satisfies the user query. This paper, authored by Bahdanau, Cho, and Bengio (2014), is widely recognized as the seminal work that introduced the concept of the attention mechanism in neural machine translation. The abstract explicitly describes the key innovation of the paper: extending the basic encoder-decoder architecture by allowing the model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word. This mechanism, now known as the attention mechanism, addresses the bottleneck of encoding a source sentence into a fixed-length vector and significantly improves translation performance. Therefore, this paper directly answers the user query as it is the foundational work introducing the attention mechanism in neural machine translation."}
{"input": "User Query: Which studies have explored audio-visual learning to learn audio-visual correspondence from videos?\nTitle: The Sound of Pixels\nAbstract:  Abstract We introduce PixelPlayer, a system that, by leveraging large amounts of unlabeled videos, learns to locate image regions which produce sounds and separate the input sounds into a set of components that represents the sound from each pixel. Our approach capitalizes on the natural synchronization of the visual and audio modalities to learn models that jointly parse sounds and images, without requiring additional manual supervision. Experimental results on a newly collected MUSIC dataset show that our proposed Mix-and-Separate framework outperforms several baselines on source separation. Qualitative results suggest our model learns to ground sounds in vision, enabling applications such as independently adjusting the volume of sound sources. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"The Sound of Pixels\" satisfies the user query as it explicitly explores audio-visual learning to learn audio-visual correspondence from videos. The abstract describes a system, PixelPlayer, that leverages the natural synchronization between audio and visual modalities in videos to jointly parse sounds and images. This approach inherently involves learning the correspondence between audio and visual components. Furthermore, the paper emphasizes the use of unlabeled videos and demonstrates its effectiveness on tasks such as source separation, which requires understanding the relationship between audio and visual elements. Therefore, the study aligns well with the user's query."}
{"input": "User Query: Which works have made significant progress in explaining model failures using generation-based approaches?\nTitle: OCTET: Object-aware Counterfactual Explanations\nAbstract:  Abstract Nowadays, deep vision models are being widely deployed in safety-critical applications, e.g., autonomous driving, and explainability of such models is becoming a pressing concern. Among explanation methods, counterfactual explanations aim to find minimal and interpretable changes to the input image that would also change the output of the model to be explained. Such explanations point end-users at the main factors that impact the decision of the model. However, previous methods struggle to explain decision models trained on images with many objects, e.g., urban scenes, which are more difficult to work with but also arguably more critical to explain. In this work, we propose to tackle this issue with an object-centric framework for counterfactual explanation generation. Our method, inspired by recent generative modeling works, encodes the query image into a latent space that is structured in a way to ease object-level manipulations. Doing so, it provides the end-user with control over which search directions (e.g., spatial displacement of objects, style modification, etc.) are to be explored during the counterfactual generation. We conduct a set of experiments on counterfactual explanation benchmarks for driving scenes, and we show that our method can be adapted beyond classification, e.g., to explain semantic segmentation models. To complete our analysis, we design and run a user study that measures the usefulness of counterfactual explanations in understanding a decision model. Code is available at https://github.com/valeoai/OCTET. ", "output": "**Decision:** True  \n**Reason:** The paper \"OCTET: Object-aware Counterfactual Explanations\" satisfies the requirements of the user query. The query specifically asks for works that have made significant progress in explaining model failures using generation-based approaches. The paper proposes a novel object-centric framework for generating counterfactual explanations, which is a generation-based approach. It focuses on explaining deep vision models, particularly in safety-critical applications like autonomous driving, where understanding model failures is crucial. The method allows for object-level manipulations in the latent space to generate counterfactuals, which can help identify factors leading to model failures. Additionally, the paper evaluates its approach on benchmarks and conducts a user study to assess the usefulness of the generated explanations, demonstrating its contribution to the field. Thus, the paper aligns well with the query's requirements."}
{"input": "User Query: Could you provide me some studies that examined what navigation agents learn about their environments?\nTitle: What do navigation agents learn about their environment?\nAbstract:  Abstract Today’s state of the art visual navigation agents typically consist of large deep learning models trained end to end. Such models offer little to no interpretability about the learned skills or the actions of the agent taken in response to its environment. While past works have explored interpreting deep learning models, little attention has been devoted to interpreting embodied AI systems, which often involve reasoning about the structure of the environment, target characteristics and the outcome of one’s actions. In this paper, we introduce the Interpretability System for Embodied agEnts (iSee) for Point Goal and Object Goal navigation agents. We use iSee to probe the dynamic representations produced by these agents for the presence of information about the agent as well as the environment. We demonstrate interesting insights about navigation agents using iSee, including the ability to encode reachable locations (to avoid obstacles), visibility of the target, progress from the initial spawn location as well as the dramatic effect on the behaviors of agents when we mask out critical individual neurons. The code is available at: https://github.com/allenai/iSEE ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"What do navigation agents learn about their environment?\" directly addresses the user query. The study investigates what navigation agents learn about their environments by introducing the Interpretability System for Embodied agEnts (iSee). The paper specifically probes the dynamic representations produced by navigation agents to uncover information about the environment, such as reachable locations, visibility of the target, and progress from the initial spawn location. These insights align with the user's request for studies examining what navigation agents learn about their environments. Thus, the paper fully satisfies the detailed requirements of the query."}
{"input": "User Query: Which works have incorporated semantics on the feature level and through self-supervision to refine optical flow?\nTitle: SemARFlow: Injecting Semantics into Unsupervised Optical Flow Estimation for Autonomous Driving\nAbstract:  Abstract Unsupervised optical flow estimation is especially hard near occlusions and motion boundaries and in low-texture regions. We show that additional information such as semantics and domain knowledge can help better constrain this problem. We introduce SemARFlow, an unsupervised optical flow network designed for autonomous driving data that takes estimated semantic segmentation masks as additional inputs. This additional information is injected into the encoder and into a learned upsampler that refines the flow output. In addition, a simple yet effective semantic augmentation module provides self-supervision when learning flow and its boundaries for vehicles, poles, and sky. Together, these injections of semantic information improve the KITTI-2015 optical flow test error rate from 11.80% to 8.38%. We also show visible improvements around object boundaries as well as a greater ability to generalize across datasets. Code is available at https://github.com/duke-vision/semantic-unsup-flow-release. ", "output": "**Decision:** True  \n**Reason:** The paper \"SemARFlow: Injecting Semantics into Unsupervised Optical Flow Estimation for Autonomous Driving\" satisfies the requirements of the user query. The work explicitly incorporates semantics on the feature level by using estimated semantic segmentation masks as additional inputs to the optical flow network. These semantic inputs are injected into the encoder and a learned upsampler, which refines the optical flow output. Additionally, the paper employs a semantic augmentation module that provides self-supervision for learning optical flow and its boundaries, particularly for specific semantic categories like vehicles, poles, and sky. This aligns with the query's focus on incorporating semantics both at the feature level and through self-supervision to refine optical flow."}
{"input": "User Query: Could you name some works that employed the concept of whitening methods to combat dimensional collapse?\nTitle: Iterative Normalization: Beyond Standardization towards Efficient Whitening\nAbstract:  Abstract Batch Normalization (BN) is ubiquitously employed for accelerating neural network training and improving the generalization capability by performing standardization within mini-batches. Decorrelated Batch Normalization (DBN) further boosts the above effectiveness by whitening. However, DBN relies heavily on either a large batch size, or eigen-decomposition that suffers from poor efficiency on GPUs. We propose Iterative Normalization (IterNorm), which employs Newton’s iterations for much more efficient whitening, while simultaneously avoiding the eigen-decomposition. Furthermore, we develop a comprehensive study to show IterNorm has better trade-off between optimization and generalization, with theoretical and experimental support. To this end, we exclusively introduce Stochastic Normalization Disturbance (SND), which measures the inherent stochastic uncertainty of samples when applied to normalization operations. With the support of SND, we provide natural explanations to several phenomena from the perspective of optimization, e.g., why group-wise whitening of DBN generally outperforms full-whitening and why the accuracy of BN degenerates with reduced batch sizes. We demonstrate the consistently improved performance of IterNorm with extensive experiments on CIFAR-10 and ImageNet over BN and DBN. ", "output": "**Decision:** True  \n**Reason:** The searched paper, \"Iterative Normalization: Beyond Standardization towards Efficient Whitening,\" directly addresses the concept of whitening methods, as it proposes Iterative Normalization (IterNorm) for efficient whitening. Whitening is explicitly discussed in the context of improving neural network training and generalization, which aligns with combating dimensional collapse. The paper also compares IterNorm to other whitening methods like Decorrelated Batch Normalization (DBN) and provides theoretical and experimental support for its effectiveness. Therefore, the paper satisfies the requirements of the user query."}
{"input": "User Query: Could you provide me some studies that approximate variational Bayesian inference using dropout or ensemble?\nTitle: Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning\nAbstract:  Abstract Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning\" fully satisfies the user query. The study explicitly develops a theoretical framework that casts dropout training in deep neural networks as approximate Bayesian inference in deep Gaussian processes. This directly aligns with the user's request for studies that approximate variational Bayesian inference using dropout. The paper also evaluates the properties of dropout's uncertainty and demonstrates its utility in tasks like regression, classification, and reinforcement learning, further supporting its relevance to the query."}
{"input": "User Query: Which studies have used an average of pushforwards generated by simulating nonequilibrium dynamics as an importance sampling proposal?\nTitle: Dynamical computation of the density of states and Bayes factors using nonequilibrium importance sampling\nAbstract: Nonequilibrium sampling is potentially much more versatile than its equilibrium counterpart, but it comes with challenges because the invariant distribution is not typically known when the dynamics breaks detailed balance. Here, we derive a generic importance sampling technique that leverages the statistical power of configurations transported by nonequilibrium trajectories, and can be used to compute averages with respect to arbitrary target distributions. As a dissipative reweighting scheme, the method can be viewed in relation to the annealed importance sampling (AIS) method and the related Jarzynski equality. Unlike AIS, our approach gives an unbiased estimator, with provably lower variance than directly estimating the average of an observable. We also establish a direct relation between a dynamical quantity, the dissipation, and the volume of phase space, from which we can compute quantities such as the density of states and Bayes factors. We illustrate the properties of estimators relying on this sampling technique in the context of density of state calculations, showing that it scales favorable with dimensionality -- in particular, we show that it can be used to compute the phase diagram of the mean-field Ising model from a single nonequilibrium trajectory. We also demonstrate the robustness and efficiency of the approach with an application to a Bayesian model comparison problem of the type encountered in astrophysics and machine learning.", "output": "**Decision:** True  \n**Reason:** The paper satisfies the detailed requirements of the user query. The abstract explicitly describes a method that leverages nonequilibrium dynamics to perform importance sampling. It mentions the use of configurations transported by nonequilibrium trajectories to compute averages with respect to arbitrary target distributions, which aligns with the concept of using an average of pushforwards generated by simulating nonequilibrium dynamics. Furthermore, the paper emphasizes the development of a generic importance sampling technique based on nonequilibrium trajectories, which directly addresses the query's focus on importance sampling proposals derived from nonequilibrium dynamics."}
{"input": "User Query: Are there any works that discussed the reliance of ERM models on syntactic or statistical heuristics in language?\nTitle: Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference\nAbstract:  Abstract A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area. ", "output": "**Decision:** True  \n**Reason:** The paper \"Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference\" directly addresses the reliance of machine learning models (including ERM-based models) on syntactic or statistical heuristics in language. Specifically, the paper investigates how statistical NLI models, such as BERT, rely on fallible syntactic heuristics (e.g., lexical overlap, subsequence, and constituent heuristics) to perform well on standard datasets but fail on controlled examples in the HANS dataset. This aligns with the user query, as it discusses the reliance of models on syntactic and statistical heuristics in language tasks."}
{"input": "User Query: In which researches was the (Stochastic) fictitious-play property explored in the context of identical payoff games?\nTitle: Fictitious Play and Best-Response Dynamics in Identical Interest and Zero Sum Stochastic Games\nAbstract:  Abstract This paper proposes an extension of a popular decentralized discrete-time learning procedure when repeating a static game called fictitious play (FP) (Brown, 1951; Robinson, 1951) to a dynamic model called discounted stochastic game (Shapley, 1953). Our family of discrete-time FP procedures is proven to converge to the set of stationary Nash equilibria in identical interest discounted stochastic games. This extends similar convergence results for static games (Monderer & Shapley, 1996a). We then analyze the continuous-time counterpart of our FP procedures, which include as a particular case the best-response dynamic introduced and studied by Leslie et al. (2020) in the context of zero-sum stochastic games. We prove the converge of this dynamics to stationary Nash equilibria in identical-interest and zero-sum discounted stochastic games. Thanks to stochastic approximations, we can infer from the continuous-time convergence some discrete time results such as the convergence to stationary equilibria in zero sum and team stochastic games (Holler, 2020). ", "output": "**Decision:** True  \n**Reason:** The searched paper explicitly explores the (Stochastic) fictitious-play property in the context of identical payoff games, which are referred to as \"identical interest\" games in the paper. The paper extends the fictitious play (FP) procedure to discounted stochastic games and proves its convergence to stationary Nash equilibria in identical interest (identical payoff) games. This aligns with the user query, as the research investigates the fictitious play property specifically within the context of identical payoff games, satisfying the detailed requirements of the query."}
{"input": "User Query: What studies have been done on Bayesian Neural Networks?\nTitle: Hands-on Bayesian Neural Networks – A Tutorial for Deep Learning Users\nAbstract:  Abstract Modern deep learning methods constitute incredibly powerful tools to tackle a myriad of challenging problems. However, since deep learning methods operate as black boxes, the uncertainty associated with their predictions is often challenging to quantify. Bayesian statistics offer a formalism to understand and quantify the uncertainty associated with deep neural network predictions. This tutorial provides deep learning practitioners with an overview of the relevant literature and a complete toolset to design, implement, train, use and evaluate Bayesian neural networks, i.e., stochastic artificial neural networks trained using Bayesian methods. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Hands-on Bayesian Neural Networks – A Tutorial for Deep Learning Users\" satisfies the user query as it provides an overview of the relevant literature on Bayesian Neural Networks (BNNs). The abstract explicitly mentions that the tutorial includes a review of the literature, which directly addresses the user's request for studies on BNNs. Additionally, the paper offers practical insights into designing, implementing, training, and evaluating BNNs, making it a valuable resource for understanding the field. Therefore, the paper aligns with the user's query."}
{"input": "User Query: Which work proposes the Latent Diffusion Model (LDM) for high-resolution image synthesis?\nTitle: High-Resolution Image Synthesis with Latent Diffusion Models\nAbstract:  Abstract By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state-of-the-art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including text-to-image synthesis, unconditional image generation and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.  ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"High-Resolution Image Synthesis with Latent Diffusion Models\" explicitly proposes the Latent Diffusion Model (LDM) for high-resolution image synthesis. The abstract clearly outlines the introduction of LDMs, which operate in the latent space of pretrained autoencoders to achieve high-quality image synthesis while reducing computational requirements compared to pixel-based diffusion models. It also highlights the use of cross-attention layers to enable flexible conditioning inputs and high-resolution synthesis. These details directly address the user query, as the paper is the one that introduces the concept and application of LDMs for high-resolution image synthesis."}
{"input": "User Query: Which work introduced a relaxation on 'worst-case' robustness by training models to be robust to most perturbations sampled uniformly?\nTitle: Probabilistically Robust Learning: Balancing Average- and Worst-case Performance\nAbstract:  Abstract Many of the successes of machine learning are based on minimizing an averaged loss function. However, it is well-known that this paradigm suffers from robustness issues that hinder its applicability in safety-critical domains. These issues are often addressed by training against worst-case perturbations of data, a technique known as adversarial training. Although empirically effective, adversarial training can be overly conservative, leading to unfavorable trade-offs between nominal performance and robustness. To this end, in this paper we propose a framework called probabilistic robustness that bridges the gap between the accurate, yet brittle average case and the robust, yet conservative worst case by enforcing robustness to most rather than to all perturbations. From a theoretical point of view, this framework overcomes the trade-offs between the performance and the sample-complexity of worst-case and average-case learning. From a practical point of view, we propose a novel algorithm based on risk-aware optimization that effectively balances average- and worst-case performance at a considerably lower computational cost relative to adversarial training. Our results on MNIST, CIFAR-10, and SVHN illustrate the advantages of this framework on the spectrum from average- to worst-case robustness. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"Probabilistically Robust Learning: Balancing Average- and Worst-case Performance\"* satisfies the requirements of the user query. The paper explicitly introduces a relaxation on \"worst-case\" robustness by proposing a framework called probabilistic robustness, which enforces robustness to *most* perturbations rather than all perturbations. This aligns with the query's focus on training models to be robust to most perturbations sampled uniformly. The abstract also emphasizes the balance between average- and worst-case performance, addressing the trade-offs inherent in adversarial training, which is consistent with the relaxation described in the query."}
{"input": "User Query: Can you mention some researches that focus on improved generation outcomes by adding prospective modalities?\nTitle: Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors\nAbstract:  Abstract Recent text-to-image generation methods provide a simple yet exciting conversion capability between text and image domains. While these methods have incrementally improved the generated image fidelity and text relevancy, several pivotal gaps remain unanswered, limiting applicability and quality. We propose a novel text-to-image method that addresses these gaps by (i) enabling a simple control mechanism complementary to text in the form of a scene, (ii) introducing elements that substantially improve the tokenization process by employing domain-specific knowledge over key image regions (faces and salient objects), and (iii) adapting classifier-free guidance for the transformer use case. Our model achieves state-of-the-art FID and human evaluation results, unlocking the ability to generate high fidelity images in a resolution of 512×512512512512\\times 512 pixels, significantly improving visual quality. Through scene controllability, we introduce several new capabilities: (i) Scene editing, (ii) text editing with anchor scenes, (iii) overcoming out-of-distribution text prompts, and (iv) story illustration generation, as demonstrated in the story we wrote. ", "output": "**Decision:** True  \n**Reason:** The paper \"Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors\" satisfies the user query as it focuses on improving generation outcomes by adding a prospective modality, specifically the use of scene-based control as a complementary mechanism to text. This scene-based modality allows for enhanced controllability and improved generation quality, addressing gaps in text-to-image generation. The paper also introduces domain-specific knowledge and novel techniques to improve tokenization and visual fidelity, aligning with the user's interest in research that enhances generation outcomes through additional modalities."}
{"input": "User Query: Is there any study using transformers to process graphs and outperform classical GNN models?\nTitle: GraphiT: Encoding Graph Structure in Transformers\nAbstract:  Abstract We show that viewing graphs as sets of node features and incorporating structural and positional information into a transformer architecture is able to outperform representations learned with classical graph neural networks (GNNs). Our model, GraphiT, encodes such information by (i) leveraging relative positional encoding strategies in self-attention scores based on positive definite kernels on graphs, and (ii) enumerating and encoding local sub-structures such as paths of short length. We thoroughly evaluate these two ideas on many classification and regression tasks, demonstrating the effectiveness of each of them independently, as well as their combination. In addition to performing well on standard benchmarks, our model also admits natural visualization mechanisms for interpreting graph motifs explaining the predictions, making it a potentially strong candidate for scientific applications where interpretation is important.444Code available at https://github.com/inria-thoth/GraphiT. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"GraphiT: Encoding Graph Structure in Transformers\" satisfies the detailed requirements of the user query. The study explicitly focuses on using transformers to process graphs by incorporating structural and positional information into the transformer architecture. Furthermore, the abstract clearly states that the proposed model, GraphiT, is able to outperform classical graph neural networks (GNNs) on various classification and regression tasks. This directly addresses the user's query about whether there is a study using transformers to process graphs and outperform classical GNN models."}
{"input": "User Query: Which researches emphasize on understanding deep learning models by quantifying and visualizing the contribution of image pixels to the model output?\nTitle: Transformer Interpretability Beyond Attention Visualization\nAbstract:  Abstract Self-attention techniques, and specifically Transformers, are dominating the field of text processing and are becoming increasingly popular in computer vision classification tasks. In order to visualize the parts of the image that led to a certain classification, existing methods either rely on the obtained attention maps or employ heuristic propagation along the attention graph. In this work, we propose a novel way to compute relevancy for Transformer networks. The method assigns local relevance based on the Deep Taylor Decomposition principle and then propagates these relevancy scores through the layers. This propagation involves attention layers and skip connections, which challenge existing methods. Our solution is based on a specific formulation that is shown to maintain the total relevancy across layers. We benchmark our method on very recent visual Transformer networks, as well as on a text classification problem, and demonstrate a clear advantage over the existing explainability methods. Our code is available at: https://github.com/hila-chefer/Transformer-Explainability. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Transformer Interpretability Beyond Attention Visualization\" satisfies the user query. The research focuses on understanding deep learning models, specifically Transformer networks, by quantifying and visualizing the contribution of input components (e.g., image pixels) to the model's output. The proposed method assigns local relevance scores based on the Deep Taylor Decomposition principle and propagates these scores through the layers of the network, including attention layers and skip connections. This approach directly addresses the user's requirement of quantifying and visualizing the contribution of image pixels to the model's output, as it provides a mechanism to compute and interpret relevancy in visual Transformer networks."}
{"input": "User Query: Any works that utilize a transformer module to calculate an attentive class prototype?\nTitle: Attentive Prototypes for Source-free Unsupervised Domain Adaptive 3D Object Detection\nAbstract:  Abstract 3D object detection networks tend to be biased towards the data they are trained on. Evaluation on datasets captured in different locations, conditions or sensors than that of the training (source) data results in a drop in model performance due to the gap in distribution with the test (or target) data. Current methods for domain adaptation either assume access to source data during training, which may not be available due to privacy or memory concerns, or require a sequence of lidar frames as an input. We propose a single-frame approach for source-free, unsupervised domain adaptation of lidar-based 3D object detectors that uses class prototypes to mitigate the effect pseudo-label noise. Addressing the limitations of traditional feature aggregation methods for prototype computation in the presence of noisy labels, we utilize a transformer module to identify outlier ROI’s that correspond to incorrect, over-confident annotations, and compute an attentive class prototype. Under an iterative training strategy, the losses associated with noisy pseudo labels are down-weighed and thus refined in the process of self-training. To validate the effectiveness of our proposed approach, we examine the domain shift associated with networks trained on large, label-rich datasets (such as the Waymo Open Dataset and nuScenes) and evaluate on smaller, label-poor datasets (such as KITTI) and vice-versa. We demonstrate our approach on two recent object detectors and achieve results that out-perform the other domain adaptation works. Code is available at https://github.com/deeptibhegde/AttentivePrototypeSFUDA ", "output": "**Decision:** True  \n**Reason:** The searched paper explicitly mentions the use of a transformer module to calculate an attentive class prototype. Specifically, the abstract states: *\"we utilize a transformer module to identify outlier ROI’s that correspond to incorrect, over-confident annotations, and compute an attentive class prototype.\"* This directly aligns with the user query, which seeks works that utilize a transformer module for this purpose. The paper satisfies the detailed requirements of the query by employing a transformer module as part of its methodology to compute attentive class prototypes in the context of source-free unsupervised domain adaptation for 3D object detection."}
{"input": "User Query: Which paper establishes the RLHF method?\nTitle: Fine-Tuning Language Models from Human Preferences\nAbstract: Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.", "output": "**Decision:** True  \n**Reason:** The searched paper, *\"Fine-Tuning Language Models from Human Preferences\"*, is widely recognized as the foundational paper that establishes the Reinforcement Learning from Human Feedback (RLHF) method. The abstract explicitly describes the core components of RLHF: using human feedback to learn a reward model and applying reinforcement learning to fine-tune language models based on this reward. This methodology is central to RLHF, and the paper demonstrates its application to natural language tasks, making it the seminal work in this area. Therefore, the paper fully satisfies the requirements of the user query."}
{"input": "User Query: Could you tell me about research works that studied variational Bayesian Neural Networks?\nTitle: Weight Uncertainty in Neural Networks\nAbstract:  Abstract We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Weight Uncertainty in Neural Networks\" directly addresses the study of variational Bayesian Neural Networks. It introduces a method called \"Bayes by Backprop,\" which is a variational inference technique for learning a probability distribution over the weights of a neural network. This aligns with the concept of variational Bayesian Neural Networks, where variational methods are used to approximate the posterior distribution of the weights. The paper also discusses the benefits of weight uncertainty in improving generalization and its application in various tasks such as regression and reinforcement learning. Therefore, it fully satisfies the user query."}
{"input": "User Query: What papers elaborated upon the problem of object association challenges in multiple object tracking?\nTitle: Discriminative Appearance Modeling with Multi-track Pooling for Real-time Multi-object Tracking\nAbstract:  Abstract In multi-object tracking, the tracker maintains in its memory the appearance and motion information for each object in the scene. This memory is utilized for finding matches between tracks and detections and is updated based on the matching result. Many approaches model each target in isolation and lack the ability to use all the targets in the scene to jointly update the memory. This can be problematic when there are similar looking objects in the scene. In this paper, we solve the problem of simultaneously considering all tracks during memory updating, with only a small spatial overhead, via a novel multi-track pooling module. We additionally propose a training strategy adapted to multi-track pooling which generates hard tracking episodes online. We show that the combination of these innovations results in a strong discriminative appearance model, enabling the use of greedy data association to achieve online tracking performance. Our experiments demonstrate real-time, state-of-the-art performance on public multi-object tracking (MOT) datasets. The code and trained models will be released at https://github.com/chkim403/blstm-mtp. ", "output": "**Decision:** True  \n**Reason:** The searched paper addresses the problem of object association challenges in multiple object tracking (MOT). Specifically, it discusses the difficulty of matching tracks and detections in scenarios where there are similar-looking objects in the scene, which is a core issue in object association. The paper proposes a novel multi-track pooling module to jointly update the memory for all targets, improving discriminative appearance modeling and addressing the challenge of distinguishing between similar objects. These contributions directly relate to the user's query about elaborating on object association challenges in MOT."}
{"input": "User Query: Which work demonstrated that equivariant ConvNets can learn to break equivariance when it's beneficial to the task at hand?\nTitle: Using and Abusing Equivariance\nAbstract:  Abstract In this paper we show how Group Equivariant Convolutional Neural Networks use subsampling to learn to break equivariance to the rotation and reflection symmetries. We focus on 2D rotations and reflections and investigate the impact of the broken equivariance on network performance. We show that a change in the input dimension of a network as small as a single pixel can be enough for commonly used architectures to become approximately equivariant, rather than exactly. We investigate the impact of networks not being exactly equivariant and find that approximately equivariant networks generalise significantly worse to unseen symmetries compared to their exactly equivariant counterparts. However, when the symmetries in the training data are not identical to the symmetries of the network, we find that approximately equivariant networks can relax their equivariance constraints, matching or outperforming exactly equivariant networks on common benchmarks. ", "output": "**Decision:** True  \n**Reason:** The searched paper, \"Using and Abusing Equivariance,\" directly addresses the phenomenon described in the user query. The abstract explicitly states that the paper demonstrates how Group Equivariant Convolutional Neural Networks (ConvNets) use subsampling to learn to break equivariance to rotation and reflection symmetries. Furthermore, it discusses the conditions under which breaking equivariance can be beneficial to the task at hand, such as when the symmetries in the training data differ from the symmetries of the network. This aligns with the user's query, which seeks a work that shows equivariant ConvNets can learn to break equivariance when advantageous for the task. Hence, the paper fully satisfies the requirements of the query."}
{"input": "User Query: Could you provide me with studies that established an equivalence between diffusion models and score matching?\nTitle: Denoising Diffusion Probabilistic Models\nAbstract:  Abstract We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion. ", "output": "**Decision:** True  \n**Reason:** The paper \"Denoising Diffusion Probabilistic Models\" establishes a connection between diffusion probabilistic models and denoising score matching with Langevin dynamics. Specifically, the abstract mentions that the authors derive a novel connection between these two approaches. This connection is central to the paper's methodology and contributes to the theoretical understanding of the equivalence between diffusion models and score matching. Therefore, the paper satisfies the requirements of the user query."}
{"input": "User Query: Are there any studies that organized LMs' behavior with systematic procedures?\nTitle: Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations\nAbstract:  Abstract Pre-trained language models (LMs) struggle with consistent reasoning; recently, prompting LMs to generate explanations that self-guide the inference has emerged as a promising direction to amend this. However, these approaches are fundamentally bounded by the correctness of explanations, which themselves are often noisy and inconsistent. In this work, we develop Maieutic prompting, which aims to infer a correct answer to a question even from the unreliable generations of LM. Maieutic prompting induces a tree of explanations abductively (e.g. X is true, because …) and recursively, then frames the inference as a satisfiability problem over these explanations and their logical relations. We test Maieutic prompting for true/false QA on three challenging benchmarks that require complex commonsense reasoning. Maieutic prompting achieves up to 20% better accuracy than state-of-the-art prompting methods, and as a fully unsupervised approach, performs competitively with supervised models. We also show that Maieutic prompting improves robustness in inference while providing interpretable rationales.111We share our code at https://github.com/jaehunjung1/ Maieutic-Prompting. ", "output": "**Decision:** True  \n**Reason:** The paper \"Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations\" satisfies the user query as it describes a systematic procedure for organizing and improving the behavior of language models (LMs). Specifically, the paper introduces \"Maieutic prompting,\" a method that systematically organizes LMs' reasoning by recursively generating a tree of explanations and framing the inference process as a satisfiability problem. This approach provides a structured and logical framework for addressing the inconsistencies in LMs' reasoning, thereby aligning with the user's requirement of organizing LMs' behavior using systematic procedures."}
{"input": "User Query: Could you provide me study examples on developing query-based black-box adversarial attacks?\nTitle: ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks without Training Substitute Models\nAbstract:  Abstract. Deep neural networks (DNNs) are one of the most prominent technologies of our time, as they achieve state-of-the-art performance in many machine learning tasks, including but not limited to image classification, text mining, and speech processing. However, recent research on DNNs has indicated ever-increasing concern on the robustness to adversarial examples, especially for security-critical tasks such as traffic sign identification for autonomous driving. Studies have unveiled the vulnerability of a well-trained DNN by demonstrating the ability of generating barely noticeable (to both human and machines) adversarial images that lead to misclassification. Furthermore, researchers have shown that these adversarial images are highly transferable by simply training and attacking a substitute model built upon the target model, known as a black-box attack to DNNs. Similar to the setting of training substitute models, in this paper we propose an effective black-box attack that also only has access to the input (images) and the output (confidence scores) of a targeted DNN. However, different from leveraging attack transferability from substitute models, we propose zeroth order optimization (ZOO) based attacks to directly estimate the gradients of the targeted DNN for generating adversarial examples. We use zeroth order stochastic coordinate descent along with dimension reduction, hierarchical attack and importance sampling techniques to efficiently attack black-box models. By exploiting zeroth order optimization, improved attacks to the targeted DNN can be accomplished, sparing the need for training substitute models and avoiding the loss in attack transferability. Experimental results on MNIST, CIFAR10 and ImageNet show that the proposed ZOO attack is as effective as the state-of-the-art white-box attack (e.g., Carlini and Wagner’s attack) and significantly outperforms existing black-box attacks via substitute models. ", "output": "**Decision:** True  \n**Reason:** The searched paper, \"ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks without Training Substitute Models,\" fully satisfies the user query. The query specifically asks for study examples on developing query-based black-box adversarial attacks. The paper aligns with this requirement as it proposes a method for black-box adversarial attacks that relies on querying the target model to access its input (images) and output (confidence scores). It introduces a zeroth-order optimization (ZOO) approach to estimate gradients and generate adversarial examples without requiring substitute models, which is a direct example of a query-based black-box attack. The experimental results further validate the effectiveness of the proposed method, making it a relevant and comprehensive study for the user's query."}
{"input": "User Query: Which papers provide methods for bag prediction in computational pathology?\nTitle: TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification\nAbstract:  Abstract Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, the current MIL methods are usually based on independent and identical distribution hypothesis, thus neglect the correlation among different instances. To address this problem, we proposed a new framework, called correlated MIL, and provided a proof for convergence. Based on this framework, we devised a Transformer based MIL (TransMIL), which explored both morphological and spatial information. The proposed TransMIL can effectively deal with unbalanced/balanced and binary/multiple classification with great visualization and interpretability. We conducted various experiments for three different computational pathology problems and achieved better performance and faster convergence compared with state-of-the-art methods. The test AUC for the binary tumor classification can be up to 93.09%percent\\% over CAMELYON16 dataset. And the AUC over the cancer subtypes classification can be up to 96.03%percent\\% and 98.82%percent\\% over TCGA-NSCLC dataset and TCGA-RCC dataset, respectively. Implementation is available at: https://github.com/szc19990412/TransMIL. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classification\" satisfies the user query. The query specifically asks for methods for bag prediction in computational pathology, and the paper addresses this by proposing a new framework, \"TransMIL,\" which is a Multiple Instance Learning (MIL) method. MIL is inherently a bag-based learning framework where predictions are made at the bag level (a bag is a collection of instances). The paper explicitly focuses on computational pathology applications, as evidenced by its experiments on whole slide images (WSIs) for tasks like tumor classification and cancer subtype classification. Furthermore, the method incorporates morphological and spatial information, which is crucial for pathology tasks, and demonstrates strong performance on relevant datasets. Thus, the paper aligns with the requirements of the query."}
{"input": "User Query: Which works introduce frequency analysis into the synthetic image detection framework?\nTitle: Thinking in Frequency: Face Forgery Detection by Mining Frequency-aware Clues\nAbstract:  Abstract As realistic facial manipulation technologies have achieved remarkable progress, social concerns about potential malicious abuse of these technologies bring out an emerging research topic of face forgery detection. However, it is extremely challenging since recent advances are able to forge faces beyond the perception ability of human eyes, especially in compressed images and videos. We find that mining forgery patterns with the awareness of frequency could be a cure, as frequency provides a complementary viewpoint where either subtle forgery artifacts or compression errors could be well described. To introduce frequency into the face forgery detection, we propose a novel Frequency in Face Forgery Network (F3-Net), taking advantages of two different but complementary frequency-aware clues, 1) frequency-aware decomposed image components, and 2) local frequency statistics, to deeply mine the forgery patterns via our two-stream collaborative learning framework. We apply DCT as the applied frequency-domain transformation. Through comprehensive studies, we show that the proposed F3-Net significantly outperforms competing state-of-the-art methods on all compression qualities in the challenging FaceForensics++ dataset, especially wins a big lead upon low-quality media. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Thinking in Frequency: Face Forgery Detection by Mining Frequency-aware Clues\"* fully satisfies the requirements of the user query. The query specifically asks for works that introduce frequency analysis into the synthetic image detection framework. This paper explicitly discusses the use of frequency analysis for face forgery detection, which is a subset of synthetic image detection. It introduces a novel framework called F3-Net that leverages frequency-aware clues, including frequency-decomposed image components and local frequency statistics, to detect forgery patterns. The use of DCT (Discrete Cosine Transform) as a frequency-domain transformation further confirms the incorporation of frequency analysis into the detection process. Therefore, the paper aligns with the query's requirements."}
{"input": "User Query: Could you give examples of research that talk about the substantial size of state-of-the-art DNN models?\nTitle: Deep Residual Learning for Image Recognition\nAbstract:  Abstract Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8×\\times deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions111http://image-net.org/challenges/LSVRC/2015/ and http://mscoco.org/dataset/#detections-challenge2015., where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. ", "output": "**Decision:** True  \n**Reason:** The paper \"Deep Residual Learning for Image Recognition\" satisfies the user query as it discusses the substantial size of state-of-the-art deep neural network (DNN) models. Specifically, it highlights the development and evaluation of residual networks (ResNets) with depths of up to 152 layers, which are significantly deeper than previous models like VGG nets. The paper emphasizes the importance of depth in achieving state-of-the-art performance and provides empirical evidence of the effectiveness of these extremely deep architectures. While the primary focus is on the depth of the networks rather than other aspects of model size (e.g., parameter count), the depth is a key factor in the \"substantial size\" of DNNs, making the paper relevant to the query."}
{"input": "User Query: Could you provide me with some studies about Parameter-efficient Fine-Tuning (PEFT) methods?\nTitle: Visual Prompt Tuning\nAbstract:  Abstract The current modus operandi in adapting pre-trained models involves updating all the backbone parameters, i.e., full fine-tuning. This paper introduces Visual Prompt Tuning (VPT) as an efficient and effective alternative to full fine-tuning for large-scale Transformer models in vision. Taking inspiration from recent advances in efficiently tuning large language models, VPT introduces only a small amount (less than 1% of model parameters) of trainable parameters in the input space while keeping the model backbone frozen. Via extensive experiments on a wide variety of downstream recognition tasks, we show that VPT achieves significant performance gains compared to other parameter efficient tuning protocols. Most importantly, VPT even outperforms full fine-tuning in many cases across model capacities and training data scales, while reducing per-task storage cost. Code is available at github.com/kmnp/vpt. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Visual Prompt Tuning\" satisfies the user query as it directly addresses Parameter-efficient Fine-Tuning (PEFT) methods. The abstract explicitly mentions that Visual Prompt Tuning (VPT) is an efficient alternative to full fine-tuning, where only a small fraction of the model parameters (less than 1%) are updated, while the backbone remains frozen. This aligns with the concept of PEFT, which focuses on reducing the number of trainable parameters while maintaining or improving performance. Additionally, the paper provides experimental evidence demonstrating the effectiveness of VPT and compares it to other PEFT approaches, further supporting its relevance to the query."}
{"input": "User Query: What works improved the performance of meta-learning methods using Kernel ridge regression (KRR)?\nTitle: Dataset Meta-Learning from Kernel Ridge-Regression\nAbstract:  Abstract One of the most fundamental aspects of any machine learning algorithm is the training data used by the algorithm. We introduce the novel concept of ϵitalic-ϵ\\epsilon-approximation of datasets, obtaining datasets which are much smaller than or are significant corruptions of the original training data while maintaining similar model performance. We introduce a meta-learning algorithm called Kernel Inducing Points (KIP ) for obtaining such remarkable datasets, inspired by the recent developments in the correspondence between infinitely-wide neural networks and kernel ridge-regression (KRR). For KRR tasks, we demonstrate that KIP can compress datasets by one or two orders of magnitude, significantly improving previous dataset distillation and subset selection methods while obtaining state of the art results for MNIST and CIFAR-10 classification. Furthermore, our KIP -learned datasets are transferable to the training of finite-width neural networks even beyond the lazy-training regime, which leads to state of the art results for neural network dataset distillation with potential applications to privacy-preservation. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Dataset Meta-Learning from Kernel Ridge-Regression\" satisfies the user query as it directly discusses improvements in meta-learning methods using Kernel Ridge Regression (KRR). Specifically, the paper introduces a meta-learning algorithm called Kernel Inducing Points (KIP) that leverages the connection between infinitely-wide neural networks and KRR to improve dataset distillation and subset selection methods. The work demonstrates how KIP enhances performance in KRR tasks by compressing datasets while maintaining or improving model performance. Additionally, the paper highlights state-of-the-art results for classification tasks (e.g., MNIST and CIFAR-10) and discusses the broader applicability of KIP-learned datasets, including their transferability to finite-width neural networks. This aligns well with the query's focus on identifying works that improve the performance of meta-learning methods using KRR."}
{"input": "User Query: Which work introduced adversarial perturbations in the weights of the network to improve the robustness?\nTitle: Adversarial Weight Perturbation Helps Robust Generalization\nAbstract:  Abstract The study on improving the robustness of deep neural networks against adversarial examples grows rapidly in recent years. Among them, adversarial training is the most promising one, which flattens the input loss landscape (loss change with respect to input) via training on adversarially perturbed examples. However, how the widely used weight loss landscape (loss change with respect to weight) performs in adversarial training is rarely explored. In this paper, we investigate the weight loss landscape from a new perspective, and identify a clear correlation between the flatness of weight loss landscape and robust generalization gap. Several well-recognized adversarial training improvements, such as early stopping, designing new objective functions, or leveraging unlabeled data, all implicitly flatten the weight loss landscape. Based on these observations, we propose a simple yet effective Adversarial Weight Perturbation (AWP) to explicitly regularize the flatness of weight loss landscape, forming a double-perturbation mechanism in the adversarial training framework that adversarially perturbs both inputs and weights. Extensive experiments demonstrate that AWP indeed brings flatter weight loss landscape and can be easily incorporated into various existing adversarial training methods to further boost their adversarial robustness. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Adversarial Weight Perturbation Helps Robust Generalization\" explicitly introduces the concept of adversarial perturbations in the weights of the network to improve robustness. The abstract clearly describes the proposal of a method called Adversarial Weight Perturbation (AWP), which regularizes the flatness of the weight loss landscape by adversarially perturbing the weights. This approach is highlighted as a novel contribution to improving adversarial robustness, aligning directly with the user's query. Thus, the paper fully satisfies the requirements of the query."}
{"input": "User Query: Which studies have attempted to leverage the prototypes to serve as a constraint on the feature alignment?\nTitle: Unsupervised Learning of Visual Features by Contrasting Cluster Assignments\nAbstract:  Abstract Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or “views”) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a “swapped” prediction mechanism where we predict the code of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements. We validate our findings by achieving 75.3%percent75.375.3\\% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Unsupervised Learning of Visual Features by Contrasting Cluster Assignments\" satisfies the user query. The method proposed in the paper, SwAV, explicitly leverages prototypes (cluster assignments) as a constraint to enforce feature alignment. Specifically, the paper describes how it simultaneously clusters data and enforces consistency between cluster assignments for different augmentations (or \"views\") of the same image. This approach uses prototypes (cluster assignments) as a mechanism to align features across views, which directly aligns with the user's query about leveraging prototypes to serve as a constraint on feature alignment."}
{"input": "User Query: What representative supervised methods are known in the field of Composed Image Retrieval?\nTitle: ARTEMIS: Attention-based Retrieval with Text-Explicit Matching & Implicit Similarity\nAbstract:  Abstract An intuitive way to search for images is to use queries composed of an example image and a complementary text. While the first provides rich and implicit context for the search, the latter explicitly calls for new traits, or specifies how some elements of the example image should be changed to retrieve the desired target image. Current approaches typically combine the features of each of the two elements of the query into a single representation, which can then be compared to the ones of the potential target images. Our work aims at shedding new light on the task by looking at it through the prism of two familiar and related frameworks: text-to-image and image-to-image retrieval. Taking inspiration from them, we exploit the specific relation of each query element with the targeted image and derive light-weight attention mechanisms which enable to mediate between the two complementary modalities. We validate our approach on several retrieval benchmarks, querying with images and their associated free-form text modifiers. Our method obtains state-of-the-art results without resorting to side information, multi-level features, heavy pre-training nor large architectures as in previous works. Our code is available at https://github.com/naver/artemis. ", "output": "**Decision:** True  \n**Reason:** The paper \"ARTEMIS: Attention-based Retrieval with Text-Explicit Matching & Implicit Similarity\" satisfies the user query as it directly addresses a supervised method in the field of Composed Image Retrieval. The paper discusses a novel approach that combines image and text queries to retrieve target images, which is a core aspect of Composed Image Retrieval. It introduces an attention-based mechanism to mediate between the two modalities (image and text), leveraging their specific relationships with the target image. Furthermore, the method is validated on retrieval benchmarks, demonstrating its supervised nature through evaluation on labeled datasets. The paper also achieves state-of-the-art results, making it a representative method in this domain. Thus, it aligns well with the requirements of the user query."}
{"input": "User Query: Could you provide me some works about methods optimizing the self-supervised losses or preventing excessive alterations to the model parameters in the context of CTTA?\nTitle: Continual Test-Time Domain Adaptation\nAbstract:  Abstract Test-time domain adaptation aims to adapt a source pre-trained model to a target domain without using any source data. Existing works mainly consider the case where the target domain is static. However, real-world machine perception systems are running in non-stationary and continually changing environments where the target domain distribution can change over time. Existing methods, which are mostly based on self-training and entropy regularization, can suffer from these non-stationary environments. Due to the distribution shift over time in the target domain, pseudo-labels become unreliable. The noisy pseudo-labels can further lead to error accumulation and catastrophic forgetting. To tackle these issues, we propose a continual test-time adaptation approach (CoTTA) which comprises two parts. Firstly, we propose to reduce the error accumulation by using weight-averaged and augmentation-averaged predictions which are often more accurate. On the other hand, to avoid catastrophic forgetting, we propose to stochastically restore a small part of the neurons to the source pre-trained weights during each iteration to help preserve source knowledge in the long-term. The proposed method enables the long-term adaptation for all parameters in the network. CoTTA is easy to implement and can be readily incorporated in off-the-shelf pre-trained models. We demonstrate the effectiveness of our approach on four classification tasks and a segmentation task for continual test-time adaptation, on which we outperform existing methods. Our code is available at https://qin.ee/cotta. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Continual Test-Time Domain Adaptation\" satisfies the user query. The proposed method, CoTTA, addresses the prevention of excessive alterations to the model parameters in the context of continual test-time adaptation (CTTA). Specifically, the paper introduces a mechanism to stochastically restore a small part of the neurons to the source pre-trained weights during each iteration, which helps preserve source knowledge and prevents catastrophic forgetting. This aligns with the requirement of preventing excessive parameter alterations. Additionally, while the paper does not explicitly focus on optimizing self-supervised losses, it incorporates techniques like weight-averaged and augmentation-averaged predictions, which can be seen as strategies to enhance the reliability of the adaptation process. Therefore, the paper is relevant to the query."}
{"input": "User Query: Could you provide me with papers proposing methods to detect whether children are gazing at the adult’s eyes during interaction?\nTitle: Detecting Gaze Towards Eyes in Natural Social Interactions and Its Use in Child Assessment\nAbstract:  Abstract. Eye contact is a crucial element of non-verbal communication that signifies interest, attention, and participation in social interactions. As a result, measures of eye contact arise in a variety of applications such as the assessment of the social communication skills of children at risk for developmental disorders such as autism, or the analysis of turn-taking and social roles during group meetings. However, the automated measurement of visual attention during naturalistic social interactions is challenging due to the difficulty of estimating a subject’s looking direction from video. This paper proposes a novel approach to eye contact detection during adult-child social interactions in which the adult wears a point-of-view camera which captures an egocentric view of the child’s behavior. By analyzing the child’s face regions and inferring their head pose we can accurately identify the onset and duration of the child’s looks to their social partner’s eyes. We introduce the Pose-Implicit CNN, a novel deep learning architecture that predicts eye contact while implicitly estimating the head pose. We present a fully automated system for eye contact detection that solves the sub-problems of end-to-end feature learning and pose estimation using deep neural networks. To train our models, we use a dataset comprising 22 hours of 156 play session videos from over 100 children, half of whom are diagnosed with Autism Spectrum Disorder. We report an overall precision of 0.76, recall of 0.80, and an area under the precision-recall curve of 0.79, all of which are significant improvements over existing methods. ", "output": "**Decision:** True  \n**Reason:** The paper fully satisfies the user query. It explicitly proposes a method to detect whether children are gazing at an adult’s eyes during interaction. The method involves using a point-of-view camera worn by the adult to capture an egocentric view of the child’s behavior. By analyzing the child’s face regions and inferring their head pose, the system identifies when the child looks at the adult's eyes. Furthermore, the paper introduces a novel deep learning architecture (Pose-Implicit CNN) to achieve this task, and the system is validated on a dataset of interactions, including children with Autism Spectrum Disorder. The paper directly addresses the detection of gaze towards the adult's eyes during interactions, aligning perfectly with the user query."}
{"input": "User Query: Which paper provides an influential benchmark of deep semi-supervised methods?\nTitle: Realistic Evaluation of Deep Semi-Supervised Learning Algorithms\nAbstract:  Abstract Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data when labels are limited or expensive to obtain. SSL algorithms based on deep neural networks have recently proven successful on standard benchmark tasks. However, we argue that these benchmarks fail to address many issues that SSL algorithms would face in real-world applications. After creating a unified reimplementation of various widely-used SSL techniques, we test them in a suite of experiments designed to address these issues. We find that the performance of simple baselines which do not use unlabeled data is often underreported, SSL methods differ in sensitivity to the amount of labeled and unlabeled data, and performance can degrade substantially when the unlabeled dataset contains out-of-distribution examples. To help guide SSL research towards real-world applicability, we make our unified reimplemention and evaluation platform publicly available.111https://github.com/brain-research/realistic-ssl-evaluation ", "output": "**Decision:** True  \n**Reason:** The searched paper, titled *\"Realistic Evaluation of Deep Semi-Supervised Learning Algorithms\"*, satisfies the user query as it provides a benchmark for evaluating deep semi-supervised learning (SSL) methods. The paper explicitly focuses on creating a unified reimplementation of various widely-used SSL techniques and evaluates them through a suite of experiments designed to address real-world challenges. This includes testing sensitivity to the amount of labeled and unlabeled data, the impact of out-of-distribution examples, and the performance of simple baselines. Additionally, the paper makes its evaluation platform publicly available, which is a critical component of establishing an influential benchmark. Therefore, it aligns well with the user query's requirement of identifying a paper that provides an influential benchmark for deep semi-supervised methods."}
{"input": "User Query: Which paper introduced LAMA for knowledge probing in MLMs?\nTitle: Language Models as Knowledge Bases?\nAbstract:  Abstract Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as “fill-in-the-blank” cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that   (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge,  (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and  (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches.   The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"Language Models as Knowledge Bases?\"* and its abstract clearly indicate that it introduces LAMA (Language Model Analysis) for knowledge probing in masked language models (MLMs). The abstract specifically mentions the analysis of relational knowledge in pretrained language models and provides a link to the LAMA code repository, confirming that this paper is the source of the LAMA framework. This fully satisfies the user query, which seeks the paper that introduced LAMA for knowledge probing in MLMs."}
{"input": "User Query: Could you provide me some research papers about recent works that included a predictor for the steering of representations in latent space, without requiring reconstruction?\nTitle: EquiMod: An Equivariance Module to Improve Visual Instance Discrimination\nAbstract:  Abstract Recent self-supervised visual representation methods are closing the gap with supervised learning performance. Most of these successful methods rely on maximizing the similarity between embeddings of related synthetic inputs created through data augmentations. This can be seen as a task that encourages embeddings to leave out factors modified by these augmentations, i.e. to be invariant to them. However, this only considers one side of the trade-off in the choice of the augmentations: they need to strongly modify the images to avoid simple solution shortcut learning (e.g. using only color histograms), but on the other hand, augmentations-related information may be lacking in the representations for some downstream tasks (e.g. literature shows that color is important for bird and flower classification). Few recent works proposed to mitigate this problem of using only an invariance task by exploring some form of equivariance to augmentations. This has been performed by learning additional embeddings space(s), where some augmentation(s) cause embeddings to differ, yet in a non-controlled way. In this work, we introduce EquiMod a generic equivariance module that structures the learned latent space, in the sense that our module learns to predict the displacement in the embedding space caused by the augmentations. We show that applying that module to state-of-the-art invariance models, such as BYOL and SimCLR, increases the performances on the usual CIFAR10 and ImageNet datasets. Moreover, while our model could collapse to a trivial equivariance, i.e. invariance, we observe that it instead automatically learns to keep some augmentations-related information beneficial to the representations. Source code is available at https://github.com/ADevillers/EquiMod ", "output": "**Decision:** True  \n**Reason:** The paper \"EquiMod: An Equivariance Module to Improve Visual Instance Discrimination\" satisfies the requirements of the user query. The work introduces a module, EquiMod, that predicts the displacement in the embedding (latent) space caused by augmentations. This aligns with the user's request for a predictor that steers representations in latent space. Additionally, the method does not rely on reconstruction, as it focuses on self-supervised learning through equivariance and invariance tasks rather than reconstructing input data. Therefore, the paper meets the detailed requirements of the query."}
{"input": "User Query: Could you provide me some research that use decomposed neural field to encode the 4D scenes?\nTitle: D2NeRF: Self-Supervised Decoupling of Dynamic and Static Objects from a Monocular Video\nAbstract:  Abstract Given a monocular video, segmenting and decoupling dynamic objects while recovering the static environment is a widely studied problem in machine intelligence. Existing solutions usually approach this problem in the image domain, limiting their performance and understanding of the environment. We introduce Decoupled Dynamic Neural Radiance Field (D2NeRF), a self-supervised approach that takes a monocular video and learns a 3D scene representation which decouples moving objects, including their shadows, from the static background. Our method represents the moving objects and the static background by two separate neural radiance fields with only one allowing for temporal changes. A naive implementation of this approach leads to the dynamic component taking over the static one as the representation of the former is inherently more general and prone to overfitting. To this end, we propose a novel loss to promote correct separation of phenomena. We further propose a shadow field network to detect and decouple dynamically moving shadows. We introduce a new dataset containing various dynamic objects and shadows and demonstrate that our method can achieve better performance than state-of-the-art approaches in decoupling dynamic and static 3D objects, occlusion and shadow removal, and image segmentation for moving objects. Project page: d2nerf.github.io ", "output": "**Decision:** True  \n**Reason:** The paper \"D2NeRF: Self-Supervised Decoupling of Dynamic and Static Objects from a Monocular Video\" satisfies the user query as it uses decomposed neural fields to encode 4D scenes. Specifically, the paper introduces a method that represents a 3D scene with temporal dynamics (4D scenes) by decoupling it into two separate neural radiance fields: one for the static background and another for dynamic objects, including their temporal changes. This decomposition aligns with the concept of using decomposed neural fields to encode 4D scenes. Additionally, the method explicitly addresses temporal changes, which are a key component of 4D scene representation."}
{"input": "User Query: Which paper developed PAC-Bayesian bounds for the analysis of adversarial generative models?\nTitle: PAC-Bayesian Generalization Bounds for Adversarial Generative Models\nAbstract:  Abstract We extend PAC-Bayesian theory to generative models and develop generalization bounds for models based on the Wasserstein distance and the total variation distance. Our first result on the Wasserstein distance assumes the instance space is bounded, while our second result takes advantage of dimensionality reduction. Our results naturally apply to Wasserstein GANs and Energy-Based GANs, and our bounds provide new training objectives for these two. Although our work is mainly theoretical, we perform numerical experiments showing non-vacuous generalization bounds for Wasserstein GANs on synthetic datasets. ", "output": "**Decision:** True  \n**Reason:** The searched paper, titled \"PAC-Bayesian Generalization Bounds for Adversarial Generative Models,\" explicitly addresses the development of PAC-Bayesian bounds for adversarial generative models. The abstract mentions the extension of PAC-Bayesian theory to generative models and the derivation of generalization bounds based on the Wasserstein distance and total variation distance. Furthermore, it specifies that the results apply to adversarial generative models, such as Wasserstein GANs and Energy-Based GANs. This directly aligns with the user query, which seeks a paper that develops PAC-Bayesian bounds for the analysis of adversarial generative models. Therefore, the paper fully satisfies the requirements of the query."}
{"input": "User Query: Any work has utilized attention mechanism to learn contributions of neighboring nodes to the target nodes?\nTitle: Graph Attention Networks\nAbstract:  Abstract We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods’ features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training). ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Graph Attention Networks\" (GATs) fully satisfies the user's query. The abstract explicitly states that the proposed GATs utilize attention mechanisms to operate on graph-structured data. Specifically, the model employs masked self-attentional layers, which allow nodes to attend to their neighbors' features and assign different weights to different neighboring nodes. This mechanism inherently learns the contributions of neighboring nodes to the target nodes. The attention mechanism enables the model to determine the relative importance of each neighbor in the context of the target node, which directly aligns with the user's requirement."}
{"input": "User Query: Any works report that contrastive visual learning could enhance contrastive sentence embeddings?\nTitle: Non-Linguistic Supervision for Contrastive Learning of Sentence Embeddings\nAbstract:  Abstract Semantic representation learning for sentences is an important and well-studied problem in NLP. The current trend for this task involves training a Transformer-based sentence encoder through a contrastive objective with text, i.e., clustering sentences with semantically similar meanings and scattering others. In this work, we find the performance of Transformer models as sentence encoders can be improved by training with multi-modal multi-task losses, using unpaired examples from another modality (e.g., sentences and unrelated image/audio data). In particular, besides learning by the contrastive loss on text, our model clusters examples from a non-linguistic domain (e.g., visual/audio) with a similar contrastive loss at the same time. The reliance of our framework on unpaired non-linguistic data makes it language-agnostic, enabling it to be widely applicable beyond English NLP. Experiments on 7 semantic textual similarity benchmarks reveal that models trained with the additional non-linguistic (images/audio) contrastive objective lead to higher quality sentence embeddings. This indicates that Transformer models are able to generalize better by doing a similar task (i.e., clustering) with unpaired examples from different modalities in a multi-task fashion. The code is available at https://github.com/yiren-jian/NonLing-CSE. ", "output": "**Decision:** True  \n**Reason:** The paper directly addresses the user's query by reporting that contrastive visual learning can enhance contrastive sentence embeddings. Specifically, the paper describes a framework where sentence embeddings are improved by training with multi-modal multi-task losses, including a contrastive objective using non-linguistic data such as images or audio. The experiments demonstrate that incorporating a contrastive visual learning objective (alongside textual contrastive learning) leads to higher quality sentence embeddings, as evidenced by improved performance on semantic textual similarity benchmarks. This aligns with the user's interest in whether contrastive visual learning can enhance contrastive sentence embeddings."}
{"input": "User Query: Which papers analyzed the generalization of one-hidden-layer fully-connected neural networks assuming the data come from mixtures of well-separated distributions?\nTitle: Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data\nAbstract:  Abstract Neural networks have many successful applications, while much less theoretical understanding has been gained. Towards bridging this gap, we study the problem of learning a two-layer overparameterized ReLU neural network for multi-class classification via stochastic gradient descent (SGD) from random initialization. In the overparameterized setting, when the data comes from mixtures of well-separated distributions, we prove that SGD learns a network with a small generalization error, albeit the network has enough capacity to fit arbitrary labels. Furthermore, the analysis provides interesting insights into several aspects of learning neural networks and can be verified based on empirical studies on synthetic data and on the MNIST dataset.  ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data\" satisfies the requirements of the user query. The paper explicitly analyzes the generalization of a two-layer (one-hidden-layer) fully-connected ReLU neural network. It assumes that the data comes from mixtures of well-separated distributions, as stated in the abstract. Furthermore, the paper provides theoretical insights into the generalization error in this setting, which aligns with the user's request for an analysis of generalization under these assumptions."}
{"input": "User Query: Which works report about enforcing control over desired attributes at inference time without updating the PLM parameters?\nTitle: GeDi: Generative Discriminator guided Sequence Generation\nAbstract:  Abstract While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives stronger controllability than the state of the art method while also achieving generation speeds more than 303030 times faster. Additionally, training GeDi on only four topics allows us to controllably generate new topics zero-shot from just a keyword, unlocking a new capability that previous controllable generation methods do not have. Lastly, we show that GeDi can make GPT-2 (1.5B parameters) significantly less toxic without sacrificing linguistic quality, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.111Code available at https://github.com/salesforce/GeDi, includes GeDi-guided GPT-3 generation using OpenAI API.  ", "output": "**Decision:** True  \n**Reason:** The paper \"GeDi: Generative Discriminator guided Sequence Generation\" satisfies the user query as it explicitly discusses enforcing control over desired attributes at inference time without updating the parameters of the pre-trained language model (PLM). The method, GeDi, uses smaller generative discriminators to guide the generation process of large PLMs like GPT-2. This guidance is achieved by computing classification probabilities and applying Bayes rule to control the output based on desired attributes (control codes) and undesired attributes (anti-control codes). The paper does not involve updating the PLM parameters but instead uses an external mechanism (the generative discriminator) to enforce control during inference. This aligns with the requirement of the query."}
{"input": "User Query: What studies used least-to-most prompting and successive prompting approach?\nTitle: Successive Prompting for Decomposing Complex Questions\nAbstract:  Abstract Answering complex questions that require making latent decisions is a challenging task, especially when limited supervision is available. Recent works leverage the capabilities of large language models (LMs) to perform complex question answering in a few-shot setting by demonstrating how to output intermediate rationalizations while solving the complex question in a single pass. We introduce “Successive Prompting”, where we iteratively break down a complex task into a simple task, solve it, and then repeat the process until we get the final solution. Successive prompting decouples the supervision for decomposing complex questions from the supervision for answering simple questions, allowing us to (1) have multiple opportunities to query in-context examples at each reasoning step (2) learn question decomposition separately from question answering, including using synthetic data, and (3) use bespoke (fine-tuned) components for reasoning steps where a large LM does not perform well. The intermediate supervision is typically manually written, which can be expensive to collect. We introduce a way to generate a synthetic dataset which can be used to bootstrap a model’s ability to decompose and answer intermediate questions. Our best model (with successive prompting) achieves an improvement of ∼similar-to\\sim5% absolute F1 on a few-shot version of the DROP dataset when compared with a state-of-the-art model with the same supervision. ", "output": "**Decision:** True  \n**Reason:** The searched paper explicitly discusses the use of \"Successive Prompting\" as a method for decomposing complex questions into simpler tasks, solving them iteratively until reaching the final solution. This approach aligns with the concept of successive prompting described in the user query. While the paper does not explicitly mention \"least-to-most prompting\" by name, the iterative decomposition of tasks from complex to simpler ones is conceptually similar to the least-to-most prompting approach. Therefore, the paper satisfies the requirements of the user query."}
{"input": "User Query: Which papers analyze the main factors that contribute to the success of in-context learning?\nTitle: Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?\nAbstract:  Abstract Large language models (LMs) are able to in-context learn—perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required—randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.  ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?\"* directly addresses the main factors contributing to the success of in-context learning. The abstract explicitly states that the paper investigates which aspects of demonstrations are key drivers of performance in in-context learning. It identifies specific factors such as the label space, the distribution of input text, and the overall format of the sequence as critical contributors. This aligns with the user's query, as the paper provides an analysis of the main factors that influence the success of in-context learning. Therefore, the paper fully satisfies the detailed requirements of the user query."}
{"input": "User Query: Which works focused on volumetric representation for geometrical information input in neural networks?\nTitle: VoxelPose: Towards Multi-Camera 3D Human Pose Estimation in Wild Environment\nAbstract:  Abstract We present VoxelPose to estimate 333D poses of multiple people from multiple camera views. In contrast to the previous efforts which require to establish cross-view correspondence based on noisy and incomplete 222D pose estimates, VoxelPose directly operates in the 333D space therefore avoids making incorrect decisions in each camera view. To achieve this goal, features in all camera views are aggregated in the 333D voxel space and fed into Cuboid Proposal Network (CPN) to localize all people. Then we propose Pose Regression Network (PRN) to estimate a detailed 333D pose for each proposal. The approach is robust to occlusion which occurs frequently in practice. Without bells and whistles, it outperforms the previous methods on several public datasets. The code is available at https://github.com/microsoft/voxelpose-pytorch ", "output": "**Decision:** True  \n**Reason:** The paper \"VoxelPose: Towards Multi-Camera 3D Human Pose Estimation in Wild Environment\" fully satisfies the user query. The paper explicitly focuses on volumetric representation by utilizing a 3D voxel space to aggregate features from multiple camera views. This volumetric representation is used as geometrical information input for neural networks, specifically the Cuboid Proposal Network (CPN) and Pose Regression Network (PRN). The use of 3D voxel space aligns directly with the concept of volumetric representation for geometrical information input in neural networks, as requested in the user query."}
{"input": "User Query: Are there any works considering whether large language models can perform legal reasoning?\nTitle: Legal Syllogism Prompting: Teaching Large Language Models for Legal Judgment Prediction\nAbstract:  Abstract. Legal syllogism is a form of deductive reasoning commonly used by legal professionals to analyze cases. In this paper, we propose legal syllogism prompting (LoT), a simple prompting method to teach large language models (LLMs) for legal judgment prediction. LoT teaches only that in the legal syllogism the major premise is law, the minor premise is the fact, and the conclusion is judgment. Then the models can produce a syllogism reasoning of the case and give the judgment without any learning, fine-tuning, or examples. On CAIL2018, a Chinese criminal case dataset, we performed zero-shot judgment prediction experiments with GPT-3 models. Our results show that LLMs with LoT achieve better performance than the baseline and chain of thought prompting, the state-of-art prompting method on diverse reasoning tasks. LoT enables the model to concentrate on the key information relevant to the judgment and to correctly understand the legal meaning of acts, as compared to other methods. Our method enables LLMs to predict judgment along with law articles and justification, which significantly enhances the explainability of models. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Legal Syllogism Prompting: Teaching Large Language Models for Legal Judgment Prediction\" satisfies the user query as it directly addresses whether large language models (LLMs) can perform legal reasoning. The paper proposes a method called \"legal syllogism prompting\" (LoT) to teach LLMs to perform legal judgment prediction, which inherently involves legal reasoning. It evaluates the ability of LLMs, specifically GPT-3, to engage in deductive reasoning based on legal syllogisms, a fundamental form of legal reasoning. The experiments and results discussed in the paper demonstrate the capacity of LLMs to reason about legal cases and provide judgments, thereby confirming that the paper considers the ability of LLMs to perform legal reasoning."}
{"input": "User Query: Are there any researches that designed a multi-round communication game for the sender and receiver?\nTitle: Emergent Graphical Conventions in a Visual Communication Game\nAbstract:  Abstract Humans communicate with graphical sketches apart from symbolic languages (Fay et al.,, 2014). Primarily focusing on the latter, recent studies of emergent communication (Lazaridou and Baroni,, 2020) overlook the sketches; they do not account for the evolution process through which symbolic sign systems emerge in the trade-off between iconicity and symbolicity. In this work, we take the very first step to model and simulate this process via two neural agents playing a visual communication game; the sender communicates with the receiver by sketching on a canvas. We devise a novel reinforcement learning method such that agents are evolved jointly towards successful communication and abstract graphical conventions. To inspect the emerged conventions, we define three fundamental properties—iconicity, symbolicity, and semanticity—and design evaluation methods accordingly. Our experimental results under different controls are consistent with the observation in studies of human graphical conventions (Hawkins et al.,, 2019, Fay et al.,, 2010). Of note, we find that evolved sketches can preserve the continuum of semantics (Mikolov et al.,, 2013) under proper environmental pressures. More interestingly, co-evolved agents can switch between conventionalized and iconic communication based on their familiarity with referents. We hope the present research can pave the path for studying emergent communication with the modality of sketches. ", "output": "**Decision:** True  \n**Reason:** The paper describes a research study in which two neural agents (a sender and a receiver) engage in a visual communication game. The sender communicates with the receiver by sketching on a canvas, and the agents co-evolve their communication strategies through reinforcement learning. The abstract explicitly mentions that the agents are \"evolved jointly towards successful communication and abstract graphical conventions,\" which implies iterative or multi-round interactions between the sender and receiver. This satisfies the user's query about the design of a multi-round communication game for the sender and receiver."}
{"input": "User Query: Which works estimate a noise matrix through special design of network architecture or loss functions for model-re-calibration methods?\nTitle: Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe Noise\nAbstract:  Abstract The growing importance of massive datasets used for deep learning makes robustness to label noise a critical property for classifiers to have. Sources of label noise include automatic labeling, non-expert labeling, and label corruption by data poisoning adversaries. Numerous previous works assume that no source of labels can be trusted. We relax this assumption and assume that a small subset of the training data is trusted. This enables substantial label corruption robustness performance gains. In addition, particularly severe label noise can be combated by using a set of trusted data with clean labels. We utilize trusted data by proposing a loss correction technique that utilizes trusted examples in a data-efficient manner to mitigate the effects of label noise on deep neural network classifiers. Across vision and natural language processing tasks, we experiment with various label noises at several strengths, and show that our method significantly outperforms existing methods. ", "output": "**Decision:** True  \n**Reason:** The paper \"Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe Noise\" satisfies the requirements of the user query. The user query specifically asks for works that estimate a noise matrix through special design of network architecture or loss functions for model re-calibration methods. The abstract of the paper indicates that the authors propose a loss correction technique that leverages a small subset of trusted data with clean labels to mitigate the effects of label noise. This approach involves designing a loss function specifically to handle label noise, which aligns with the requirement of estimating a noise matrix through the design of loss functions. Although the abstract does not explicitly mention the term \"noise matrix,\" the description of the loss correction technique implies that the method involves addressing label noise, which is central to the query. Therefore, the paper is relevant to the user query."}
{"input": "User Query: What works have used self-reference techniques to boost LLM performance?\nTitle: Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification\nAbstract:  Abstract Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has brought significant advancements in addressing math reasoning problems. In particular, OpenAI’s latest version of GPT-4, known as GPT-4 Code Interpreter, shows remarkable performance on challenging math datasets. In this paper, we explore the effect of code on enhancing LLMs’ reasoning capability by introducing different constraints on the Code Usage Frequency of GPT-4 Code Interpreter. We found that its success can be largely attributed to its powerful skills in generating and executing code, evaluating the output of code execution, and rectifying its solution when receiving unreasonable outputs. Based on this insight, we propose a novel and effective prompting method, explicit code-based self-verification (CSV), to further boost the mathematical reasoning potential of GPT-4 Code Interpreter. This method employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to use code to self-verify its answers. In instances where the verification state registers as “False”, the model shall automatically amend its solution, analogous to our approach of rectifying errors during a mathematics examination. Furthermore, we recognize that the states of the verification result indicate the confidence of a solution, which can improve the effectiveness of majority voting. With GPT-4 Code Interpreter and CSV, we achieve an impressive zero-shot accuracy on MATH dataset (53.9% →→\\to 84.3%). ", "output": "**Decision:** True  \n**Reason:** The paper satisfies the user query as it explicitly discusses the use of self-reference techniques to boost the performance of a large language model (LLM). Specifically, the proposed method, \"explicit code-based self-verification (CSV),\" involves the model generating code to verify its own answers and then rectifying its solutions if the verification fails. This process is inherently self-referential, as the model evaluates and improves its own outputs based on the results of its self-verification. The method leverages self-reference to enhance the reasoning capability of GPT-4 Code Interpreter, particularly in solving challenging math word problems, which aligns with the user's query about works that use self-reference techniques to improve LLM performance."}
{"input": "User Query: Which work originally proposed the Feature Pyramid Networks (FPN)?\nTitle: Feature Pyramid Networks for Object Detection\nAbstract:  Abstract Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 6 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Feature Pyramid Networks for Object Detection\" is the original work that proposed the concept of Feature Pyramid Networks (FPN). The abstract explicitly describes the development of the FPN architecture, including its top-down architecture with lateral connections for building high-level semantic feature maps at multiple scales. Furthermore, the paper introduces FPN as a novel approach to efficiently construct feature pyramids using the inherent multi-scale hierarchy of deep convolutional networks. This aligns directly with the user query, which seeks the work that originally proposed FPN. Therefore, the paper fully satisfies the detailed requirements of the query."}
{"input": "User Query: Which studies have been conducted to enhance the performance of LLMs in targeted domains by using a simple set of tools?\nTitle: Toolformer: Language Models Can Teach Themselves to Use Tools\nAbstract:  Abstract Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q&A system, a search engine, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities. ", "output": "**Decision:** True  \n**Reason:** The paper \"Toolformer: Language Models Can Teach Themselves to Use Tools\" fully satisfies the user query. The study specifically investigates how language models (LLMs) can enhance their performance in targeted domains by using a simple set of tools, such as a calculator, Q&A system, search engine, translation system, and calendar. The approach involves training the model to decide when and how to use these tools via simple APIs, thereby improving performance on specific tasks. This aligns with the user query's focus on enhancing LLM performance in targeted domains through the use of tools. Additionally, the methodology is straightforward, requiring only a handful of demonstrations for each tool, which further matches the query's emphasis on simplicity."}
{"input": "User Query: What studies have been conducted on unlearning specific features, such as “Bang”, “Hat” or “Beard”, in GANs?\nTitle: Adapt then Unlearn: Exploiting Parameter Space Semantics for Unlearning in Generative Adversarial Networks\nAbstract:  Abstract The increased attention to regulating the outputs of deep generative models, driven by growing concerns about privacy and regulatory compliance, has highlighted the need for effective control over these models. This necessity arises from instances where generative models produce outputs containing undesirable, offensive, or potentially harmful content. To tackle this challenge, the concept of machine unlearning has emerged, aiming to forget specific learned information or to erase the influence of undesired data subsets from a trained model. The objective of this work is to prevent the generation of outputs containing undesired features from a pre-trained Generative Adversarial Network (GAN) where the underlying training data set is inaccessible. Our approach is inspired by a crucial observation: the parameter space of GANs exhibits meaningful directions that can be leveraged to suppress specific undesired features. However, such directions usually result in the degradation of the quality of generated samples. Our proposed method, known as ‘Adapt-then-Unlearn,’ excels at unlearning such undesirable features while also maintaining the quality of generated samples. This method unfolds in two stages: in the initial stage, we adapt the pre-trained GAN using negative samples provided by the user, while in the subsequent stage, we focus on unlearning the undesired feature. During the latter phase, we train the pre-trained GAN using positive samples, incorporating a repulsion regularizer. This regularizer actively encourages the model’s learned parameters to move away from the parameters associated with the adapted model from the first stage while also maintaining the quality of generated samples. To the best of our knowledge, our approach stands as a pioneering method addressing unlearning within the realm of GANs. We validate the effectiveness of our method through comprehensive experiments, encompassing both class-level unlearning on the MNIST dataset and feature-level unlearning tasks on the CelebA-HQ dataset. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Adapt then Unlearn: Exploiting Parameter Space Semantics for Unlearning in Generative Adversarial Networks\" satisfies the user query. The user is specifically asking about studies that focus on unlearning specific features, such as \"Bang,\" \"Hat,\" or \"Beard,\" in GANs. The abstract of the paper explicitly mentions that the proposed method, \"Adapt-then-Unlearn,\" is designed to unlearn undesirable features in pre-trained GANs. Furthermore, the paper validates its approach through feature-level unlearning tasks on the CelebA-HQ dataset, which is known to include attributes like \"Bang,\" \"Hat,\" and \"Beard.\" This directly aligns with the user's query about unlearning specific features in GANs. Therefore, the paper fully satisfies the detailed requirements of the user query."}
{"input": "User Query: What research discusses the tendency of SGD to lean toward smaller-norm solutions?\nTitle: Implicit Regularization in Matrix Factorization\nAbstract:  Abstract We study implicit regularization when optimizing an underdetermined quadratic objective over a matrix X𝑋X with gradient descent on a factorization of X𝑋X. We conjecture and provide empirical and theoretical evidence that with small enough step sizes and initialization close enough to the origin, gradient descent on a full dimensional factorization converges to the minimum nuclear norm solution. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"Implicit Regularization in Matrix Factorization\"* discusses the implicit regularization effect of gradient descent (GD) when optimizing a quadratic objective over a matrix factorization. Specifically, the paper provides evidence that GD converges to the minimum nuclear norm solution under certain conditions, such as small step sizes and initialization near the origin. This aligns with the user's query about research discussing the tendency of stochastic gradient descent (SGD) or gradient-based methods to favor smaller-norm solutions. While the paper focuses on GD rather than explicitly mentioning SGD, the findings are relevant to the broader context of gradient-based optimization methods and their implicit bias toward smaller-norm solutions. Hence, the paper satisfies the user's query."}
{"input": "User Query: Could you provide me studies about 3DGS rendering?\nTitle: 3D Gaussian Splatting for Real-Time Radiance Field Rendering\nAbstract:  Abstract. Radiance Field methods have recently revolutionized novel-view synthesis of scenes captured with multiple photos or videos. However, achieving high visual quality still requires neural networks that are costly to train and render, while recent faster methods inevitably trade off speed for quality. For unbounded and complete scenes (rather than isolated objects) and 1080p resolution rendering, no current method can achieve real-time display rates. We introduce three key elements that allow us to achieve state-of-the-art visual quality while maintaining competitive training times and importantly allow high-quality real-time (≥30absent30\\geq 30 fps) novel-view synthesis at 1080p resolution. First, starting from sparse points produced during camera calibration, we represent the scene with 3D Gaussians that preserve desirable properties of continuous volumetric radiance fields for scene optimization while avoiding unnecessary computation in empty space; Second, we perform interleaved optimization/density control of the 3D Gaussians, notably optimizing anisotropic covariance to achieve an accurate representation of the scene; Third, we develop a fast visibility-aware rendering algorithm that supports anisotropic splatting and both accelerates training and allows real-time rendering. We demonstrate state-of-the-art visual quality and real-time rendering on several established datasets. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"3D Gaussian Splatting for Real-Time Radiance Field Rendering\"* directly addresses the user's query about studies related to 3DGS (3D Gaussian Splatting) rendering. The abstract discusses the use of 3D Gaussian representations for scene optimization and rendering, specifically focusing on achieving real-time rendering at high resolutions (1080p) while maintaining state-of-the-art visual quality. The paper introduces methods for optimizing 3D Gaussian splatting and a fast rendering algorithm, which are central to the topic of 3DGS rendering. Therefore, the paper fully satisfies the user's query."}
{"input": "User Query: What research tries to discern between points in adaptive training that are learnable, worth learning and not yet learned?\nTitle: Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt\nAbstract:  Abstract Training on web-scale data can take months. But most computation and time is wasted on redundant and noisy points that are already learnt or not learnable. To accelerate training, we introduce Reducible Holdout Loss Selection (RHO-LOSS), a simple but principled technique which selects approximately those points for training that most reduce the model’s generalization loss. As a result, RHO-LOSS mitigates the weaknesses of existing data selection methods: techniques from the optimization literature typically select “hard” (e.g. high loss) points, but such points are often noisy (not learnable) or less task-relevant. Conversely, curriculum learning prioritizes “easy” points, but such points need not be trained on once learnt. In contrast, RHO-LOSS selects points that are learnable, worth learning, and not yet learnt. RHO-LOSS trains in far fewer steps than prior art, improves accuracy, and speeds up training on a wide range of datasets, hyperparameters, and architectures (MLPs, CNNs, and BERT). On the large web-scraped image dataset Clothing-1M, RHO-LOSS trains in 18x fewer steps and reaches 2% higher final accuracy than uniform data shuffling. †† Code: https://github.com/OATML/RHO-Loss  ", "output": "**Decision:** True  \n**Reason:**  \nThe paper titled *\"Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt\"* directly addresses the user query. The research introduces a method called Reducible Holdout Loss Selection (RHO-LOSS), which explicitly focuses on identifying and prioritizing data points during training that are **learnable**, **worth learning**, and **not yet learnt**. The abstract explains how RHO-LOSS mitigates the inefficiencies of training on redundant or noisy data by selecting points that most reduce the model's generalization loss. This aligns perfectly with the user's interest in research that discerns between these specific categories of data points in adaptive training. Furthermore, the paper evaluates the method across various datasets and architectures, demonstrating its relevance and effectiveness in achieving the stated goal. Therefore, the paper fully satisfies the requirements of the user query."}
{"input": "User Query: What papers design loss functions that are robust to label noise?\nTitle: Peer Loss Functions: Learning from Noisy Labels without Knowing Noise Rates\nAbstract:  Abstract Learning with noisy labels is a common challenge in supervised learning. Existing approaches often require practitioners to specify noise rates, i.e., a set of parameters controlling the severity of label noises in the problem, and the specifications are either assumed to be given or estimated using additional steps. In this work, we introduce a new family of loss functions that we name as peer loss functions, which enables learning from noisy labels and does not require a priori specification of the noise rates. Peer loss functions work within the standard empirical risk minimization (ERM) framework. We show that, under mild conditions, performing ERM with peer loss functions on the noisy data leads to the optimal or a near-optimal classifier as if performing ERM over the clean training data, which we do not have access to. We pair our results with an extensive set of experiments. Peer loss provides a way to simplify model development when facing potentially noisy training labels, and can be promoted as a robust candidate loss function in such situations. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Peer Loss Functions: Learning from Noisy Labels without Knowing Noise Rates\" directly addresses the design of loss functions that are robust to label noise. The abstract explicitly mentions the introduction of a new family of loss functions, called peer loss functions, which are designed to handle noisy labels. Furthermore, the paper demonstrates that these loss functions enable learning from noisy labels without requiring prior knowledge of noise rates and achieve optimal or near-optimal performance as if clean training data were available. This aligns with the user's query about papers that design loss functions robust to label noise."}
{"input": "User Query: Could you provide me the study that utilizes local dependencies to generate counterfactual samples to enhance sample efficiency?\nTitle: Counterfactual Data Augmentation using Locally Factored Dynamics\nAbstract:  Abstract Many dynamic processes, including common scenarios in robotic control and reinforcement learning (RL), involve a set of interacting subprocesses. Though the subprocesses are not independent, their interactions are often sparse, and the dynamics at any given time step can often be decomposed into locally independent causal mechanisms. Such local causal structures can be leveraged to improve the sample efficiency of sequence prediction and off-policy reinforcement learning. We formalize this by introducing local causal models (LCMs), which are induced from a global causal model by conditioning on a subset of the state space. We propose an approach to inferring these structures given an object-oriented state representation, as well as a novel algorithm for Counterfactual Data Augmentation (CoDA). CoDA uses local structures and an experience replay to generate counterfactual experiences that are causally valid in the global model. We find that CoDA significantly improves the performance of RL agents in locally factored tasks, including the batch-constrained and goal-conditioned settings.111Code available at https://github.com/spitis/mrl ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"Counterfactual Data Augmentation using Locally Factored Dynamics\"* satisfies the requirements of the user query. The paper explicitly discusses utilizing local dependencies (referred to as locally independent causal mechanisms or local causal models) to generate counterfactual samples. These counterfactual samples are created using the proposed Counterfactual Data Augmentation (CoDA) method, which leverages local causal structures and experience replay. Furthermore, the paper demonstrates that this approach enhances sample efficiency, particularly in reinforcement learning tasks. Therefore, the paper aligns well with the user's query."}
{"input": "User Query: What works proposed the idea of language-image contrastive learning (CLIP) for pretraining?\nTitle: Learning Transferable Visual Models From Natural Language Supervision\nAbstract:  Abstract State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"Learning Transferable Visual Models From Natural Language Supervision\"* fully satisfies the user query. The abstract explicitly describes the idea of language-image contrastive learning (CLIP) for pretraining. It introduces the concept of learning visual representations by predicting which caption corresponds to which image, leveraging a large dataset of (image, text) pairs. This approach is the foundation of CLIP, which uses contrastive learning between image and text modalities to pretrain models. The paper is indeed the one that proposed the idea of CLIP, as it aligns with the user's query."}
{"input": "User Query: Which work is similar to LIV in learning implicit value function?\nTitle: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training\nAbstract:  Abstract Reward and representation learning are two long-standing challenges for learning an expanding set of robot manipulation skills from sensory observations. Given the inherent cost and scarcity of in-domain, task-specific robot data, learning from large, diverse, offline human videos has emerged as a promising path towards acquiring a generally useful visual representation for control; however, how these human videos can be used for general-purpose reward learning remains an open question. We introduce Value-Implicit Pre-training (VIP), a self-supervised pre-trained visual representation capable of generating dense and smooth reward functions for unseen robotic tasks. VIP casts representation learning from human videos as an offline goal-conditioned reinforcement learning problem and derives a self-supervised goal-conditioned value-function objective that does not depend on actions, enabling pre-training on unlabeled human videos. Theoretically, VIP can be understood as a novel implicit time contrastive objective that generates a temporally smooth embedding, enabling the value function to be implicitly defined via the embedding distance, which can then be used to construct the reward function for any goal-image specified downstream task. Trained on large-scale Ego4D human videos and without any fine-tuning on in-domain, task-specific data, VIP can provide dense visual reward for an extensive set of simulated and real-robot tasks, enabling diverse reward-based visual control methods and outperforming all prior pre-trained representations. Notably, VIP can enable simple, few-shot offline RL on a suite of real-world robot tasks with as few as 20 trajectories. ", "output": "**Decision:** True  \n**Reason:** The searched paper, \"Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training,\" aligns well with the user query regarding works similar to LIV (Learning Implicit Value) in learning implicit value functions. The paper introduces Value-Implicit Pre-training (VIP), which focuses on deriving a self-supervised goal-conditioned value-function objective. This approach implicitly defines the value function via a temporally smooth embedding distance, which is conceptually similar to the idea of learning implicit value functions. Both LIV and VIP aim to learn value functions in an implicit manner, with VIP specifically leveraging large-scale human video data for pre-training and applying it to robotic tasks. The theoretical framework and methodology of VIP, particularly its focus on implicit value learning, make it relevant to the query."}
{"input": "User Query: Which works directly establish correspondence between a pair of point clouds in non-rigid point cloud matching?\nTitle: CorrNet3D: Unsupervised End-to-end Learning of Dense Correspondence for 3D Point Clouds\nAbstract:  Abstract Motivated by the intuition that one can transform two aligned point clouds to each other more easily and meaningfully than a misaligned pair, we propose CorrNet3D – the first unsupervised and end-to-end deep learning-based framework – to drive the learning of dense correspondence between 3D shapes by means of deformation-like reconstruction to overcome the need for annotated data. Specifically, CorrNet3D consists of a deep feature embedding module and two novel modules called correspondence indicator and symmetric deformer. Feeding a pair of raw point clouds, our model first learns the pointwise features and passes them into the indicator to generate a learnable correspondence matrix used to permute the input pair. The symmetric deformer, with an additional regularized loss, transforms the two permuted point clouds to each other to drive the unsupervised learning of the correspondence. The extensive experiments on both synthetic and real-world datasets of rigid and non-rigid 3D shapes show our CorrNet3D outperforms state-of-the-art methods to a large extent, including those taking meshes as input. CorrNet3D is a flexible framework in that it can be easily adapted to supervised learning if annotated data are available. The source code and pre-trained model will be available at https://github.com/ZENGYIMING-EAMON/CorrNet3D.git. ", "output": "**Decision:** True  \n**Reason:** The paper \"CorrNet3D: Unsupervised End-to-end Learning of Dense Correspondence for 3D Point Clouds\" directly addresses the problem of establishing correspondence between a pair of point clouds, including non-rigid point cloud matching. The abstract explicitly mentions that the proposed framework, CorrNet3D, learns dense correspondence between 3D shapes by leveraging a deformation-like reconstruction approach. It also describes the use of a correspondence indicator module, which generates a correspondence matrix to permute the input pair of point clouds. Furthermore, the paper demonstrates the applicability of the method to both rigid and non-rigid 3D shapes, satisfying the requirement of the user query. Thus, the paper fully aligns with the detailed requirements of the query."}
{"input": "User Query: What research proposes methods to remove ghosting artifacts by using a spatial attention module?\nTitle: Attention-Guided Progressive Neural Texture Fusion for High Dynamic Range Image Restoration\nAbstract:  Abstract High Dynamic Range (HDR) imaging via multi-exposure fusion is an important task for most modern imaging platforms. In spite of recent developments in both hardware and algorithm innovations, challenges remain over content association ambiguities caused by saturation, motion, and various artifacts introduced during multi-exposure fusion such as ghosting, noise, and blur. In this work, we propose an Attention-guided Progressive Neural Texture Fusion (APNT-Fusion) HDR restoration model which aims to address these issues within one framework. An efficient two-stream structure is proposed which separately focuses on texture feature transfer over saturated regions and multi-exposure tonal and texture feature fusion. A neural feature transfer mechanism is proposed which establishes spatial correspondence between different exposures based on multi-scale VGG features in the masked saturated HDR domain for discriminative contextual clues over the ambiguous image areas. A progressive texture blending module is designed to blend the encoded two-stream features in a multi-scale and progressive manner. In addition, we introduce several novel attention mechanisms, i.e., the motion attention module detects and suppresses the content discrepancies among the reference images; the saturation attention module facilitates differentiating the misalignment caused by saturation from those caused by motion; and the scale attention module ensures texture blending consistency between different coder/decoder scales. We carry out comprehensive qualitative and quantitative evaluations and ablation studies, which validate that these novel modules work coherently under the same framework and outperform state-of-the-art methods. ", "output": "**Decision:** True  \n**Reason:** The searched paper, titled \"Attention-Guided Progressive Neural Texture Fusion for High Dynamic Range Image Restoration,\" satisfies the requirements of the user query. The paper explicitly addresses the removal of ghosting artifacts, which are common in HDR imaging, as part of its objectives. It proposes several novel attention mechanisms, including a **motion attention module**, which is designed to detect and suppress content discrepancies (such as ghosting) among reference images. While the abstract does not explicitly mention a \"spatial attention module\" by name, the motion attention module inherently operates in a spatial domain to address misalignments and discrepancies, which aligns with the concept of a spatial attention mechanism. Therefore, the paper fulfills the user's query by proposing a method to remove ghosting artifacts using an attention-based mechanism that operates spatially."}
{"input": "User Query: Which papers explored using a neural network to represent the solution of PDEs?\nTitle: The Deep Ritz method: A deep learning-based numerical algorithm for solving variational problems\nAbstract: ", "output": "**Decision:** True  \n**Reason:** The paper titled \"The Deep Ritz method: A deep learning-based numerical algorithm for solving variational problems\" explores using neural networks to represent the solution of partial differential equations (PDEs). The Deep Ritz method leverages neural networks as function approximators to solve variational problems, which are closely tied to PDEs. By representing the solution of the PDE through a neural network and optimizing the associated variational formulation, the paper directly addresses the use of neural networks for PDE solutions. This aligns with the requirements of the user query."}
{"input": "User Query: Which papers have made advancements in enhancing the rendering quality of the NeRF framework?\nTitle: Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields\nAbstract:  Abstract Despite the tremendous progress in neural radiance fields (NeRF), we still face a dilemma of the trade-off between quality and efficiency, e.g., MipNeRF [3] presents fine-detailed and anti-aliased renderings but takes days for training, while Instant-ngp [37] can accomplish the reconstruction in a few minutes but suffers from blurring or aliasing when rendering at various distances or resolutions due to ignoring the sampling area. To this end, we propose a novel Tri−MipTriMip\\operatorname{Tri-Mip} encoding (à la “mipmap”) that enables both instant reconstruction and anti-aliased high-fidelity rendering for neural radiance fields. The key is to factorize the pre-filtered 3D feature spaces in three orthogonal mipmaps. In this way, we can efficiently perform 3D area sampling by taking advantage of 2D pre-filtered feature maps, which significantly elevates the rendering quality without sacrificing efficiency. To cope with the novel Tri−MipTriMip\\operatorname{Tri-Mip} representation, we propose a cone-casting rendering technique to efficiently sample anti-aliased 3D features with the Tri−MipTriMip\\operatorname{Tri-Mip} encoding considering both pixel imaging and observing distance. Extensive experiments on both synthetic and real-world datasets demonstrate our method achieves state-of-the-art rendering quality and reconstruction speed while maintaining a compact representation that reduces 25%percent2525\\% model size compared against Instant-ngp. Code is available at the project webpage: https://wbhu.github.io/projects/Tri-MipRF ", "output": "**Decision:** True  \n**Reason:** The paper \"Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields\" directly addresses advancements in enhancing the rendering quality of the NeRF framework. The proposed Tri-Mip encoding method improves rendering quality by enabling anti-aliased high-fidelity rendering while maintaining efficiency. The paper introduces a novel representation and cone-casting rendering technique that significantly elevates rendering quality without sacrificing speed, as evidenced by its state-of-the-art performance in experiments. These contributions align well with the user query, which seeks papers that advance the rendering quality of NeRF."}
{"input": "User Query: Who introduced greedy rejection coding (GRC) which generalizes the rejection sampling algorithm to arbitrary probability spaces and arbitrary splitting functions?\nTitle: Faster Relative Entropy Coding with Greedy Rejection Coding\nAbstract:  Abstract Relative entropy coding (REC) algorithms encode a sample from a target distribution Q𝑄Q using a proposal distribution P𝑃P using as few bits as possible. Unlike entropy coding, REC does not assume discrete distributions or require quantisation. As such, it can be naturally integrated into communication pipelines such as learnt compression and differentially private federated learning. Unfortunately, despite their practical benefits, REC algorithms have not seen widespread application, due to their prohibitively slow runtimes or restrictive assumptions. In this paper, we make progress towards addressing these issues. We introduce Greedy Rejection Coding (GRC), which generalises the rejection based-algorithm of Harsha et al. (2007) to arbitrary probability spaces and partitioning schemes. We first show that GRC terminates almost surely and returns unbiased samples from Q𝑄Q, after which we focus on two of its variants: GRCS and GRCD. We show that for continuous Q𝑄Q and P𝑃P over ℝℝ\\mathbb{R} with unimodal density ratio d​Q/d​P𝑑𝑄𝑑𝑃dQ/dP, the expected runtime of GRCS is upper bounded by β​DKL​[Q∥P]+𝒪​(1)𝛽subscript𝐷KLdelimited-[]conditional𝑄𝑃𝒪1\\beta D_{\\mathrm{KL}}[Q\\|P]+\\mathcal{O}(1) where β≈4.82𝛽4.82\\beta\\approx 4.82, and its expected codelength is optimal. This makes GRCS the first REC algorithm with guaranteed optimal runtime for this class of distributions, up to the multiplicative constant β𝛽\\beta. This significantly improves upon the previous state-of-the-art method, A* coding (Flamich et al., 2022). Under the same assumptions, we experimentally observe and conjecture that the expected runtime and codelength of GRCD are upper bounded by DKL​[Q∥P]+𝒪​(1)subscript𝐷KLdelimited-[]conditional𝑄𝑃𝒪1D_{\\mathrm{KL}}[Q\\|P]+\\mathcal{O}(1). Finally, we evaluate GRC in a variational autoencoder-based compression pipeline on MNIST, and show that a modified ELBO and an index-compression method can further improve compression efficiency. ", "output": "**Decision:** True  \n**Reason:** The paper explicitly states that it introduces Greedy Rejection Coding (GRC), which generalizes the rejection sampling algorithm to arbitrary probability spaces and partitioning schemes (splitting functions). This directly satisfies the user query, as the paper identifies the authors as the ones who introduced GRC and explains its generalization of rejection sampling."}
{"input": "User Query: What are some papers about enhancing vision transformers using efficient local-attention modules?\nTitle: Glance-and-Gaze Vision Transformer\nAbstract:  Abstract Recently, there emerges a series of vision Transformers, which show superior performance with a more compact model size than conventional convolutional neural networks, thanks to the strong ability of Transformers to model long-range dependencies. However, the advantages of vision Transformers also come with a price: Self-attention, the core part of Transformer, has a quadratic complexity to the input sequence length. This leads to a dramatic increase of computation and memory cost with the increase of sequence length, thus introducing difficulties when applying Transformers to the vision tasks that require dense predictions based on high-resolution feature maps. In this paper, we propose a new vision Transformer, named Glance-and-Gaze Transformer (GG-Transformer), to address the aforementioned issues. It is motivated by the Glance and Gaze behavior of human beings when recognizing objects in natural scenes, with the ability to efficiently model both long-range dependencies and local context. In GG-Transformer, the Glance and Gaze behavior is realized by two parallel branches: The Glance branch is achieved by performing self-attention on the adaptively-dilated partitions of the input, which leads to a linear complexity while still enjoying a global receptive field; The Gaze branch is implemented by a simple depth-wise convolutional layer, which compensates local image context to the features obtained by the Glance mechanism. We empirically demonstrate our method achieves consistently superior performance over previous state-of-the-art Transformers on various vision tasks and benchmarks. The codes and models will be made available at https://github.com/yucornetto/GG-Transformer. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Glance-and-Gaze Vision Transformer\" fully satisfies the user query. The query specifically asks for papers about enhancing vision transformers using efficient local-attention modules. This paper introduces the GG-Transformer, which explicitly incorporates an efficient local-attention mechanism via the \"Gaze branch,\" implemented using a depth-wise convolutional layer. This branch is designed to capture local image context efficiently, addressing the computational challenges of self-attention in vision transformers. Additionally, the paper aligns with the query's focus on enhancing vision transformers by combining this local-attention mechanism with a global attention mechanism (the \"Glance branch\") to improve performance on vision tasks. Therefore, it is relevant and satisfies the detailed requirements of the query."}
{"input": "User Query: Any works that discuss the DINO model and its various uses?\nTitle: Deep ViT Features as Dense Visual Descriptors\nAbstract:  Abstract We study the use of deep features extracted from a pre-trained Vision Transformer (ViT) as dense visual descriptors. We observe and empirically demonstrate that such features, when extracted from a self-supervised ViT model (DINO-ViT), exhibit several striking properties, including: (i) the features encode powerful, well-localized semantic information, at high spatial granularity, such as object parts; (ii) the encoded semantic information is shared across related, yet different object categories, and (iii) positional bias changes gradually throughout the layers. These properties allow us to design simple methods for a variety of applications, including co-segmentation, part co-segmentation and semantic correspondences. To distill the power of ViT features from convoluted design choices, we restrict ourselves to lightweight zero-shot methodologies (e.g., binning and clustering) applied directly to the features. Since our methods require no additional training nor data, they are readily applicable across a variety of domains. We show by extensive qualitative and quantitative evaluation that our simple methodologies achieve competitive results with recent state-of-the-art supervised methods, and outperform previous unsupervised methods by a large margin. Code is available in dino-vit-features.github.io. ", "output": "**Decision:** True  \n**Reason:** The paper explicitly discusses the DINO model, specifically the DINO-ViT (a self-supervised Vision Transformer model), and explores its various uses. It highlights the properties of deep features extracted from DINO-ViT and demonstrates their applications in tasks such as co-segmentation, part co-segmentation, and semantic correspondences. These applications align with the user's query about the DINO model and its various uses. The detailed analysis and examples provided in the paper satisfy the requirements of the query."}
{"input": "User Query: Are there any studies that proposed methods to reduce the runtime overhead of Constrained beam search (CBS)?\nTitle: Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation\nAbstract:  Abstract The end-to-end nature of neural machine translation (NMT) removes many ways of manually guiding the translation process that were available in older paradigms. Recent work, however, has introduced a new capability: lexically constrained or guided decoding, a modification to beam search that forces the inclusion of pre-specified words and phrases in the output. However, while theoretically sound, existing approaches have computational complexities that are either linear Hokamp and Liu (2017) or exponential Anderson et al. (2017) in the number of constraints. We present an algorithm for lexically constrained decoding with a complexity of 𝒪​(1)𝒪1\\mathcal{O}(1) in the number of constraints. We demonstrate the algorithm’s remarkable ability to properly place these constraints, and use it to explore the shaky relationship between model and BLEU scores. Our implementation is available as part of Sockeye.111https://awslabs.github.io/sockeye/inference.html#lexical-constraints ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation\" satisfies the user query. The user is asking for studies that propose methods to reduce the runtime overhead of Constrained Beam Search (CBS). The paper addresses this by introducing an algorithm for lexically constrained decoding with a complexity of \\( \\mathcal{O}(1) \\) in the number of constraints, which is a significant improvement over the linear or exponential complexities of previous approaches. This directly relates to reducing the runtime overhead of CBS. The paper also demonstrates the algorithm's effectiveness and efficiency, making it relevant to the query."}
{"input": "User Query: What studies have discussed a ranked reward mechanism in self-competition?\nTitle: Ranked Reward: Enabling Self-Play Reinforcement Learning for Combinatorial Optimization\nAbstract:  Abstract Adversarial self-play in two-player games has delivered impressive results when used with reinforcement learning algorithms that combine deep neural networks and tree search. Algorithms like AlphaZero and Expert Iteration learn tabula-rasa, producing highly informative training data on the fly. However, the self-play training strategy is not directly applicable to single-player games. Recently, several practically important combinatorial optimization problems, such as the traveling salesman problem and the bin packing problem, have been reformulated as reinforcement learning problems, increasing the importance of enabling the benefits of self-play beyond two-player games. We present the Ranked Reward (R2) algorithm which accomplishes this by ranking the rewards obtained by a single agent over multiple games to create a relative performance metric. Results from applying the R2 algorithm to instances of a two-dimensional and three-dimensional bin packing problems show that it outperforms generic Monte Carlo tree search, heuristic algorithms and integer programming solvers. We also present an analysis of the ranked reward mechanism, in particular, the effects of problem instances with varying difficulty and different ranking thresholds. ", "output": "**Decision:** True  \n**Reason:** The searched paper directly addresses the concept of a ranked reward mechanism in self-competition. Specifically, the paper introduces the Ranked Reward (R2) algorithm, which employs a ranking of rewards obtained by a single agent over multiple games to create a relative performance metric. This mechanism enables self-play reinforcement learning to be applied to single-player combinatorial optimization problems, effectively creating a form of self-competition. Additionally, the paper provides an analysis of the ranked reward mechanism, including its effects under different conditions. Therefore, the paper fully satisfies the requirements of the user query."}
{"input": "User Query: Which works include a method for detecting self-contradictions or factual errors in responses to enhance quality?\nTitle: LM vs LM: Detecting Factual Errors via Cross Examination\nAbstract:  Abstract A prominent weakness of modern language models (LMs) is their tendency to generate factually incorrect text, which hinders their usability. A natural question is whether such factual errors can be detected automatically. Inspired by truth-seeking mechanisms in law, we propose a factuality evaluation framework for LMs that is based on cross-examination. Our key idea is that an incorrect claim is likely to result in inconsistency with other claims that the model generates. To discover such inconsistencies, we facilitate a multi-turn interaction between the LM that generated the claim and another LM (acting as an examiner) which introduces questions to discover inconsistencies. We empirically evaluate our method on factual claims made by multiple recent LMs on four benchmarks, finding that it outperforms existing methods and baselines, often by a large gap. Our results demonstrate the potential of using interacting LMs to capture factual errors. ", "output": "**Decision:** True  \n**Reason:** The paper \"LM vs LM: Detecting Factual Errors via Cross Examination\" fully satisfies the requirements of the user query. The method proposed in the paper explicitly focuses on detecting factual errors in language model (LM) responses. The approach involves a cross-examination framework where one LM generates claims and another LM acts as an examiner to identify inconsistencies or contradictions in the generated claims. This process directly addresses the detection of self-contradictions and factual errors, which aligns with the user's query about methods to enhance the quality of responses. The empirical evaluation further demonstrates the effectiveness of the proposed method, confirming its relevance to the query."}
{"input": "User Query: Which study investigated the emergence of object recognition in children?\nTitle: The developmental trajectory of object recognition robustness: children are like small adults but unlike big deep neural networks\nAbstract:  Abstract In laboratory object recognition tasks based on undistorted photographs, both adult humans and Deep Neural Networks (DNNs) perform close to ceiling. Unlike adults’, whose object recognition performance is robust against a wide range of image distortions, DNNs trained on standard ImageNet (1.3M images) perform poorly on distorted images. However, the last two years have seen impressive gains in DNN distortion robustness, predominantly achieved through ever-increasing large-scale datasets—orders of magnitude larger than ImageNet. While this simple brute-force approach is very effective in achieving human-level robustness in DNNs, it raises the question of whether human robustness, too, is simply due to extensive experience with (distorted) visual input during childhood and beyond. Here we investigate this question by comparing the core object recognition performance of 146 children (aged 4–15) against adults and against DNNs. We find, first, that already 4–6 year-olds showed remarkable robustness to image distortions and outperform DNNs trained on ImageNet. Second, we estimated the number of “images” children have been exposed to during their lifetime. Compared to various DNNs, children’s high robustness requires relatively little data. Third, when recognizing objects children—like adults but unlike DNNs—rely heavily on shape but not on texture cues. Together our results suggest that the remarkable robustness to distortions emerges early in the developmental trajectory of human object recognition and is unlikely the result of a mere accumulation of experience with distorted visual input. Even though current DNNs match human performance regarding robustness they seem to rely on different and more data-hungry strategies to do so. Keywords: bject recognition, robustness, out-of-distribution, deep learning, development, generalization, children, deep neural networks, machine vision  ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"The developmental trajectory of object recognition robustness: children are like small adults but unlike big deep neural networks\"* satisfies the user query. The study explicitly investigates the emergence of object recognition in children by examining how children aged 4–15 recognize objects and their robustness to image distortions. It compares children's performance to adults and deep neural networks, providing insights into the developmental trajectory of object recognition. The abstract highlights that the study explores when and how robustness to distortions emerges in children, which directly aligns with the user's query about the emergence of object recognition in children."}
{"input": "User Query: Which papers studied RL with function approximation?\nTitle: Bilinear Classes: A Structural Framework for Provable Generalization in RL\nAbstract:  Abstract This work introduces Bilinear Classes, a new structural framework, which permit generalization in reinforcement learning in a wide variety of settings through the use of function approximation. The framework incorporates nearly all existing models in which a polynomial sample complexity is achievable, and, notably, also includes new models, such as the Linear Q∗/V∗superscript𝑄superscript𝑉Q^{*}/V^{*} model in which both the optimal Q𝑄Q-function and the optimal V𝑉V-function are linear in some known feature space. Our main result provides an RL algorithm which has polynomial sample complexity for Bilinear Classes; notably, this sample complexity is stated in terms of a reduction to the generalization error of an underlying supervised learning sub-problem. These bounds nearly match the best known sample complexity bounds for existing models. Furthermore, this framework also extends to the infinite dimensional (RKHS) setting: for the the Linear Q∗/V∗superscript𝑄superscript𝑉Q^{*}/V^{*} model, linear MDPs, and linear mixture MDPs, we provide sample complexities that have no explicit dependence on the explicit feature dimension (which could be infinite), but instead depends only on information theoretic quantities. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Bilinear Classes: A Structural Framework for Provable Generalization in RL\"* explicitly studies reinforcement learning (RL) with function approximation. The abstract mentions that the framework introduced, Bilinear Classes, permits generalization in RL through the use of function approximation. It also discusses models where the optimal Q-function and V-function are linear in a known feature space, which is a form of function approximation. Furthermore, the framework extends to infinite-dimensional settings (e.g., RKHS), which is another form of function approximation. Therefore, the paper fully satisfies the requirements of the user query."}
{"input": "User Query: Which papers provide notable examples of perturbation-based post hoc explanation methods?\nTitle: “Why Should I Trust You?” Explaining the Predictions of Any Classifier\nAbstract:  Abstract Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted. ", "output": "**Decision:** True  \n**Reason:** The paper titled “Why Should I Trust You?” Explaining the Predictions of Any Classifier introduces LIME (Local Interpretable Model-agnostic Explanations), which is a perturbation-based post hoc explanation method. LIME works by perturbing the input data locally around a prediction and then fitting an interpretable model (e.g., a linear model) to approximate the behavior of the original black-box model in that local region. This approach aligns directly with the user query, as it provides a notable example of a perturbation-based post hoc explanation method. The paper also demonstrates the utility of LIME in explaining predictions for various models and use cases, further solidifying its relevance to the query."}
{"input": "User Query: Which studies investigate adversarial robustness in few-shot settings?\nTitle: Adversarially Robust Few-Shot Learning: A Meta-Learning Approach\nAbstract:  Abstract Previous work on adversarially robust neural networks for image classification requires large training sets and computationally expensive training procedures. On the other hand, few-shot learning methods are highly vulnerable to adversarial examples. The goal of our work is to produce networks which both perform well at few-shot classification tasks and are simultaneously robust to adversarial examples. We develop an algorithm, called Adversarial Querying (AQ), for producing adversarially robust meta-learners, and we thoroughly investigate the causes for adversarial vulnerability. Moreover, our method achieves far superior robust performance on few-shot image classification tasks, such as Mini-ImageNet and CIFAR-FS, than robust transfer learning. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Adversarially Robust Few-Shot Learning: A Meta-Learning Approach\"* explicitly investigates adversarial robustness in few-shot settings. The abstract clearly states that the study focuses on producing networks that perform well in few-shot classification tasks while being robust to adversarial examples. It introduces a specific algorithm, Adversarial Querying (AQ), designed for adversarially robust meta-learners, and evaluates its performance on few-shot image classification tasks. This aligns directly with the user query, as the study addresses both adversarial robustness and few-shot learning."}
{"input": "User Query: Which works have leveraged diffusion models in the field of natural language processing?\nTitle: Structured Denoising Diffusion Models in Discrete State-Spaces\nAbstract: Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. 2021, by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.", "output": "**Decision:** True  \n**Reason:** The paper titled \"Structured Denoising Diffusion Models in Discrete State-Spaces\" explicitly discusses the application of diffusion models to text generation, which is a task in the field of natural language processing (NLP). The abstract mentions that the proposed Discrete Denoising Diffusion Probabilistic Models (D3PMs) achieve strong results on character-level text generation and scale to large vocabularies on the LM1B dataset, which is a benchmark for language modeling. This demonstrates that the work leverages diffusion models for NLP tasks, satisfying the requirements of the user query."}
{"input": "User Query: Which studies integrated a pretrained image encoder with autoregressive language model?\nTitle: BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\nAbstract:  Abstract The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model’s emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions. ", "output": "**Decision:** True  \n**Reason:** The paper \"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\" satisfies the user query. It explicitly describes integrating a pretrained image encoder (frozen) with a large language model, which is autoregressive in nature. The paper introduces a lightweight Querying Transformer to bridge the gap between the modalities and details a two-stage pre-training process. The second stage specifically focuses on vision-to-language generative learning, which involves the use of a frozen language model. This aligns with the query's requirement of integrating a pretrained image encoder with an autoregressive language model."}
{"input": "User Query: Which studies have proposed methods for controlling the generated content in T2I diffusion models besides the provided text caption?\nTitle: Gligen: Open-Set Grounded Text-to-Image Generation\nAbstract:  Abstract Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose Gligen, Grounded-Language-to-Image Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. Gligen’s zero-shot performance on COCO and LVIS outperforms existing supervised layout-to-image baselines by a large margin.  ††§§\\S Part of the work performed at Microsoft; ¶¶{\\P} Co-senior authors  ", "output": "**Decision:** True  \n**Reason:** The paper \"Gligen: Open-Set Grounded Text-to-Image Generation\" satisfies the user query because it proposes a method for controlling the generated content in text-to-image (T2I) diffusion models beyond the provided text caption. Specifically, the paper introduces Gligen, which allows the model to be conditioned on additional grounding inputs, such as bounding boxes, in addition to text captions. This approach enhances controllability by incorporating spatial and grounding information, which aligns with the user's request for methods that go beyond text captions for content control."}
{"input": "User Query: Could you provide me some papers discussing the use of natural language to refer target objects to robots for goal-conditioned manipulation tasks?\nTitle: Language-Conditioned Imitation Learning for Robot Manipulation Tasks\nAbstract:  Abstract Imitation learning is a popular approach for teaching motor skills to robots. However, most approaches focus on extracting policy parameters from execution traces alone (i.e., motion trajectories and perceptual data). No adequate communication channel exists between the human expert and the robot to describe critical aspects of the task, such as the properties of the target object or the intended shape of the motion. Motivated by insights into the human teaching process, we introduce a method for incorporating unstructured natural language into imitation learning. At training time, the expert can provide demonstrations along with verbal descriptions in order to describe the underlying intent (e.g., “go to the large green bowl”). The training process then interrelates these two modalities to encode the correlations between language, perception, and motion. The resulting language-conditioned visuomotor policies can be conditioned at runtime on new human commands and instructions, which allows for more fine-grained control over the trained policies while also reducing situational ambiguity. We demonstrate in a set of simulation experiments how our approach can learn language-conditioned manipulation policies for a seven-degree-of-freedom robot arm and compare the results to a variety of alternative methods. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Language-Conditioned Imitation Learning for Robot Manipulation Tasks\" satisfies the user query. The user is looking for papers discussing the use of natural language to refer to target objects for goal-conditioned manipulation tasks. This paper explicitly addresses the incorporation of unstructured natural language into imitation learning for robot manipulation. It describes how verbal descriptions (e.g., \"go to the large green bowl\") provided during training are used to condition visuomotor policies, enabling robots to interpret and act upon new human commands at runtime. The paper also demonstrates the application of this approach to manipulation tasks using a robot arm, which aligns with the user's interest in goal-conditioned manipulation tasks."}
{"input": "User Query: Can you name papers that tackle challenges in multiagent collaboration to get rid of the limitations in single-agent perception in autonomous driving?\nTitle: Collaborative Multi-Object Tracking with Conformal Uncertainty Propagation\nAbstract:  Abstract Object detection and multiple object tracking (MOT) are essential components of self-driving systems. Accurate detection and uncertainty quantification are both critical for onboard modules, such as perception, prediction, and planning, to improve the safety and robustness of autonomous vehicles. Collaborative object detection (COD) has been proposed to improve detection accuracy and reduce uncertainty by leveraging the viewpoints of multiple agents. However, little attention has been paid to how to leverage the uncertainty quantification from COD to enhance MOT performance. In this paper, as the first attempt to address this challenge, we design an uncertainty propagation framework called MOT-CUP. Our framework first quantifies the uncertainty of COD through direct modeling and conformal prediction, and propagates this uncertainty information into the motion prediction and association steps. MOT-CUP is designed to work with different collaborative object detectors and baseline MOT algorithms. We evaluate MOT-CUP on V2X-Sim, a comprehensive collaborative perception dataset, and demonstrate a 2%percent22\\% improvement in accuracy and a 2.67×2.67\\times reduction in uncertainty compared to the baselines, e.g. SORT and ByteTrack. In scenarios characterized by high occlusion levels, our MOT-CUP demonstrates a noteworthy 4.01%percent4.014.01\\% improvement in accuracy. MOT-CUP demonstrates the importance of uncertainty quantification in both COD and MOT, and provides the first attempt to improve the accuracy and reduce the uncertainty in MOT based on COD through uncertainty propagation. Our code is public on https://coperception.github.io/MOT-CUP/. ", "output": "**Decision:** True  \n**Reason:** The searched paper, \"Collaborative Multi-Object Tracking with Conformal Uncertainty Propagation,\" satisfies the requirements of the user query. The paper directly addresses challenges in multiagent collaboration by leveraging collaborative object detection (COD) to improve perception in autonomous driving. It specifically focuses on overcoming limitations of single-agent perception by utilizing the viewpoints of multiple agents to enhance detection accuracy and reduce uncertainty. Furthermore, the paper proposes a novel framework, MOT-CUP, which propagates uncertainty information from COD into multi-object tracking (MOT), thereby improving performance in scenarios with high occlusion. This aligns with the user's interest in multiagent collaboration to address the limitations of single-agent perception in autonomous driving."}
{"input": "User Query: Which papers employed memory networks in video object tracking and segmentation?\nTitle: Video Object Segmentation using Space-Time Memory Networks\nAbstract:  Abstract We propose a novel solution for semi-supervised video object segmentation. By the nature of the problem, available cues (e.g. video frame(s) with object masks) become richer with the intermediate predictions. However, the existing methods are unable to fully exploit this rich source of information. We resolve the issue by leveraging memory networks and learn to read relevant information from all available sources. In our framework, the past frames with object masks form an external memory, and the current frame as the query is segmented using the mask information in the memory. Specifically, the query and the memory are densely matched in the feature space, covering all the space-time pixel locations in a feed-forward fashion. Contrast to the previous approaches, the abundant use of the guidance information allows us to better handle the challenges such as appearance changes and occlussions. We validate our method on the latest benchmark sets and achieved the state-of-the-art performance (overall score of 79.4 on Youtube-VOS val set, 𝒥𝒥\\mathcal{J} of 88.7 and 79.2 on DAVIS 2016/2017 val set respectively) while having a fast runtime (0.16 second/frame on DAVIS 2016 val set). ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Video Object Segmentation using Space-Time Memory Networks\" explicitly mentions employing memory networks in the context of video object segmentation. The abstract describes how the method leverages memory networks to store past frames with object masks as an external memory, which is then used to segment the current frame. This approach aligns with the user's query as it involves the use of memory networks in video object segmentation. While the paper focuses on segmentation rather than tracking, the query does not explicitly require both tracking and segmentation to be addressed simultaneously. Therefore, the paper satisfies the requirements of the user query."}
{"input": "User Query: Could you tell me about the studies that introduce semi-supervised approaches for building behavior embeddings using task programming?\nTitle: Task Programming: Learning Data Efficient Behavior Representations\nAbstract:  Abstract Specialized domain knowledge is often necessary to accurately annotate training sets for in-depth analysis, but can be burdensome and time-consuming to acquire from domain experts. This issue arises prominently in automated behavior analysis, in which agent movements or actions of interest are detected from video tracking data. To reduce annotation effort, we present TREBA: a method to learn annotation-sample efficient trajectory embedding for behavior analysis, based on multi-task self-supervised learning. The tasks in our method can be efficiently engineered by domain experts through a process we call “task programming”, which uses programs to explicitly encode structured knowledge from domain experts. Total domain expert effort can be reduced by exchanging data annotation time for the construction of a small number of programmed tasks. We evaluate this trade-off using data from behavioral neuroscience, in which specialized domain knowledge is used to identify behaviors. We present experimental results in three datasets across two domains: mice and fruit flies. Using embeddings from TREBA, we reduce annotation burden by up to a factor of 10 without compromising accuracy compared to state-of-the-art features. Our results thus suggest that task programming and self-supervision can be an effective way to reduce annotation effort for domain experts. ", "output": "**Decision:** True  \n**Reason:** The paper \"Task Programming: Learning Data Efficient Behavior Representations\" satisfies the requirements of the user query. It introduces a method called TREBA that uses task programming to build behavior embeddings. The approach is based on multi-task self-supervised learning, which aligns with the concept of semi-supervised learning as it leverages structured knowledge (via task programming) without requiring extensive labeled data. The method explicitly focuses on reducing annotation effort by using domain expert-designed tasks, which are programmed to encode structured knowledge. This aligns with the user query's focus on semi-supervised approaches for building behavior embeddings using task programming."}
{"input": "User Query: Can you identify the studies that have contributed to the area of Diffusion models in image generation?\nTitle: Generative Modeling by Estimating Gradients of the Data Distribution\nAbstract:  Abstract We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Generative Modeling by Estimating Gradients of the Data Distribution\" contributes to the area of diffusion models in image generation. It introduces a generative modeling framework based on estimating gradients of the data distribution and sampling via Langevin dynamics. This approach is closely related to the foundational ideas behind diffusion models, which involve perturbing data with noise and progressively refining samples to generate realistic images. The paper's methodology of using noise perturbations at different levels and annealed Langevin dynamics aligns with the principles of diffusion models. Furthermore, the paper demonstrates its effectiveness in image generation tasks on datasets like MNIST, CelebA, and CIFAR-10, making it relevant to the user query."}
{"input": "User Query: Which works include diversifying caricature generation to multiple facial exaggeration types?\nTitle: CariMe: Unpaired Caricature Generation with Multiple Exaggerations\nAbstract:  Abstract Caricature generation aims to translate real photos into caricatures with artistic styles and shape exaggerations while maintaining the identity of the subject. Different from the generic image-to-image translation, drawing a caricature automatically is a more challenging task due to the existence of various spacial deformations. Previous caricature generation methods are obsessed with predicting definite image warping from a given photo while ignoring the intrinsic representation and distribution for exaggerations in caricatures. This limits their ability on diverse exaggeration generation. In this paper, we generalize the caricature generation problem from instance-level warping prediction to distribution-level deformation modeling. Based on this assumption, we present the first exploration for unpaired CARIcature generation with Multiple Exaggerations (CariMe). Technically, we propose a Multi-exaggeration Warper network to learn the distribution-level mapping from photo to facial exaggerations. This makes it possible to generate diverse and reasonable exaggerations from randomly sampled warp codes given one input photo. To better represent the facial exaggeration and produce fine-grained warping, a deformation-field-based warping method is also proposed, which helps us to capture more detailed exaggerations than other point-based warping methods. Experiments and two perceptual studies prove the superiority of our method comparing with other state-of-the-art methods, showing the improvement of our work on caricature generation. ", "output": "**Decision:** True  \n**Reason:** The paper \"CariMe: Unpaired Caricature Generation with Multiple Exaggerations\" explicitly addresses the diversification of caricature generation to multiple facial exaggeration types. The abstract highlights the development of a \"Multi-exaggeration Warper network\" designed to learn distribution-level mappings, enabling the generation of diverse and reasonable facial exaggerations from randomly sampled warp codes for a given input photo. This directly aligns with the user query, as the paper focuses on diversifying caricature generation to multiple facial exaggeration types."}
{"input": "User Query: Which works propose ways to improve dense retrieval models?\nTitle: ERNIE-Search: Bridging Cross-Encoder with Dual-Encoder via Self On-the-fly Distillation for Dense Passage Retrieval\nAbstract:  Abstract Neural retrievers based on pre-trained language models (PLMs), such as dual-encoders, have achieved promising performance on the task of open-domain question answering (QA). Their effectiveness can further reach new state-of-the-arts by incorporating cross-architecture knowledge distillation. However, most of the existing studies just directly apply conventional distillation methods. They fail to consider the particular situation where the teacher and student have different structures. In this paper, we propose a novel distillation method that significantly advances cross-architecture distillation for dual-encoders. Our method 1) introduces a self on-the-fly distillation method that can effectively distill late interaction (i.e., ColBERT) to vanilla dual-encoder, and 2) incorporates a cascade distillation process to further improve the performance with a cross-encoder teacher. Extensive experiments are conducted to validate that our proposed solution outperforms strong baselines and establish a new state-of-the-art on open-domain QA benchmarks.  ", "output": "**Decision:** True  \n**Reason:** The paper \"ERNIE-Search: Bridging Cross-Encoder with Dual-Encoder via Self On-the-fly Distillation for Dense Passage Retrieval\" satisfies the user query as it explicitly proposes methods to improve dense retrieval models. Dense retrieval models, such as dual-encoders, are the focus of the paper, and the authors introduce a novel self on-the-fly distillation method and a cascade distillation process to enhance the performance of these models. The proposed methods aim to address the challenges of cross-architecture knowledge distillation and improve the effectiveness of dense retrieval models, as demonstrated by their experiments and results on open-domain QA benchmarks. Thus, the paper aligns with the user's request for works proposing ways to improve dense retrieval models."}
{"input": "User Query: Can you name a paper that discusses interpolation of environment distribution for enhancement in reinforcement learning?\nTitle: Curriculum Reinforcement Learning using Optimal Transport via Gradual Domain Adaptation\nAbstract:  Abstract Curriculum Reinforcement Learning (CRL) aims to create a sequence of tasks, starting from easy ones and gradually learning towards difficult tasks. In this work, we focus on the idea of framing CRL as interpolations between a source (auxiliary) and a target task distribution. Although existing studies have shown the great potential of this idea, it remains unclear how to formally quantify and generate the movement between task distributions. Inspired by the insights from gradual domain adaptation in semi-supervised learning, we create a natural curriculum by breaking down the potentially large task distributional shift in CRL into smaller shifts. We propose GRADIENT, which formulates CRL as an optimal transport problem with a tailored distance metric between tasks. Specifically, we generate a sequence of task distributions as a geodesic interpolation (i.e., Wasserstein barycenter) between the source and target distributions. Different from many existing methods, our algorithm considers a task-dependent contextual distance metric and is capable of handling nonparametric distributions in both continuous and discrete context settings. In addition, we theoretically show that GRADIENT enables smooth transfer between subsequent stages in the curriculum under certain conditions. We conduct extensive experiments in locomotion and manipulation tasks and show that our proposed GRADIENT achieves higher performance than baselines in terms of learning efficiency and asymptotic performance. ", "output": "**Decision:** True  \n**Reason:** The searched paper, \"Curriculum Reinforcement Learning using Optimal Transport via Gradual Domain Adaptation,\" directly discusses the interpolation of environment (task) distributions for enhancement in reinforcement learning. The abstract explicitly mentions framing Curriculum Reinforcement Learning (CRL) as interpolations between a source and target task distribution. It further elaborates on generating a sequence of task distributions as a geodesic interpolation (i.e., Wasserstein barycenter) between these distributions. This aligns well with the user's query, which seeks a paper discussing interpolation of environment distribution for reinforcement learning enhancement. Hence, the paper satisfies the detailed requirements of the query."}
{"input": "User Query: Which works have attempted to generate multi-person interactions?\nTitle: Human Motion Diffusion as a Generative Prior\nAbstract:  Abstract. Recent work has demonstrated the significant potential of denoising diffusion models for generating human motion, including text-to-motion capabilities. However, these methods are restricted by the paucity of annotated motion data, a focus on single-person motions, and a lack of detailed control. In this paper, we introduce three forms of composition based on diffusion priors: sequential, parallel, and model composition. Using sequential composition, we tackle the challenge of long sequence generation. We introduce DoubleTake, an inference-time method with which we generate long animations consisting of sequences of prompted intervals and their transitions, using a prior trained only for short clips. Using parallel composition, we show promising steps toward two-person generation. Beginning with two fixed priors as well as a few two-person training examples, we learn a slim communication block, ComMDM, to coordinate interaction between the two resulting motions. Lastly, using model composition, we first train individual priors to complete motions that realize a prescribed motion for a given joint. We then introduce DiffusionBlending, an interpolation mechanism to effectively blend several such models to enable flexible and efficient fine-grained joint and trajectory-level control and editing. We evaluate the composition methods using an off-the-shelf motion diffusion model, and further compare the results to dedicated models trained for these specific tasks. https://priormdm.github.io/priorMDM-page/ 111Our code and trained models are available at https://github.com/priorMDM/priorMDM. ††* The authors contributed equally ", "output": "**Decision:** True  \n**Reason:** The paper explicitly addresses the generation of multi-person interactions. In the abstract, it mentions the use of \"parallel composition\" to achieve \"two-person generation.\" The authors describe a method involving a communication block, ComMDM, which coordinates interactions between two motions. This directly aligns with the user query about works attempting to generate multi-person interactions."}
{"input": "User Query: What papers focus on backward-warping-based Video Frame Interpolation?\nTitle: AMT: All-Pairs Multi-Field Transforms for Efficient Frame Interpolation\nAbstract:  Abstract We present All-Pairs Multi-Field Transforms (AMT), a new network architecture for video frame interpolation. It is based on two essential designs. First, we build bidirectional correlation volumes for all pairs of pixels, and use the predicted bilateral flows to retrieve correlations for updating both flows and the interpolated content feature. Second, we derive multiple groups of fine-grained flow fields from one pair of updated coarse flows for performing backward warping on the input frames separately. Combining these two designs enables us to generate promising task-oriented flows and reduce the difficulties in modeling large motions and handling occluded areas during frame interpolation. These qualities promote our model to achieve state-of-the-art performance on various benchmarks with high efficiency. Moreover, our convolution-based model competes favorably compared to Transformer-based models in terms of accuracy and efficiency. Our code is available at https://github.com/MCG-NKU/AMT. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"AMT: All-Pairs Multi-Field Transforms for Efficient Frame Interpolation\" satisfies the user query as it explicitly mentions the use of backward warping for video frame interpolation. The abstract highlights that the method derives multiple groups of fine-grained flow fields from coarse flows, which are then used to perform backward warping on the input frames. This directly aligns with the user's interest in papers focusing on backward-warping-based video frame interpolation."}
{"input": "User Query: Which works represent the gradient-based or backpropagation-based category in XAI methodology?\nTitle: Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\nAbstract:  Abstract We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent and explainable. Our approach – Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say ‘dog’ in a classification network or a sequence of words in captioning network) flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, all without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are robust to adversarial perturbations, (d) are more faithful to the underlying model, and (e) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show that even non-attention based models learn to localize discriminative regions of input image. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names netdissect  to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a ‘stronger’ deep network from a ‘weaker’ one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo on CloudCV agrawal2015cloudcv 111http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\" fully satisfies the requirements of the user query. The methodology described in the paper, Gradient-weighted Class Activation Mapping (Grad-CAM), explicitly uses gradients flowing into the final convolutional layer of a neural network to produce visual explanations. This approach is inherently gradient-based, as it relies on backpropagation to compute the gradients of the target concept with respect to the feature maps. The paper also emphasizes its applicability across various CNN architectures and tasks, further solidifying its position as a gradient-based method in the field of Explainable AI (XAI). Therefore, it directly aligns with the query's focus on works representing the gradient-based or backpropagation-based category in XAI methodology."}
{"input": "User Query: Can you give some examples of work on retraining-free sampler strategy for diffusion model acceleration?\nTitle: Denoising Diffusion Implicit Models\nAbstract:  Abstract Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples 10×10\\times to 50×50\\times faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error. ", "output": "**Decision:** True  \n**Reason:** The paper \"Denoising Diffusion Implicit Models\" (DDIMs) satisfies the user query as it directly addresses a retraining-free sampler strategy for accelerating diffusion models. The key contribution of the paper is the introduction of DDIMs, which share the same training procedure as DDPMs (Denoising Diffusion Probabilistic Models). This means that the acceleration achieved by DDIMs does not require retraining the diffusion model. Instead, the paper proposes a modification in the sampling process by generalizing DDPMs to non-Markovian diffusion processes, enabling faster sampling without altering the original training. This aligns with the user's request for examples of retraining-free sampler strategies for diffusion model acceleration."}
{"input": "User Query: What papers discuss the fairness of an objective different from clustering in relation to suodular maximization?\nTitle: Fairness in Streaming Submodular Maximization: Algorithms and Hardness\nAbstract:  Abstract Submodular maximization has become established as the method of choice for the task of selecting representative and diverse summaries of data. However, if datapoints have sensitive attributes such as gender or age, such machine learning algorithms, left unchecked, are known to exhibit bias: under- or over-representation of particular groups. This has made the design of fair machine learning algorithms increasingly important. In this work we address the question: Is it possible to create fair summaries for massive datasets? To this end, we develop the first streaming approximation algorithms for submodular maximization under fairness constraints, for both monotone and non-monotone functions. We validate our findings empirically on exemplar-based clustering, movie recommendation, DPP-based summarization, and maximum coverage in social networks, showing that fairness constraints do not significantly impact utility. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Fairness in Streaming Submodular Maximization: Algorithms and Hardness\" satisfies the user's query. The paper explicitly discusses fairness in the context of submodular maximization for objectives beyond clustering. Specifically, it mentions applications such as movie recommendation, DPP-based summarization, and maximum coverage in social networks, in addition to exemplar-based clustering. These objectives are distinct from clustering and are analyzed with fairness constraints, making the paper relevant to the user's query about fairness in objectives other than clustering in relation to submodular maximization."}
{"input": "User Query: Which papers discuss the phenomenon of Negative Transfer (NT) in machine learning?\nTitle: Transferability in Deep Learning: A Survey\nAbstract:  Abstract The success of deep learning algorithms generally depends on large-scale data, while humans appear to have inherent ability of knowledge transfer, by recognizing and applying relevant knowledge from previous learning experiences when encountering and solving unseen tasks. Such an ability to acquire and reuse knowledge is known as transferability in deep learning. It has formed the long-term quest towards making deep learning as data-efficient as human learning, and has been motivating fruitful design of more powerful deep learning algorithms. We present this survey to connect different isolated areas in deep learning with their relation to transferability, and to provide a unified and complete view to investigating transferability through the whole lifecycle of deep learning. The survey elaborates the fundamental goals and challenges in parallel with the core principles and methods, covering recent cornerstones in deep architectures, pre-training, task adaptation and domain adaptation. This highlights unanswered questions on the appropriate objectives for learning transferable knowledge and for adapting the knowledge to new tasks and domains, avoiding catastrophic forgetting and negative transfer. Finally, we implement a benchmark and an open-source library, enabling a fair evaluation of deep learning methods in terms of transferability. ", "output": "**Decision:** True  \n**Reason:** The abstract of the paper explicitly mentions the phenomenon of \"negative transfer\" as one of the challenges in transferability in deep learning. While the primary focus of the paper is on transferability in general, it includes a discussion of avoiding negative transfer, which directly aligns with the user's query about papers discussing the phenomenon of negative transfer in machine learning. Therefore, the paper satisfies the requirements of the user query."}
{"input": "User Query: What studies are geared towards high-quality instance segmentation by making full use of high-resolution masks?\nTitle: DCT-Mask: Discrete Cosine Transform Mask Representation for Instance Segmentation\nAbstract:  Abstract Binary grid mask representation is broadly used in instance segmentation. A representative instantiation is Mask R-CNN which predicts masks on a 28×28282828\\times 28 binary grid. Generally, a low-resolution grid is not sufficient to capture the details, while a high-resolution grid dramatically increases the training complexity. In this paper, we propose a new mask representation by applying the discrete cosine transform(DCT) to encode the high-resolution binary grid mask into a compact vector. Our method, termed DCT-Mask, could be easily integrated into most pixel-based instance segmentation methods. Without any bells and whistles, DCT-Mask yields significant gains on different frameworks, backbones, datasets, and training schedules. It does not require any pre-processing or pre-training, and almost no harm to the running speed. Especially, for higher-quality annotations and more complex backbones, our method has a greater improvement. Moreover, we analyze the performance of our method from the perspective of the quality of mask representation. The main reason why DCT-Mask works well is that it obtains a high-quality mask representation with low complexity. Code is available at https://github.com/aliyun/DCT-Mask.git ", "output": "**Decision:** True  \n**Reason:** The paper \"DCT-Mask: Discrete Cosine Transform Mask Representation for Instance Segmentation\" satisfies the user query as it directly addresses the challenge of achieving high-quality instance segmentation by making full use of high-resolution masks. The paper proposes a novel method, DCT-Mask, which encodes high-resolution binary grid masks into a compact vector using the discrete cosine transform (DCT). This approach enables the use of high-resolution masks without significantly increasing training complexity, which is a key aspect of the user query. Furthermore, the paper emphasizes the quality of mask representation and demonstrates improvements across various frameworks, backbones, datasets, and training schedules, showcasing its effectiveness in producing high-quality instance segmentation. Therefore, the paper aligns well with the user's requirements."}
{"input": "User Query: Which works discuss the role of human input in natural language processing tasks?\nTitle: Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization\nAbstract:  Abstract ††footnotetext: **{}^{\\textbf{*}}Denotes Equal Contribution We tackle the problem of aligning pre-trained large language models (LMs) with human preferences. If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework. However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment. Thus, a question rises in the research community: is RL a practical paradigm for NLP? To help answer this, we first introduce an open-source modular library, RL4LMs111Code: https://github.com/allenai/RL4LMs,222Project Website: https://rl4lms.apps.allenai.org/ for optimizing language generators with RL. The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al., 2020) with an arbitrary reward function. Next, we present the GRUE (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference. GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally, we introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language Policy Optimization) that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al., 2017)), based on both automatic and human evaluations. ", "output": "**Decision:** True  \n**Reason:** The paper discusses the role of human input in natural language processing tasks, specifically in the context of aligning pre-trained large language models (LMs) with human preferences. The abstract explicitly mentions that the reward functions used in the GRUE benchmark capture automated measures of human preference, which directly ties to the role of human input in evaluating and guiding NLP tasks. Additionally, the paper demonstrates how reinforcement learning techniques can be used to optimize language models to better align with human preferences, further emphasizing the importance of human input in the process. Therefore, the paper satisfies the requirements of the user query."}
{"input": "User Query: In what works have researchers aimed to reduce training time through the use of linear decay for learning rate?\nTitle: Budgeted Training: Rethinking Deep Neural Network Training Under Resource Constraints\nAbstract:  Abstract In most practical settings and theoretical analyses, one assumes that a model can be trained until convergence. However, the growing complexity of machine learning datasets and models may violate such assumptions. Indeed, current approaches for hyper-parameter tuning and neural architecture search tend to be limited by practical resource constraints. Therefore, we introduce a formal setting for studying training under the non-asymptotic, resource-constrained regime, i.e., budgeted training. We analyze the following problem: “given a dataset, algorithm, and fixed resource budget, what is the best achievable performance?” We focus on the number of optimization iterations as the representative resource. Under such a setting, we show that it is critical to adjust the learning rate schedule according to the given budget. Among budget-aware learning schedules, we find simple linear decay to be both robust and high-performing. We support our claim through extensive experiments with state-of-the-art models on ImageNet (image classification), Kinetics (video classification), MS COCO (object detection and instance segmentation), and Cityscapes (semantic segmentation). We also analyze our results and find that the key to a good schedule is budgeted convergence, a phenomenon whereby the gradient vanishes at the end of each allowed budget. We also revisit existing approaches for fast convergence and show that budget-aware learning schedules readily outperform such approaches under (the practical but under-explored) budgeted training setting. ", "output": "**Decision:** True  \n**Reason:** The searched paper explicitly addresses the use of linear decay for learning rate as a method to optimize training under resource constraints, which includes reducing training time. The abstract highlights that the research focuses on \"budgeted training,\" where the number of optimization iterations is constrained, and demonstrates that linear decay is a robust and high-performing learning rate schedule in such scenarios. This aligns with the user query, as the paper investigates reducing training time (or optimizing performance within a limited training budget) through the use of linear decay for the learning rate."}
{"input": "User Query: What studies estimate the camera pose with inverse NeRF optimization when the neural implicit network is fully trained?\nTitle: iNeRF: Inverting Neural Radiance Fields for Pose Estimation\nAbstract:  Abstract We present iNeRF, a framework that performs mesh-free pose estimation by “inverting” a Neural Radiance Field (NeRF). NeRFs have been shown to be remarkably effective for the task of view synthesis — synthesizing photorealistic novel views of real-world scenes or objects. In this work, we investigate whether we can apply analysis-by-synthesis via NeRF for mesh-free, RGB-only 6DoF pose estimation – given an image, find the translation and rotation of a camera relative to a 3D object or scene. Our method assumes that no object mesh models are available during either training or test time. Starting from an initial pose estimate, we use gradient descent to minimize the residual between pixels rendered from a NeRF and pixels in an observed image. In our experiments, we first study 1) how to sample rays during pose refinement for iNeRF to collect informative gradients and 2) how different batch sizes of rays affect iNeRF on a synthetic dataset. We then show that for complex real-world scenes from the LLFF dataset, iNeRF can improve NeRF by estimating the camera poses of novel images and using these images as additional training data for NeRF. Finally, we show iNeRF can perform category-level object pose estimation, including object instances not seen during training, with RGB images by inverting a NeRF model inferred from a single view. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"iNeRF: Inverting Neural Radiance Fields for Pose Estimation\" satisfies the user query. The query specifically asks for studies that estimate camera pose using inverse NeRF optimization when the neural implicit network (NeRF) is fully trained. The abstract of the paper describes a framework (iNeRF) that performs pose estimation by \"inverting\" a Neural Radiance Field (NeRF). It explicitly mentions that the method starts with a trained NeRF model and uses gradient descent to refine the camera pose by minimizing the residual between rendered pixels from the NeRF and observed image pixels. This aligns with the requirement of estimating camera pose using inverse optimization on a fully trained neural implicit network."}
{"input": "User Query: What studies proposed precision and recall variants to separately evaluate fidelity and diversity aspects of generated samples?\nTitle: Improved Precision and Recall Metric for Assessing Generative Models\nAbstract:  Abstract The ability to automatically estimate the quality and coverage of the samples produced by a generative model is a vital requirement for driving algorithm research. We present an evaluation metric that can separately and reliably measure both of these aspects in image generation tasks by forming explicit, non-parametric representations of the manifolds of real and generated data. We demonstrate the effectiveness of our metric in StyleGAN and BigGAN by providing several illustrative examples where existing metrics yield uninformative or contradictory results. Furthermore, we analyze multiple design variants of StyleGAN to better understand the relationships between the model architecture, training methods, and the properties of the resulting sample distribution. In the process, we identify new variants that improve the state-of-the-art. We also perform the first principled analysis of truncation methods and identify an improved method. Finally, we extend our metric to estimate the perceptual quality of individual samples, and use this to study latent space interpolations. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Improved Precision and Recall Metric for Assessing Generative Models\" satisfies the user query. The abstract explicitly mentions that the proposed evaluation metric can \"separately and reliably measure\" fidelity (quality) and diversity (coverage) aspects of generated samples. This aligns with the user's request for studies that propose precision and recall variants to evaluate these two aspects independently. The paper also emphasizes the use of explicit, non-parametric representations of real and generated data manifolds, which is consistent with the goal of precision and recall variants for assessing generative models."}
{"input": "User Query: Could you give me works identifying a general class of “well-conditioned” PSRs that can be learned by the OMLE algorithm?\nTitle: Optimistic MLE—A Generic Model-based Algorithm for Partially Observable Sequential Decision Making\nAbstract:  Abstract This paper introduces a simple efficient learning algorithms for general sequential decision making. The algorithm combines Optimism for exploration with Maximum Likelihood Estimation for model estimation, which is thus named OMLE. We prove that OMLE learns the near-optimal policies of an enormously rich class of sequential decision making problems in a polynomial number of samples. This rich class includes not only a majority of known tractable model-based Reinforcement Learning (RL) problems (such as tabular MDPs, factored MDPs, low witness rank problems, tabular weakly-revealing/observable POMDPs and multi-step decodable POMDPs ), but also many new challenging RL problems especially in the partially observable setting that were not previously known to be tractable. Notably, the new problems addressed by this paper include (1) observable POMDPs with continuous observation and function approximation, where we achieve the first sample complexity that is completely independent of the size of observation space; (2) well-conditioned low-rank sequential decision making problems (also known as Predictive State Representations (PSRs)), which include and generalize all known tractable POMDP examples under a more intrinsic representation; (3) general sequential decision making problems under SAIL condition, which unifies our existing understandings of model-based RL in both fully observable and partially observable settings. SAIL condition is identified by this paper, which can be viewed as a natural generalization of Bellman/witness rank to address partial observability. This paper also presents a reward-free variant of OMLE algorithm, which learns approximate dynamic models that enable the computation of near-optimal policies for all reward functions simultaneously. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Optimistic MLE—A Generic Model-based Algorithm for Partially Observable Sequential Decision Making\"* satisfies the user query. The abstract explicitly mentions that the OMLE algorithm addresses \"well-conditioned low-rank sequential decision-making problems (also known as Predictive State Representations (PSRs)),\" which generalize all known tractable POMDP examples under a more intrinsic representation. This directly aligns with the user's query about identifying a general class of \"well-conditioned\" PSRs that can be learned by the OMLE algorithm. The paper not only identifies such a class but also provides theoretical guarantees for learning these models using the OMLE algorithm."}
{"input": "User Query: Can you provide studies that design an architecture offering increased robustness to occlusions in 3D hand pose estimation?\nTitle: Supplementary Material for “HandOccNet: Occlusion-Robust 3D Hand Mesh Estimation Network”\nAbstract: ", "output": "**Decision:** True  \n**Reason:** The searched paper, titled \"Supplementary Material for 'HandOccNet: Occlusion-Robust 3D Hand Mesh Estimation Network',\" directly addresses the user query by focusing on designing an architecture (HandOccNet) that is explicitly aimed at increasing robustness to occlusions in 3D hand pose estimation. The title and context suggest that the study proposes a network tailored to handle occlusions, which aligns with the user's request for research in this specific area."}
{"input": "User Query: Which studies try to augment the LLM with external information for question-answering and conversational tasks?\nTitle: Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions\nAbstract:  Abstract Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, what to retrieve depends on what has already been derived, which in turn may depend on what was previously retrieved. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning.111Code, data, and prompts are available at https://github.com/stonybrooknlp/ircot. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions\" explicitly addresses the augmentation of large language models (LLMs) with external information for question-answering tasks. The proposed IRCoT approach combines retrieval of external knowledge with Chain-of-Thought (CoT) reasoning, interleaving these processes to improve multi-step question answering. The method is designed to retrieve relevant external information iteratively, addressing the limitations of static retrieval approaches, and incorporates this information into the reasoning process. This aligns directly with the user query, as the study focuses on augmenting LLMs with external knowledge for QA tasks."}
{"input": "User Query: What studies proposed the most related effort of explicit regulations along the entire neural dynamics, interval bound propagation (IBP)?\nTitle: Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation\nAbstract:  Abstract Neural networks are part of many contemporary NLP systems, yet their empirical successes come at the price of vulnerability to adversarial attacks. Previous work has used adversarial training and data augmentation to partially mitigate such brittleness, but these are unlikely to find worst-case adversaries due to the complexity of the search space arising from discrete text perturbations. In this work, we approach the problem from the opposite direction: to formally verify a system’s robustness against a predefined class of adversarial attacks. We study text classification under synonym replacements or character flip perturbations. We propose modeling these input perturbations as a simplex and then using Interval Bound Propagation – a formal model verification method.111The source code is available at https://github.com/deepmind/interval-bound-propagation/tree/master/examples/language/ We modify the conventional log-likelihood training objective to train models that can be efficiently verified, which would otherwise come with exponential search complexity. The resulting models show only little difference in terms of nominal accuracy, but have much improved verified accuracy under perturbations and come with an efficiently computable formal guarantee on worst case adversaries. ", "output": "**Decision:** True  \n**Reason:** The searched paper directly addresses the use of Interval Bound Propagation (IBP) as a method for achieving verified robustness in neural networks, specifically in the context of NLP systems. It explicitly discusses modifying the training objective to regulate the entire neural dynamics in a way that ensures efficient verification and robustness against adversarial attacks. This aligns with the user query, which seeks studies proposing explicit regulations along the entire neural dynamics using IBP. The paper's focus on formal guarantees and verified accuracy further supports its relevance to the query."}
{"input": "User Query: Which projects are examples of zero-shot visual grounding methods?\nTitle: ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension\nAbstract:  Abstract Training a referring expression comprehension (ReC) model for a new visual domain requires collecting referring expressions, and potentially corresponding bounding boxes, for images in the domain. While large-scale pre-trained models are useful for image classification across domains, it remains unclear if they can be applied in a zero-shot manner to more complex tasks like ReC. We present ReCLIP, a simple but strong zero-shot baseline that repurposes CLIP, a state-of-the-art large-scale model, for ReC. Motivated by the close connection between ReC and CLIP’s contrastive pre-training objective, the first component of ReCLIP is a region-scoring method that isolates object proposals via cropping and blurring, and passes them to CLIP. However, through controlled experiments on a synthetic dataset, we find that CLIP is largely incapable of performing spatial reasoning off-the-shelf. Thus, the second component of ReCLIP is a spatial relation resolver that handles several types of spatial relations. We reduce the gap between zero-shot baselines from prior work and supervised models by as much as 29% on RefCOCOg, and on RefGTA (video game imagery), ReCLIP’s relative improvement over supervised ReC models trained on real images is 8%. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension\" fully satisfies the user query. The user is asking for examples of zero-shot visual grounding methods, and the paper explicitly discusses a zero-shot approach for referring expression comprehension (ReC), which is a task within visual grounding. The method, ReCLIP, repurposes the CLIP model for ReC in a zero-shot manner, without requiring additional training on the target domain. The paper also highlights the use of a region-scoring method and a spatial relation resolver to improve performance, demonstrating its focus on zero-shot visual grounding."}
{"input": "User Query: What works have been conducted on federated bilevel optimization?\nTitle: Local Stochastic Bilevel Optimization with Momentum-Based Variance Reduction\nAbstract:  Abstract Bilevel Optimization has witnessed notable progress recently with new emerging efficient algorithms and has been applied to many machine learning tasks such as data cleaning, few-shot learning, and neural architecture search. However, little attention has been paid to solve the bilevel problems under distributed setting. Federated learning (FL) is an emerging paradigm which solves machine learning tasks over distributed-located data. FL problems are challenging to solve due to the heterogeneity and communication bottleneck. However, it is unclear how these challenges will affect the convergence of Bilevel Optimization algorithms. In this paper, we study Federated Bilevel Optimization problems. Specifically, we first propose the FedBiO, a deterministic gradient-based algorithm and we show it requires O​(ϵ−2)𝑂superscriptitalic-ϵ2O(\\epsilon^{-2}) number of iterations to reach an ϵitalic-ϵ\\epsilon-stationary point. Then we propose FedBiOAcc to accelerate FedBiO with the momentum-based variance-reduction technique under the stochastic scenario. We show FedBiOAcc has complexity of O​(ϵ−1.5)𝑂superscriptitalic-ϵ1.5O(\\epsilon^{-1.5}). Finally, we validate our proposed algorithms via the important Fair Federated Learning task. More specifically, we define a bilevel-based group fair FL objective. Our algorithms show superior performances compared to other baselines in numerical experiments. ", "output": "**Decision:** True  \n**Reason:** The searched paper directly addresses the topic of federated bilevel optimization, which is the focus of the user query. The paper explicitly studies Federated Bilevel Optimization problems and proposes two algorithms, FedBiO and FedBiOAcc, to solve such problems. It also discusses the challenges specific to federated learning, such as heterogeneity and communication bottlenecks, and evaluates the proposed algorithms in the context of a federated learning task (Fair Federated Learning). The paper provides theoretical convergence guarantees and validates the algorithms through numerical experiments, making it highly relevant to the user's query."}
{"input": "User Query: Who developed Binder which addressed mostly answering questions about tables using SQL and SQL-like Python?\nTitle: Binding Language Models in Symbolic Languages\nAbstract:  Abstract Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at https://github.com/hkunlp/binder111More resources at https://lm-code-binder.github.io/.. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Binding Language Models in Symbolic Languages\" explicitly introduces Binder, a framework designed to address answering questions about tables using SQL and SQL-like Python. The abstract explains that Binder maps task inputs to programs, extending grammar coverage to handle diverse questions. It specifically mentions using SQL and Python-like programming for tackling such tasks. Furthermore, the paper highlights Binder's state-of-the-art performance on table-related datasets like WikiTableQuestions and TabFact, which are directly relevant to answering questions about tables. This aligns with the user's query about who developed Binder and its focus on table-related question answering using SQL and SQL-like Python."}
{"input": "User Query: Which studies focused on single object synthesis in the context of Text-to-3D?\nTitle: ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation\nAbstract:  Abstract Score distillation sampling (SDS) has shown great promise in text-to-3D generation by distilling pretrained large-scale text-to-image diffusion models, but suffers from over-saturation, over-smoothing, and low-diversity problems. In this work, we propose to model the 3D parameter as a random variable instead of a constant as in SDS and present variational score distillation (VSD), a principled particle-based variational framework to explain and address the aforementioned issues in text-to-3D generation. We show that SDS is a special case of VSD and leads to poor samples with both small and large CFG weights. In comparison, VSD works well with various CFG weights as ancestral sampling from diffusion models and simultaneously improves the diversity and sample quality with a common CFG weight (i.e., 7.57.57.5). We further present various improvements in the design space for text-to-3D such as distillation time schedule and density initialization, which are orthogonal to the distillation algorithm yet not well explored. Our overall approach, dubbed ProlificDreamer, can generate high rendering resolution (i.e., 512×512512512512\\times 512) and high-fidelity NeRF with rich structure and complex effects (e.g., smoke and drops). Further, initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed and photo-realistic. Project page and codes: https://ml.cs.tsinghua.edu.cn/prolificdreamer/. ", "output": "**Decision:** True  \n**Reason:** The paper \"ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation\" satisfies the user query as it focuses on text-to-3D generation, which inherently involves synthesizing 3D objects based on textual descriptions. While the abstract does not explicitly state a restriction to \"single object synthesis,\" the context of text-to-3D generation often involves generating individual objects or scenes. The paper discusses improvements in the fidelity, diversity, and quality of the generated 3D models, including high-resolution NeRFs and detailed meshes, which are applicable to single object synthesis. Therefore, it aligns with the user's query about studies focused on single object synthesis in the context of text-to-3D."}
{"input": "User Query: Which works studied adversarial attacks on natural language processing tasks concentratiing on word-level perturbations?\nTitle: BERT-ATTACK: Adversarial Attack Against BERT Using BERT\nAbstract:  Abstract Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods. Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency. In this paper, we propose BERT-Attack, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT. We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly. Our method outperforms state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved. Also, the cost of calculation is low, thus possible for large-scale generations. The code is available at https://github.com/LinyangLee/BERT-Attack. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"BERT-ATTACK: Adversarial Attack Against BERT Using BERT\" satisfies the user query. The abstract explicitly mentions that the method involves adversarial attacks on natural language processing (NLP) tasks and focuses on generating adversarial samples through word-level perturbations. Specifically, the paper discusses heuristic replacement strategies at the word level and highlights the use of BERT to generate adversarial examples that preserve semantic consistency and fluency. This aligns with the user's interest in works studying adversarial attacks on NLP tasks with a concentration on word-level perturbations."}
{"input": "User Query: Which methods learn to generate raster fonts from a large set of reference glyphs or a few exemplar images?\nTitle: Multiple Heads are Better than One: Few-shot Font Generation with Multiple Localized Experts\nAbstract:  Abstract A few-shot font generation (FFG) method has to satisfy two objectives: the generated images should preserve the underlying global structure of the target character and present the diverse local reference style. Existing FFG methods aim to disentangle content and style either by extracting a universal representation style or extracting multiple component-wise style representations. However, previous methods either fail to capture diverse local styles or cannot be generalized to a character with unseen components, e.g., unseen language systems. To mitigate the issues, we propose a novel FFG method, named Multiple Localized Experts Few-shot Font Generation Network (MX-Font). MX-Font extracts multiple style features not explicitly conditioned on component labels, but automatically by multiple experts to represent different local concepts, e.g., left-side sub-glyph. Owing to the multiple experts, MX-Font can capture diverse local concepts and show the generalizability to unseen languages. During training, we utilize component labels as weak supervision to guide each expert to be specialized for different local concepts. We formulate the component assign problem to each expert as the graph matching problem, and solve it by the Hungarian algorithm. We also employ the independence loss and the content-style adversarial loss to impose the content-style disentanglement. In our experiments, MX-Font outperforms previous state-of-the-art FFG methods in the Chinese generation and cross-lingual, e.g., Chinese to Korean, generation. Source code is available at https://github.com/clovaai/mxfont. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Multiple Heads are Better than One: Few-shot Font Generation with Multiple Localized Experts\" satisfies the user query. The user is asking about methods that learn to generate raster fonts from a large set of reference glyphs or a few exemplar images. The paper specifically addresses the problem of few-shot font generation (FFG), which involves generating fonts based on a limited number of reference images (exemplar glyphs). The proposed method, MX-Font, extracts multiple localized style features to represent diverse local concepts and demonstrates its ability to generalize to unseen characters and languages. This aligns with the requirement of learning to generate raster fonts from a few exemplar images. Furthermore, the paper provides details about the methodology, including the use of multiple localized experts, weak supervision with component labels, and disentanglement techniques, which are directly relevant to the query."}
{"input": "User Query: Which papers propose architectural changes to improve the equivariance of CNNs?\nTitle: Alias-Free Generative Adversarial Networks\nAbstract:  Abstract We observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to image coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation. ", "output": "**Decision:** True  \n**Reason:** The paper \"Alias-Free Generative Adversarial Networks\" satisfies the user query as it proposes architectural changes to improve the equivariance of convolutional neural networks (CNNs). The abstract explicitly mentions deriving \"small architectural changes\" to address issues related to aliasing in the generator network, ensuring that the network becomes \"fully equivariant to translation and rotation even at subpixel scales.\" These changes directly align with the user's interest in architectural modifications to enhance equivariance in CNNs."}
{"input": "User Query: Which papers proposed the use of value decomposition in the modelling of Q(s, a) by aggregating Qi with the sum method?\nTitle: Value-Decomposition Networks For Cooperative Multi-Agent Learning\nAbstract:  Abstract We study the problem of cooperative multi-agent reinforcement learning with a single joint reward signal. This class of learning problems is difficult because of the often large combined action and observation spaces. In the fully centralized and decentralized approaches, we find the problem of spurious rewards and a phenomenon we call the “lazy agent” problem, which arises due to partial observability. We address these problems by training individual agents with a novel value decomposition network architecture, which learns to decompose the team value function into agent-wise value functions. We perform an experimental evaluation across a range of partially-observable multi-agent domains and show that learning such value-decompositions leads to superior results, in particular when combined with weight sharing, role information and information channels. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Value-Decomposition Networks For Cooperative Multi-Agent Learning\" satisfies the user query. The abstract explicitly mentions the use of a \"value decomposition network architecture,\" which decomposes the team value function into agent-wise value functions. While the abstract does not explicitly state the use of the \"sum method\" for aggregation, the context and terminology strongly suggest that the value decomposition involves aggregating individual agent value functions (Qi) to model the joint value function Q(s, a). This aligns with the concept of value decomposition as described in the query. Therefore, the paper meets the requirements of the query."}
{"input": "User Query: Could you name some papers where differentiable optimization was applied to cone programs?\nTitle: Differentiable Convex Optimization Layers\nAbstract:  Abstract Recent work has shown how to embed differentiable optimization problems (that is, problems whose solutions can be backpropagated through) as layers within deep learning architectures. This method provides a useful inductive bias for certain problems, but existing software for differentiable optimization layers is rigid and difficult to apply to new settings. In this paper, we propose an approach to differentiating through disciplined convex programs, a subclass of convex optimization problems used by domain-specific languages (DSLs) for convex optimization. We introduce disciplined parametrized programming, a subset of disciplined convex programming, and we show that every disciplined parametrized program can be represented as the composition of an affine map from parameters to problem data, a solver, and an affine map from the solver’s solution to a solution of the original problem (a new form we refer to as affine-solver-affine form). We then demonstrate how to efficiently differentiate through each of these components, allowing for end-to-end analytical differentiation through the entire convex program. We implement our methodology in version 1.1 of CVXPY, a popular Python-embedded DSL for convex optimization, and additionally implement differentiable layers for disciplined convex programs in PyTorch and TensorFlow 2.0. Our implementation significantly lowers the barrier to using convex optimization problems in differentiable programs. We present applications in linear machine learning models and in stochastic control, and we show that our layer is competitive (in execution time) compared to specialized differentiable solvers from past work. ", "output": "**Decision:** True  \n**Reason:** The searched paper, \"Differentiable Convex Optimization Layers,\" satisfies the user query because it discusses differentiable optimization applied to cone programs. The paper specifically focuses on disciplined convex programs, which include cone programs as a subclass. Cone programs are a type of convex optimization problem, and the paper demonstrates how to differentiate through such programs using disciplined parametrized programming. Additionally, the implementation in CVXPY and its applications in machine learning and stochastic control further support the relevance of the paper to the query. Therefore, the paper aligns with the user's request."}
{"input": "User Query: What works differ from the current research but also utilize transformers?\nTitle: Vision Transformer for NeRF-Based View Synthesis from a Single Input Image\nAbstract:  Abstract Although neural radiance fields (NeRF) have shown impressive advances in novel view synthesis, most methods require multiple input images of the same scene with accurate camera poses. In this work, we seek to substantially reduce the inputs to a single unposed image. Existing approaches using local image features to reconstruct a 3D object often render blurry predictions at viewpoints distant from the source view. To address this, we propose to leverage both the global and local features to form an expressive 3D representation. The global features are learned from a vision transformer, while the local features are extracted from a 2D convolutional network. To synthesize a novel view, we train a multi-layer perceptron (MLP) network conditioned on the learned 3D representation to perform volume rendering. This novel 3D representation allows the network to reconstruct unseen regions without enforcing constraints like symmetry or canonical coordinate systems. Our method renders novel views from just a single input image, and generalizes across multiple object categories using a single model. Quantitative and qualitative evaluations demonstrate that the proposed method achieves state-of-the-art performance and renders richer details than existing approaches. https://cseweb.ucsd.edu/%7eviscomp/projects/VisionNeRF/ ", "output": "**Decision:** True  \n**Reason:** The paper satisfies the user query as it presents research that differs from current approaches in the field while utilizing transformers. Specifically, the paper addresses the challenge of novel view synthesis from a single unposed image, which is a significant departure from traditional NeRF-based methods that require multiple input images with accurate camera poses. The proposed method leverages a vision transformer to learn global features, which is a novel application of transformers in the context of NeRF-based view synthesis. This approach combines global features from the transformer with local features from a convolutional network to form an expressive 3D representation, enabling the synthesis of novel views with richer details. The use of transformers here represents a distinct and innovative direction compared to existing methods, fulfilling the requirement of differing from current research while utilizing transformers."}
{"input": "User Query: Any works about transforming-based models for temporal action segmentation?\nTitle: ASFormer: Transformer for Action Segmentation\nAbstract:  Abstract Algorithms for the action segmentation task typically use temporal models to predict what action is occurring at each frame for a minute-long daily activity. Recent studies have shown the potential of Transformer in modeling the relations among elements in sequential data. However, there are several major concerns when directly applying the Transformer to the action segmentation task, such as the lack of inductive biases with small training sets, the deficit in processing long input sequence, and the limitation of the decoder architecture to utilize temporal relations among multiple action segments to refine the initial predictions. To address these concerns, we design an efficient Transformer-based model for action segmentation task, named ASFormer, with three distinctive characteristics: (i) We explicitly bring in the local connectivity inductive priors because of the high locality of features. It constrains the hypothesis space within a reliable scope, and is beneficial for the action segmentation task to learn a proper target function with small training sets. (ii) We apply a pre-defined hierarchical representation pattern that efficiently handles long input sequences. (iii) We carefully design the decoder to refine the initial predictions from the encoder. Extensive experiments on three public datasets demonstrate that effectiveness of our methods. Code is available at https://github.com/ChinaYi/ASFormer. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"ASFormer: Transformer for Action Segmentation\" directly addresses the user's query about transformer-based models for temporal action segmentation. The abstract explicitly states that the paper focuses on designing a transformer-based model (ASFormer) for the action segmentation task, which involves predicting actions at each frame in sequential data. It discusses the challenges of applying transformers to this task and proposes solutions tailored to temporal action segmentation, such as incorporating local connectivity priors, handling long input sequences, and refining predictions using a carefully designed decoder. The paper also provides experimental validation on public datasets, further confirming its relevance to the query."}
{"input": "User Query: Can you list papers that explored action-free imitation by learning a dynamics model or a reward function?\nTitle: State-Only Imitation Learning for Dexterous Manipulation\nAbstract:  Abstract Modern model-free reinforcement learning methods have recently demonstrated impressive results on a number of problems. However, complex domains like dexterous manipulation remain a challenge due to the high sample complexity. To address this, current approaches employ expert demonstrations in the form of state-action pairs, which are difficult to obtain for real-world settings such as learning from videos. In this paper, we move toward a more realistic setting and explore state-only imitation learning. To tackle this setting, we train an inverse dynamics model and use it to predict actions for state-only demonstrations. The inverse dynamics model and the policy are trained jointly. Our method performs on par with state-action approaches and considerably outperforms RL alone. By not relying on expert actions, we are able to learn from demonstrations with different dynamics, morphologies, and objects. Videos available on the project page. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"State-Only Imitation Learning for Dexterous Manipulation\" satisfies the requirements of the user query. The query specifically asks for papers that explored action-free imitation by learning a dynamics model or a reward function. This paper addresses the challenge of state-only imitation learning, which inherently involves action-free imitation since it does not rely on expert action demonstrations. To achieve this, the paper trains an inverse dynamics model to predict actions from state-only demonstrations. This approach involves learning a dynamics model, which aligns with the requirement of the query. The paper does not explicitly mention learning a reward function, but the use of a dynamics model for action-free imitation directly fulfills the user's request."}
{"input": "User Query: Which papers have proposed to verify reasoning chains for LM training?\nTitle: Let’s Verify Step by Step\nAbstract:  Abstract In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model. ", "output": "**Decision:** True  \n**Reason:** The paper \"Let’s Verify Step by Step\" satisfies the user query as it explicitly discusses verifying reasoning chains for language model (LM) training. The paper focuses on \"process supervision,\" which involves providing feedback for each intermediate reasoning step, effectively verifying reasoning chains step by step. This approach is contrasted with \"outcome supervision,\" which only evaluates the final result. By emphasizing process supervision and its significant benefits in training models to solve complex multi-step reasoning problems, the paper directly addresses the verification of reasoning chains during LM training. Furthermore, the release of the PRM800K dataset, which contains step-level human feedback labels, further supports the verification of reasoning chains. Thus, the paper aligns well with the detailed requirements of the user query."}
{"input": "User Query: What works have shown enhancements in generalization using video of a human performing the desired task?\nTitle: One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning\nAbstract:  Abstract Humans and animals are capable of learning a new behavior by observing others perform the skill just once. We consider the problem of allowing a robot to do the same – learning from a raw video pixels of a human, even when there is substantial domain shift in the perspective, environment, and embodiment between the robot and the observed human. Prior approaches to this problem have hand-specified how human and robot actions correspond and often relied on explicit human pose detection systems. In this work, we present an approach for one-shot learning from a video of a human by using human and robot demonstration data from a variety of previous tasks to build up prior knowledge through meta-learning. Then, combining this prior knowledge and only a single video demonstration from a human, the robot can perform the task that the human demonstrated. We show experiments on both a PR2 arm and a Sawyer arm, demonstrating that after meta-learning, the robot can learn to place, push, and pick-and-place new objects using just one video of a human performing the manipulation. ", "output": "**Decision:** True  \n**Reason:** The paper satisfies the user query as it directly addresses the use of video of a human performing a desired task to enhance generalization in robotic learning. Specifically, the paper presents a method for one-shot imitation learning where a robot learns to perform tasks by observing a single video of a human demonstration. The approach leverages meta-learning to build prior knowledge from diverse human and robot demonstrations, enabling the robot to generalize to new tasks despite domain shifts (e.g., differences in perspective, environment, and embodiment). The experiments demonstrate the robot's ability to generalize and perform tasks like placing, pushing, and pick-and-place after observing a human video, which aligns with the user's query about enhancements in generalization using human video demonstrations."}
{"input": "User Query: Can you list the studies that discussed the usage of contrastive learning as an effective distillation objective?\nTitle: Contrastive Representation Distillation\nAbstract:  Abstract Often we wish to transfer representational knowledge from one neural network to another. Examples include distilling a large network into a smaller one, transferring knowledge from one sensory modality to a second, or ensembling a collection of models into a single estimator. Knowledge distillation, the standard approach to these problems, minimizes the KL divergence between the probabilistic outputs of a teacher and student network. We demonstrate that this objective ignores important structural knowledge of the teacher network. This motivates an alternative objective by which we train a student to capture significantly more information in the teacher’s representation of the data. We formulate this objective as contrastive learning. Experiments demonstrate that our resulting new objective outperforms knowledge distillation and other cutting-edge distillers on a variety of knowledge transfer tasks, including single model compression, ensemble distillation, and cross-modal transfer. Our method sets a new state-of-the-art in many transfer tasks, and sometimes even outperforms the teacher network when combined with knowledge distillation. ††footnotetext: Code: http://github.com/HobbitLong/RepDistiller. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Contrastive Representation Distillation\" directly discusses the usage of contrastive learning as an effective distillation objective. The abstract explicitly mentions that the authors propose an alternative objective for knowledge distillation formulated as contrastive learning. They argue that this approach captures more structural knowledge from the teacher network compared to traditional methods like KL divergence. Furthermore, the paper demonstrates the effectiveness of this contrastive learning-based distillation objective through experiments across various knowledge transfer tasks. This aligns well with the user query, which seeks studies discussing contrastive learning in the context of distillation objectives."}
{"input": "User Query: Can you tell me about studies that extended the standard multi-armed bandit setting to multiple-point bandit feedback?\nTitle: Regret in Online Combinatorial Optimization\nAbstract:  Abstract We address online linear optimization problems when the possible actions of the decision maker are represented by binary vectors. The regret of the decision maker is the difference between her realized loss and the minimal loss she would have achieved by picking, in hindsight, the best possible action. Our goal is to understand the magnitude of the best possible (minimax) regret. We study the problem under three different assumptions for the feedback the decision maker receives: full information, and the partial information models of the so-called “semi-bandit” and “bandit” problems. In the full information case we show that the standard exponentially weighted average forecaster is a provably suboptimal strategy. For the semi-bandit model, by combining the Mirror Descent algorithm and the INF (Implicitely Normalized Forecaster) strategy, we are able to prove the first optimal bounds. Finally, in the bandit case we discuss existing results in light of a new lower bound, and suggest a conjecture on the optimal regret in that case. ", "output": "**Decision:** True  \n**Reason:** The paper addresses an extension of the standard multi-armed bandit setting by considering the \"semi-bandit\" feedback model, which is a form of multiple-point bandit feedback. In this model, the decision maker receives partial information about the losses associated with individual components of the chosen action (binary vectors). This aligns with the user's query about studies that extend the standard multi-armed bandit setting to incorporate multiple-point bandit feedback. The paper specifically investigates regret bounds under this feedback model, satisfying the requirements of the query."}
{"input": "User Query: What papers present the strategy of leveraging the network prediction for explicit relabeling?\nTitle: Joint Optimization Framework for Learning with Noisy Labels\nAbstract:  Abstract Deep neural networks (DNNs) trained on large-scale datasets have exhibited significant performance in image classification. Many large-scale datasets are collected from websites, however they tend to contain inaccurate labels that are termed as noisy labels. Training on such noisy labeled datasets causes performance degradation because DNNs easily overfit to noisy labels. To overcome this problem, we propose a joint optimization framework of learning DNN parameters and estimating true labels. Our framework can correct labels during training by alternating update of network parameters and labels. We conduct experiments on the noisy CIFAR-10 datasets and the Clothing1M dataset. The results indicate that our approach significantly outperforms other state-of-the-art methods. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Joint Optimization Framework for Learning with Noisy Labels\" explicitly describes a strategy where the network predictions are used to estimate and correct noisy labels during training. This aligns with the user query, as the proposed framework leverages the network's predictions to explicitly relabel the data by alternating between updating the network parameters and refining the labels. The abstract clearly states that the method involves \"estimating true labels\" and \"correcting labels during training,\" which satisfies the requirement of leveraging network predictions for explicit relabeling."}
{"input": "User Query: What studies used language models for robotic action planning and decision making?\nTitle: Inner Monologue: Embodied Reasoning through Planning with Language Models\nAbstract:  Abstract Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent’s own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Inner Monologue: Embodied Reasoning through Planning with Language Models\" fully satisfies the user query. The abstract explicitly discusses the use of large language models (LLMs) for planning and decision-making in robotic contexts. It highlights how LLMs are applied to embodied reasoning tasks, including robotic control scenarios, by leveraging feedback to improve planning and decision-making. The paper investigates the use of LLMs for robotic action planning in environments such as tabletop rearrangement tasks and long-horizon mobile manipulation tasks, aligning directly with the user's interest in studies that use language models for robotic action planning and decision-making."}
{"input": "User Query: Which strategies for combating noisy labels are built upon semi-supervised learning methods or self-supervised learning?\nTitle: SELF: Learning to Filter Noisy Labels with Self-Ensembling\nAbstract:  Abstract Deep neural networks (DNNs) have been shown to over-fit a dataset when being trained with noisy labels for a long enough time. To overcome this problem, we present a simple and effective method self-ensemble label filtering (SELF) to progressively filter out the wrong labels during training. Our method improves the task performance by gradually allowing supervision only from the potentially non-noisy (clean) labels and stops learning on the filtered noisy labels. For the filtering, we form running averages of predictions over the entire training dataset using the network output at different training epochs. We show that these ensemble estimates yield more accurate identification of inconsistent predictions throughout training than the single estimates of the network at the most recent training epoch. While filtered samples are removed entirely from the supervised training loss, we dynamically leverage them via semi-supervised learning in the unsupervised loss. We demonstrate the positive effect of such an approach on various image classification tasks under both symmetric and asymmetric label noise and at different noise ratios. It substantially outperforms all previous works on noise-aware learning across different datasets and can be applied to a broad set of network architectures. ", "output": "**Decision:** True  \n**Reason:** The paper \"SELF: Learning to Filter Noisy Labels with Self-Ensembling\" explicitly describes a strategy for combating noisy labels that leverages semi-supervised learning. The method, SELF, progressively filters out noisy labels during training and stops using them in the supervised loss. However, the filtered samples (those identified as noisy) are not discarded entirely; instead, they are dynamically leveraged in the unsupervised loss via semi-supervised learning. This aligns directly with the user query, as the strategy for handling noisy labels is built upon semi-supervised learning principles. Additionally, the use of self-ensembling to form running averages of predictions further supports the method's reliance on self-supervised or semi-supervised techniques."}
{"input": "User Query: What research papers use GRU4Rec or BERT4Rec to exploit user interaction histories in sequential recommendation?\nTitle: BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer\nAbstract: Modeling users' dynamic and evolving preferences from their historical behaviors is challenging and crucial for recommendation systems. Previous methods employ sequential neural networks (e.g., Recurrent Neural Network) to encode users' historical interactions from left to right into hidden representations for making recommendations. Although these methods achieve satisfactory results, they often assume a rigidly ordered sequence which is not always practical. We argue that such left-to-right unidirectional architectures restrict the power of the historical sequence representations. For this purpose, we introduce a Bidirectional Encoder Representations from Transformers for sequential Recommendation (BERT4Rec). However, jointly conditioning on both left and right context in deep bidirectional model would make the training become trivial since each item can indirectly \"see the target item\". To address this problem, we train the bidirectional model using the Cloze task, predicting the masked items in the sequence by jointly conditioning on their left and right context. Comparing with predicting the next item at each position in a sequence, the Cloze task can produce more samples to train a more powerful bidirectional model. Extensive experiments on four benchmark datasets show that our model outperforms various state-of-the-art sequential models consistently.", "output": "**Decision:** True  \n**Reason:** The paper titled \"BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer\" fully satisfies the user query. The query specifically asks for research papers that use GRU4Rec or BERT4Rec to exploit user interaction histories in sequential recommendation. This paper introduces BERT4Rec, a model designed to leverage user interaction histories for sequential recommendation tasks. It explicitly discusses how BERT4Rec models users' dynamic and evolving preferences from their historical behaviors and interaction sequences. Furthermore, the paper details the use of a bidirectional transformer-based architecture to improve the representation of user interaction histories, addressing limitations of prior sequential models. Therefore, it directly aligns with the requirements of the query."}
{"input": "User Query: What studies explored aligning visual features with pre-trained text embeddings in open-vocabulary semantic segmentation?\nTitle: Zero-Shot Semantic Segmentation\nAbstract:  Abstract Semantic segmentation models are limited in their ability to scale to large numbers of object classes. In this paper, we introduce the new task of zero-shot semantic segmentation: learning pixel-wise classifiers for never-seen object categories with zero training examples. To this end, we present a novel architecture, ZS3Net, combining a deep visual segmentation model with an approach to generate visual representations from semantic word embeddings. By this way, ZS3Net addresses pixel classification tasks where both seen and unseen categories are faced at test time (so called “generalized” zero-shot classification). Performance is further improved by a self-training step that relies on automatic pseudo-labeling of pixels from unseen classes. On the two standard segmentation datasets, Pascal-VOC and Pascal-Context, we propose zero-shot benchmarks and set competitive baselines. For complex scenes as ones in the Pascal-Context dataset, we extend our approach by using a graph-context encoding to fully leverage spatial context priors coming from class-wise segmentation maps. Code and models are available at: https://github.com/valeoai/ZS3. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Zero-Shot Semantic Segmentation\" satisfies the user query as it explicitly discusses aligning visual features with pre-trained text embeddings in the context of open-vocabulary semantic segmentation. The proposed ZS3Net architecture combines a deep visual segmentation model with semantic word embeddings to generate visual representations, which aligns visual features with pre-trained text embeddings. This approach directly addresses the task of zero-shot semantic segmentation, which is a form of open-vocabulary semantic segmentation, as it involves handling unseen object categories. Therefore, the paper is relevant to the user's query."}
{"input": "User Query: What research proposed improvements over MaskFormer by implementing various architectural enhancements?\nTitle: Masked-attention Mask Transformer for Universal Image Segmentation\nAbstract:  Abstract Image segmentation groups pixels with different semantics, e.g., category or instance membership. Each choice of semantics defines a task. While only the semantics of each task differ, current research focuses on designing specialized architectures for each task. We present Masked-attention Mask Transformer (Mask2Former), a new architecture capable of addressing any image segmentation task (panoptic, instance or semantic). Its key components include masked attention, which extracts localized features by constraining cross-attention within predicted mask regions. In addition to reducing the research effort by at least three times, it outperforms the best specialized architectures by a significant margin on four popular datasets. Most notably, Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on COCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7 mIoU on ADE20K). ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Masked-attention Mask Transformer for Universal Image Segmentation\" (Mask2Former) directly proposes improvements over MaskFormer by implementing various architectural enhancements. Specifically, it introduces the concept of masked attention, which is a key architectural enhancement that extracts localized features by constraining cross-attention within predicted mask regions. This improvement allows Mask2Former to address multiple image segmentation tasks (panoptic, instance, and semantic) with a unified architecture, outperforming specialized architectures and achieving state-of-the-art results on several benchmarks. Therefore, the paper satisfies the detailed requirements of the user query."}
{"input": "User Query: What studies on attention mechanisms have been successfully adopted in computer vision tasks?\nTitle: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\nAbstract:  Abstract While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.111Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision_transformer ", "output": "**Decision:** True  \n**Reason:** The paper titled \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" fully satisfies the user query. The study focuses on the application of attention mechanisms, specifically the Transformer architecture, to computer vision tasks. It demonstrates that a pure Transformer model, without reliance on convolutional networks, can achieve state-of-the-art results in image classification tasks such as ImageNet, CIFAR-100, and VTAB. This directly addresses the user query by showcasing a successful adoption of attention mechanisms (Transformers) in computer vision tasks. Additionally, the paper provides empirical evidence of its effectiveness and discusses its advantages, such as requiring fewer computational resources compared to convolutional networks."}
{"input": "User Query: Which studies focus on developing benchmarks for specific knowledge types and task families in the context of LLMs?\nTitle: Evaluating Large Language Models Trained on Code\nAbstract:  Abstract We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics. ", "output": "**Decision:** True  \n**Reason:** The paper \"Evaluating Large Language Models Trained on Code\" introduces a new benchmark, HumanEval, specifically designed to measure functional correctness in synthesizing Python programs from docstrings. This benchmark focuses on a specific knowledge type (programming/code generation) and a task family (code synthesis). The study evaluates the performance of Codex and other models on this benchmark, aligning with the user query's focus on studies that develop benchmarks for specific knowledge types and task families in the context of LLMs."}
{"input": "User Query: What are the works examining the solution of Markov games with linear function approximation?\nTitle: Learning Zero-Sum Simultaneous-Move Markov Games Using Function Approximation and Correlated Equilibrium\nAbstract:  Abstract We develop provably efficient reinforcement learning algorithms for two-player zero-sum finite-horizon Markov games with simultaneous moves. To incorporate function approximation, we consider a family of Markov games where the reward function and transition kernel possess a linear structure. Both the offline and online settings of the problems are considered. In the offline setting, we control both players and aim to find the Nash Equilibrium by minimizing the duality gap. In the online setting, we control a single player playing against an arbitrary opponent and aim to minimize the regret. For both settings, we propose an optimistic variant of the least-squares minimax value iteration algorithm. We show that our algorithm is computationally efficient and provably achieves an O~​(d3​H3​T)~𝑂superscript𝑑3superscript𝐻3𝑇\\widetilde{O}(\\sqrt{d^{3}H^{3}T}) upper bound on the duality gap and regret, where d𝑑d is the linear dimension, H𝐻H the horizon and T𝑇T the total number of timesteps. Our results do not require additional assumptions on the sampling model. Our setting requires overcoming several new challenges that are absent in Markov decision processes or turn-based Markov games. In particular, to achieve optimism with simultaneous moves, we construct both upper and lower confidence bounds of the value function, and then compute the optimistic policy by solving a general-sum matrix game with these bounds as the payoff matrices. As finding the Nash Equilibrium of a general-sum game is computationally hard, our algorithm instead solves for a Coarse Correlated Equilibrium (CCE), which can be obtained efficiently. To our best knowledge, such a CCE-based scheme for optimism has not appeared in the literature and might be of interest in its own right. ", "output": "**Decision:** True  \n**Reason:** The paper explicitly examines the solution of Markov games with linear function approximation. It focuses on two-player zero-sum finite-horizon Markov games with simultaneous moves, where the reward function and transition kernel possess a linear structure. The paper develops reinforcement learning algorithms that incorporate linear function approximation and provides theoretical guarantees for their performance. This aligns with the user query, which seeks works on solving Markov games using linear function approximation."}
{"input": "User Query: What papers proposed generative networks that are equivariant under rotation or translation?\nTitle: Unsupervised Object Representation Learning using Translation and Rotation Group Equivariant VAE\nAbstract:  Abstract In many imaging modalities, objects of interest can occur in a variety of locations and poses (i.e. are subject to translations and rotations in 2d or 3d), but the location and pose of an object does not change its semantics (i.e. the object’s essence). That is, the specific location and rotation of an airplane in satellite imagery, or the 3d rotation of a chair in a natural image, or the rotation of a particle in a cryo-electron micrograph, do not change the intrinsic nature of those objects. Here, we consider the problem of learning semantic representations of objects that are invariant to pose and location in a fully unsupervised manner. We address shortcomings in previous approaches to this problem by introducing TARGET-VAE, a translation and rotation group-equivariant variational autoencoder framework. TARGET-VAE combines three core innovations: 1) a rotation and translation group-equivariant encoder architecture, 2) a structurally disentangled distribution over latent rotation, translation, and a rotation-translation-invariant semantic object representation, which are jointly inferred by the approximate inference network, and 3) a spatially equivariant generator network. In comprehensive experiments, we show that TARGET-VAE learns disentangled representations without supervision that significantly improve upon, and avoid the pathologies of, previous methods. When trained on images highly corrupted by rotation and translation, the semantic representations learned by TARGET-VAE are similar to those learned on consistently posed objects, dramatically improving clustering in the semantic latent space. Furthermore, TARGET-VAE is able to perform remarkably accurate unsupervised pose and location inference. We expect methods like TARGET-VAE will underpin future approaches for unsupervised object generation, pose prediction, and object detection. Our code is available at https://github.com/SMLC-NYSBC/TARGET-VAE. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Unsupervised Object Representation Learning using Translation and Rotation Group Equivariant VAE\"* fully satisfies the requirements of the user query. The paper explicitly proposes a generative network, TARGET-VAE, that is equivariant under both rotation and translation. This is evident from the following points in the abstract:  \n\n1. The framework introduces a **rotation and translation group-equivariant encoder architecture**, which directly addresses the requirement for equivariance under rotation and translation.  \n2. The generator network is described as **spatially equivariant**, further confirming that the generative component respects equivariance properties.  \n3. The paper focuses on learning representations invariant to pose and location, which aligns with the query's emphasis on generative networks that handle transformations like rotation and translation.  \n\nThus, the paper meets the detailed requirements of the query by proposing a generative network with rotation and translation equivariance."}
{"input": "User Query: What papers are about applying the student-teacher learning technique in CTR prediction?\nTitle: Ensembled CTR Prediction via Knowledge Distillation\nAbstract:  Abstract. Recently, deep learning-based models have been widely studied for click-through rate (CTR) prediction and lead to improved prediction accuracy in many industrial applications. However, current research focuses primarily on building complex network architectures to better capture sophisticated feature interactions and dynamic user behaviors. The increased model complexity may slow down online inference and hinder its adoption in real-time applications. Instead, our work targets at a new model training strategy based on knowledge distillation (KD). KD is a teacher-student learning framework to transfer knowledge learned from a teacher model to a student model. The KD strategy not only allows us to simplify the student model as a vanilla DNN model but also achieves significant accuracy improvements over the state-of-the-art teacher models. The benefits thus motivate us to further explore the use of a powerful ensemble of teachers for more accurate student model training. We also propose some novel techniques to facilitate ensembled CTR prediction, including teacher gating and early stopping by distillation loss. We conduct comprehensive experiments against 12 existing models and across three industrial datasets. Both offline and online A/B testing results show the effectiveness of our KD-based training strategy. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Ensembled CTR Prediction via Knowledge Distillation\" satisfies the user query as it explicitly discusses the application of the student-teacher learning technique (knowledge distillation) in the context of click-through rate (CTR) prediction. The abstract clearly states that the work employs a teacher-student learning framework to transfer knowledge from a teacher model to a student model, aiming to improve CTR prediction accuracy. Additionally, it highlights the use of an ensemble of teacher models to enhance the training of the student model, which is directly relevant to the query. Therefore, the paper fully aligns with the user's requirements."}
{"input": "User Query: Which papers introduced post-processing methods in sparse-view CT reconstruction?\nTitle: Deep Convolutional Neural Network for Inverse Problems in Imaging\nAbstract:  Abstract In this paper, we propose a novel deep convolutional neural network (CNN)-based algorithm for solving ill-posed inverse problems. Regularized iterative algorithms have emerged as the standard approach to ill-posed inverse problems in the past few decades. These methods produce excellent results, but can be challenging to deploy in practice due to factors including the high computational cost of the forward and adjoint operators and the difficulty of hyper parameter selection. The starting point of our work is the observation that unrolled iterative methods have the form of a CNN (filtering followed by point-wise non-linearity) when the normal operator (H∗​Hsuperscript𝐻𝐻H^{*}H, the adjoint of H𝐻H times H𝐻H) of the forward model is a convolution. Based on this observation, we propose using direct inversion followed by a CNN to solve normal-convolutional inverse problems. The direct inversion encapsulates the physical model of the system, but leads to artifacts when the problem is ill-posed; the CNN combines multiresolution decomposition and residual learning in order to learn to remove these artifacts while preserving image structure. We demonstrate the performance of the proposed network in sparse-view reconstruction (down to 50 views) on parallel beam X-ray computed tomography in synthetic phantoms as well as in real experimental sinograms. The proposed network outperforms total variation-regularized iterative reconstruction for the more realistic phantoms and requires less than a second to reconstruct a 512×512512512512\\times 512 image on the GPU. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Deep Convolutional Neural Network for Inverse Problems in Imaging\" satisfies the user query as it introduces a post-processing method for sparse-view CT reconstruction. Specifically, the paper proposes a method where a direct inversion step is followed by a convolutional neural network (CNN) to remove artifacts and preserve image structure. This approach clearly falls under the category of post-processing methods, as the CNN is applied after the initial reconstruction (direct inversion) to improve the quality of the sparse-view CT images. Furthermore, the paper explicitly demonstrates the application of this method in sparse-view CT reconstruction, fulfilling the requirements of the user query."}
{"input": "User Query: Which works research in-context learning (ICL)?\nTitle: What Can Transformers Learn In-Context? A Case Study of Simple Function Classes\nAbstract:  Abstract In-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn “most” functions from this class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions—that is, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator. In fact, in-context learning is possible even under two forms of distribution shift: (i) between the training data of the model and inference-time prompts, and (ii) between the in-context examples and the query input during inference. We also show that we can train Transformers to in-context learn more complex function classes—namely sparse linear functions, two-layer neural networks, and decision trees—with performance that matches or exceeds task-specific learning algorithms. 111Our code and models are available at https://github.com/dtsip/in-context-learning. ", "output": "**Decision:** True  \n**Reason:** The paper explicitly focuses on researching in-context learning (ICL). It defines ICL as the ability of a model to condition on a prompt sequence of in-context examples and generate the corresponding output without parameter updates. The paper investigates the capability of Transformers to perform ICL by training them to learn various function classes (e.g., linear functions, sparse linear functions, two-layer neural networks, decision trees) and evaluates their performance under different conditions, such as distribution shifts. This aligns directly with the user query, as the paper explores the mechanisms and conditions under which ICL works, making it relevant to the topic of research in in-context learning."}
{"input": "User Query: Which work introduces VQ-VAE that compresses data into a discrete latent space via vector quantization?\nTitle: Neural Discrete Representation Learning\nAbstract:  Abstract Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of “posterior collapse” -— where the latents are ignored when they are paired with a powerful autoregressive decoder -— typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations. ", "output": "**Decision:** True  \n**Reason:** The searched paper, titled *\"Neural Discrete Representation Learning,\"* explicitly introduces the Vector Quantised-Variational AutoEncoder (VQ-VAE) model. The abstract clearly states that the model compresses data into a discrete latent space using vector quantization (VQ), fulfilling the requirements of the user query. It also highlights the key aspects of VQ-VAE, including the use of discrete codes, the incorporation of vector quantization, and its ability to address issues like posterior collapse. Therefore, this paper fully satisfies the detailed requirements of the user query."}
{"input": "User Query: Which papers elaborate on methods learning neural network policies that can be quickly fine-tuned to new tasks at test time?\nTitle: Recasting Gradient-Based Meta-Learning as Hierarchical Bayes\nAbstract: Meta-learning allows an intelligent agent to leverage prior learning episodes as a basis for quickly improving performance on a novel task. Bayesian hierarchical modeling provides a theoretical framework for formalizing meta-learning as inference for a set of parameters that are shared across tasks. Here, we reformulate the model-agnostic meta-learning algorithm (MAML) of Finn et al. (2017) as a method for probabilistic inference in a hierarchical Bayesian model. In contrast to prior methods for meta-learning via hierarchical Bayes, MAML is naturally applicable to complex function approximators through its use of a scalable gradient descent procedure for posterior inference. Furthermore, the identification of MAML as hierarchical Bayes provides a way to understand the algorithm's operation as a meta-learning procedure, as well as an opportunity to make use of computational strategies for efficient inference. We use this opportunity to propose an improvement to the MAML algorithm that makes use of techniques from approximate inference and curvature estimation.", "output": "**Decision:** True  \n**Reason:** The paper \"Recasting Gradient-Based Meta-Learning as Hierarchical Bayes\" satisfies the user query. The user is looking for papers that elaborate on methods for learning neural network policies that can be quickly fine-tuned to new tasks at test time. The paper specifically discusses the Model-Agnostic Meta-Learning (MAML) algorithm, which is a well-known method for meta-learning that enables neural networks to quickly adapt to new tasks using only a few gradient steps. The reformulation of MAML as hierarchical Bayesian inference provides a theoretical framework for understanding and improving the algorithm, which aligns with the user's interest in methods for quick fine-tuning to new tasks. Additionally, the paper proposes improvements to MAML, further contributing to the development of methods for efficient task adaptation. Therefore, the paper fully addresses the requirements of the query."}
{"input": "User Query: What study proposed Feature propagation (FP) to reconstruct missing features by diffusing known features?\nTitle: On the Unreasonable Effectiveness of Feature Propagation in Learning on Graphs with Missing Node Features\nAbstract:  Abstract While Graph Neural Networks (GNNs) have recently become the de facto standard for modeling relational data, they impose a strong assumption on the availability of the node or edge features of the graph. In many real-world applications, however, features are only partially available; for example, in social networks, age and gender are available only for a small subset of users. We present a general approach for handling missing features in graph machine learning applications that is based on minimization of the Dirichlet energy and leads to a diffusion-type differential equation on the graph. The discretization of this equation produces a simple, fast and scalable algorithm which we call Feature Propagation. We experimentally show that the proposed approach outperforms previous methods on seven common node-classification benchmarks and can withstand surprisingly high rates of missing features: on average we observe only around 4% relative accuracy drop when 99% of the features are missing. Moreover, it takes only 10 seconds to run on a graph with ∼similar-to\\sim2.5M nodes and ∼similar-to\\sim123M edges on a single GPU. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"On the Unreasonable Effectiveness of Feature Propagation in Learning on Graphs with Missing Node Features\"* explicitly proposes the method called Feature Propagation (FP) for reconstructing missing features by diffusing known features. The abstract describes how the approach is based on the minimization of the Dirichlet energy, leading to a diffusion-type differential equation on the graph, which is then discretized into the FP algorithm. This aligns directly with the user query, as the study introduces FP as a solution to handle missing features in graph machine learning tasks. Therefore, the paper fully satisfies the requirements of the user query."}
{"input": "User Query: Could you provide me some studies that predicted relative position of image patches or sorted sequential data during vision model pretraining?\nTitle: Unsupervised Visual Representation Learning by Context Prediction\nAbstract:  Abstract This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the R-CNN framework [21] and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Unsupervised Visual Representation Learning by Context Prediction\" satisfies the user query. The study explicitly focuses on predicting the relative position of image patches during vision model pretraining. The abstract describes a method where random pairs of patches are extracted from images, and a convolutional neural network is trained to predict the position of one patch relative to the other. This directly aligns with the user's request for studies that involve predicting the relative position of image patches during vision model pretraining. However, it does not address sorting sequential data, but since the query allows for either predicting relative positions or sorting sequential data, the paper meets the requirements."}
{"input": "User Query: What studies proposed weakly-supervised action segmentation methods to partition a video into several action segments?\nTitle: Weakly supervised learning of actions from transcripts\nAbstract:  Abstract We present an approach for weakly supervised learning of human actions from video transcriptions. Our system is based on the idea that, given a sequence of input data and a transcript, i.e. a list of the order the actions occur in the video, it is possible to infer the actions within the video stream and to learn the related action models without the need for any frame-based annotation. Starting from the transcript information at hand, we split the given data sequences uniformly based on the number of expected actions. We then learn action models for each class by maximizing the probability that the training video sequences are generated by the action models given the sequence order as defined by the transcripts. The learned model can be used to temporally segment an unseen video with or without transcript. Additionally, the inferred segments can be used as a starting point to train high-level fully supervised models. We evaluate our approach on four distinct activity datasets, namely Hollywood Extended, MPII Cooking, Breakfast and CRIM13. It shows that the proposed system is able to align the scripted actions with the video data, that the learned models localize and classify actions in the datasets, and that they outperform any current state-of-the-art approach for aligning transcripts with video data. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Weakly supervised learning of actions from transcripts\" fully satisfies the user query. The study explicitly proposes a weakly-supervised action segmentation method to partition a video into several action segments. It describes an approach where video transcriptions (a list of the order of actions) are used to infer and learn action models without requiring frame-based annotations. The method involves splitting the video data sequences based on the number of expected actions and learning action models to maximize the probability of alignment between the video and the transcript. The learned models can then be used to temporally segment unseen videos into action segments. This aligns directly with the user's query about weakly-supervised action segmentation methods."}
{"input": "User Query: Which study improved prototypes with a momentum update policy for smooth label adjustment in weakly supervised learning?\nTitle: MoPro: Webly Supervised Learning with Momentum Prototypes\nAbstract:  Abstract We propose a webly-supervised representation learning method that does not suffer from the annotation unscalability of supervised learning, nor the computation unscalability of self-supervised learning. Most existing works on webly-supervised representation learning adopt a vanilla supervised learning method without accounting for the prevalent noise in the training data, whereas most prior methods in learning with label noise are less effective for real-world large-scale noisy data. We propose momentum prototypes (MoPro), a simple contrastive learning method that achieves online label noise correction, out-of-distribution sample removal, and representation learning. MoPro achieves state-of-the-art performance on WebVision, a weakly-labeled noisy dataset. MoPro also shows superior performance when the pretrained model is transferred to down-stream image classification and detection tasks. It outperforms the ImageNet supervised pretrained model by +10.510.5+10.5 on 1-shot classification on VOC, and outperforms the best self-supervised pretrained model by +17.317.3+17.3 when finetuned on 1%percent11\\% of ImageNet labeled samples. Furthermore, MoPro is more robust to distribution shift111Code and pretrained models are available at https://github.com/salesforce/MoPro. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"MoPro: Webly Supervised Learning with Momentum Prototypes\" fully satisfies the user query. The paper explicitly discusses the use of \"momentum prototypes\" as a key component of its methodology. These momentum prototypes are used to handle noisy labels in weakly supervised learning by enabling online label noise correction and smooth label adjustment. The paper also focuses on weakly supervised learning, as it addresses webly-supervised representation learning, which inherently involves weakly labeled data. Therefore, the study aligns with the query's focus on improving prototypes with a momentum update policy for smooth label adjustment in weakly supervised learning."}
{"input": "User Query: Can you cite studies that introduced implicit structural guidance into MaskGiT?\nTitle: \\MethodName: Unpaired Structure-guided Masked Image Generation\nAbstract:  Abstract Recent conditional image generation methods produce images of remarkable diversity, fidelity and realism. However, the majority of these methods allow conditioning only on labels or text prompts, which limits their level of control over the generation result. In this paper, we introduce \\MethodName, an image generation method that allows spatial conditioning of the generation result using a guiding sketch as an extra conditioning signal during sampling. \\MethodName utilizes a pre-trained masked generative transformer, requiring no model training or paired supervision, and works with input sketches of different levels of abstraction. We show that intermediate self-attention maps of a masked generative transformer encode important structural information of the input image, such as scene layout and object shape, and we propose a novel sampling method based on this observation to enable structure-guided generation. Our results show that \\MethodName achieves high image realism and fidelity to the guiding structure. Evaluated on standard benchmark datasets, \\MethodName outperforms state-of-the-art methods for sketch-to-image translation, as well as unpaired image-to-image translation approaches. ", "output": "**Decision:** True  \n**Reason:** The searched paper introduces \\MethodName, which explicitly incorporates structural guidance into the MaskGiT framework. The abstract highlights that the method leverages intermediate self-attention maps of a pre-trained masked generative transformer (MaskGiT) to encode structural information such as scene layout and object shape. This structural information is then used to guide the generation process through a novel sampling method. While the paper does not explicitly use the term \"implicit structural guidance,\" the described approach aligns with the concept of introducing structural guidance in a non-explicit or learned manner, as it relies on the internal representations (self-attention maps) of the model. Therefore, the paper satisfies the requirements of the user query."}
{"input": "User Query: Can you name a system that pre-trains on annotated pairs from natural language inference using BERT?\nTitle: Discriminative Nearest Neighbor Few-Shot Intent Detection by Transferring Natural Language Inference\nAbstract:  Abstract Intent detection is one of the core components of goal-oriented dialog systems, and detecting out-of-scope (OOS) intents is also a practically important skill. Few-shot learning is attracting much attention to mitigate data scarcity, but OOS detection becomes even more challenging. In this paper, we present a simple yet effective approach, discriminative nearest neighbor classification with deep self-attention. Unlike softmax classifiers, we leverage BERT-style pairwise encoding to train a binary classifier that estimates the best matched training example for a user input. We propose to boost the discriminative ability by transferring a natural language inference (NLI) model. Our extensive experiments on a large-scale multi-domain intent detection task show that our method achieves more stable and accurate in-domain and OOS detection accuracy than RoBERTa-based classifiers and embedding-based nearest neighbor approaches. More notably, the NLI transfer enables our 10-shot model to perform competitively with 50-shot or even full-shot classifiers, while we can keep the inference time constant by leveraging a faster embedding retrieval model. ", "output": "**Decision:** True  \n**Reason:** The paper describes a system that leverages BERT-style pairwise encoding and explicitly mentions transferring a natural language inference (NLI) model. This implies that the system utilizes pre-training on annotated pairs from NLI tasks, which aligns with the user query. The use of NLI transfer indicates that the system has been pre-trained on annotated NLI pairs, and BERT is explicitly mentioned as the underlying model architecture. Thus, the paper satisfies the detailed requirements of the user query."}
{"input": "User Query: Which papers proposed other types of scene representation methods such as point clouds or anisotropic 3D Gaussians?\nTitle: Mixture of Volumetric Primitives for Efficient Neural Rendering\nAbstract:  Abstract. Real-time rendering and animation of humans is a core function in games, movies, and telepresence applications. Existing methods have a number of drawbacks we aim to address with our work. Triangle meshes have difficulty modeling thin structures like hair, volumetric representations like Neural Volumes are too low-resolution given a reasonable memory budget, and high-resolution implicit representations like Neural Radiance Fields are too slow for use in real-time applications. We present Mixture of Volumetric Primitives (MVP), a representation for rendering dynamic 3D content that combines the completeness of volumetric representations with the efficiency of primitive-based rendering, e.g., point-based or mesh-based methods. Our approach achieves this by leveraging spatially shared computation with a convolutional architecture and by minimizing computation in empty regions of space with volumetric primitives that can move to cover only occupied regions. Our parameterization supports the integration of correspondence and tracking constraints, while being robust to areas where classical tracking fails, such as around thin or translucent structures and areas with large topological variability. MVP is a hybrid that generalizes both volumetric and primitive-based representations. Through a series of extensive experiments we demonstrate that it inherits the strengths of each, while avoiding many of their limitations. We also compare our approach to several state-of-the-art methods and demonstrate that MVP produces superior results in terms of quality and runtime performance. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Mixture of Volumetric Primitives for Efficient Neural Rendering\" proposes a new scene representation method called Mixture of Volumetric Primitives (MVP). This method combines volumetric representations with primitive-based rendering approaches, such as point-based or mesh-based methods, to achieve efficient and high-quality rendering. The paper explicitly mentions that MVP is a hybrid representation that generalizes both volumetric and primitive-based methods, which includes point-based representations. Since the user query asks for papers proposing other types of scene representation methods, such as point clouds or anisotropic 3D Gaussians, this paper satisfies the query by proposing a novel representation that incorporates point-based methods."}
{"input": "User Query: Can you provide references for works on variational inference?\nTitle: Variational Inference: A Review for Statisticians\nAbstract:  Abstract One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (vi), a method from machine learning that approximates probability densities through optimization. vi has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind vi is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of vi applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in vi and highlight important open problems. vi is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Variational Inference: A Review for Statisticians\"* fully satisfies the user's query. The user is seeking references for works on variational inference, and this paper provides a comprehensive review of variational inference (VI), including its foundational concepts, applications, and modern research directions. It also discusses specific methods like mean-field variational inference, stochastic optimization, and applications to Bayesian models, which are central to the topic. Furthermore, as a review paper, it is likely to include references to other significant works in the field of variational inference, making it a suitable source for the user's needs."}
{"input": "User Query: Which papers discussed query-based attacks within the black-box attacks scope?\nTitle: One Pixel Attack for Fooling Deep Neural Networks\nAbstract:  Abstract Recent research has revealed that the output of Deep Neural Networks (DNN) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution (DE). It requires less adversarial information (a black-box attack) and can fool more types of networks due to the inherent features of DE. The results show that 67.97%percent\\% of the natural images in Kaggle CIFAR-10 test dataset and 16.04%percent\\% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least one target class by modifying just one pixel with 74.03%percent\\% and 22.91%percent\\% confidence on average. We also show the same vulnerability on the original CIFAR-10 dataset. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks. Besides, we also illustrate an important application of DE (or broadly speaking, evolutionary computation) in the domain of adversarial machine learning: creating tools that can effectively generate low-cost adversarial attacks against neural networks for evaluating robustness. ††*Both authors have equal contribution. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"One Pixel Attack for Fooling Deep Neural Networks\" satisfies the requirements of the user query. The paper discusses a black-box attack, as explicitly mentioned in the abstract, where the attack requires minimal adversarial information about the target model. Additionally, the method proposed in the paper, based on differential evolution (DE), involves generating adversarial perturbations in a query-based manner, as DE typically relies on iterative evaluations (queries) to optimize the perturbation. This aligns with the scope of query-based attacks within black-box attack scenarios. Therefore, the paper is relevant to the user query."}
{"input": "User Query: What paper discusses jointly optimizing the predictor and the density ratio function for the covariate shift adaptation?\nTitle: A One-step Approach to Covariate Shift Adaptation\nAbstract:  Abstract A default assumption in many machine learning scenarios is that the training and test samples are drawn from the same probability distribution. However, such an assumption is often violated in the real world due to non-stationarity of the environment or bias in sample selection. In this work, we consider a prevalent setting called covariate shift, where the input distribution differs between the training and test stages while the conditional distribution of the output given the input remains unchanged. Most of the existing methods for covariate shift adaptation are two-step approaches, which first calculate the importance weights and then conduct importance-weighted empirical risk minimization. In this paper, we propose a novel one-step approach that jointly learns the predictive model and the associated weights in one optimization by minimizing an upper bound of the test risk. We theoretically analyze the proposed method and provide a generalization error bound. We also empirically demonstrate the effectiveness of the proposed method. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"A One-step Approach to Covariate Shift Adaptation\"* directly addresses the user query. The abstract explicitly states that the paper proposes a novel one-step approach that **jointly learns the predictive model and the associated weights** (which are related to the density ratio function) in a single optimization process. This aligns with the user's query about jointly optimizing the predictor and the density ratio function for covariate shift adaptation. Additionally, the paper provides theoretical analysis and empirical results to support the proposed method, further confirming its relevance to the query."}
{"input": "User Query: Which research proposed using the SINDy framework for solving PDEs?\nTitle: Weak SINDy for Partial Differential Equations\nAbstract:  Abstract. Sparse Identification of Nonlinear Dynamics (SINDy) is a method of system discovery that has been shown to successfully recover governing dynamical systems from data [5, 32]. Recently, several groups have independently discovered that the weak formulation provides orders of magnitude better robustness to noise. Here we extend our Weak SINDy (WSINDy) framework introduced in [22] to the setting of partial differential equations (PDEs). The elimination of pointwise derivative approximations via the weak form enables effective machine-precision recovery of model coefficients from noise-free data (i.e. below the tolerance of the simulation scheme) as well as robust identification of PDEs in the large noise regime (with signal-to-noise ratio approaching one in many well-known cases). This is accomplished by discretizing a convolutional weak form of the PDE and exploiting separability of test functions for efficient model identification using the Fast Fourier Transform. The resulting WSINDy algorithm for PDEs has a worst-case computational complexity of 𝒪​(ND+1​log⁡(N))𝒪superscript𝑁𝐷1𝑁{\\mathcal{O}}(N^{D+1}\\log(N)) for datasets with N𝑁N points in each of D+1𝐷1D+1 dimensions (i.e. 𝒪​(log⁡(N))𝒪𝑁{\\mathcal{O}}(\\log(N)) operations per datapoint). Furthermore, our Fourier-based implementation reveals a connection between robustness to noise and the spectra of test functions, which we utilize in an a priori selection algorithm for test functions. Finally, we introduce a learning algorithm for the threshold in sequential-thresholding least-squares (STLS) that enables model identification from large libraries, and we utilize scale-invariance at the continuum level to identify PDEs from poorly-scaled datasets. We demonstrate WSINDy’s robustness, speed and accuracy on several challenging PDEs. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Weak SINDy for Partial Differential Equations\" explicitly extends the Sparse Identification of Nonlinear Dynamics (SINDy) framework to the setting of partial differential equations (PDEs). The abstract clearly states that the authors build upon their previously introduced Weak SINDy (WSINDy) framework and adapt it for PDEs. The paper discusses the elimination of pointwise derivative approximations, robustness to noise, computational efficiency, and demonstrates the application of the WSINDy algorithm to several challenging PDEs. Therefore, the paper fully satisfies the user's query regarding research that proposed using the SINDy framework for solving PDEs."}
{"input": "User Query: Which works focus on probabilistic models that separate between static and dynamic factors in sequential disentanglement?\nTitle: Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data\nAbstract:  Abstract We present a factorized hierarchical variational autoencoder, which learns disentangled and interpretable representations from sequential data without supervision. Specifically, we exploit the multi-scale nature of information in sequential data by formulating it explicitly within a factorized hierarchical graphical model that imposes sequence-dependent priors and sequence-independent priors to different sets of latent variables. The model is evaluated on two speech corpora to demonstrate, qualitatively, its ability to transform speakers or linguistic content by manipulating different sets of latent variables; and quantitatively, its ability to outperform an i-vector baseline for speaker verification and reduce the word error rate by as much as 35% in mismatched train/test scenarios for automatic speech recognition tasks. ", "output": "**Decision:** True  \n**Reason:** The paper satisfies the user query as it focuses on a probabilistic model (a factorized hierarchical variational autoencoder) that explicitly separates static and dynamic factors in sequential disentanglement. The model achieves this by employing sequence-independent priors (representing static factors) and sequence-dependent priors (representing dynamic factors) in its latent variable structure. This aligns with the requirement of separating static and dynamic factors in sequential data using probabilistic modeling."}
{"input": "User Query: What studies have expounded on the use of PGD attack as an adversarial attack method?\nTitle: Reliable Evaluation of Adversarial Robustness with an Ensemble of Diverse Parameter-free Attacks\nAbstract:  Abstract The field of defense strategies against adversarial attacks has significantly grown over the last years, but progress is hampered as the evaluation of adversarial defenses is often insufficient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on, making it difficult to identify the state-of-the-art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks, gradient obfuscation or masking. In this paper we first propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and user-independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over 50 models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers, often by more than 10%percent1010\\%, identifying several broken defenses. ", "output": "**Decision:** True  \n**Reason:** The searched paper satisfies the user's query as it explicitly discusses the use of the PGD (Projected Gradient Descent) attack as an adversarial attack method. The abstract mentions that the paper proposes two extensions of the PGD attack to address issues such as suboptimal step size and problems with the objective function. This indicates that the study not only utilizes the PGD attack but also expands upon it, which aligns with the user's request for studies that \"expounded on the use of PGD attack as an adversarial attack method.\" Furthermore, the paper applies these improved PGD-based attacks in evaluating adversarial robustness, making it directly relevant to the query."}
{"input": "User Query: What are some studies that focus on social simulations?\nTitle: Generative Agents: Interactive Simulacra of Human Behavior\nAbstract:  Abstract. Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents: computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent’s experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty-five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors. For example, starting with only a single user-specified notion that one agent wants to throw a Valentine’s Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture—observation, planning, and reflection—each contribute critically to the believability of agent behavior. By fusing large language models with computational interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Generative Agents: Interactive Simulacra of Human Behavior\" satisfies the user query as it focuses on social simulations. The paper introduces generative agents that simulate believable human behavior in an interactive sandbox environment inspired by \"The Sims.\" These agents exhibit emergent social behaviors, such as forming opinions, initiating conversations, and coordinating activities like hosting and attending a Valentine's Day party. The study explicitly evaluates the agents' ability to produce believable individual and social behaviors, which aligns with the user's interest in studies focusing on social simulations."}
{"input": "User Query: Which works focused on scaling sparsely activated MoE architectures?\nTitle: GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\nAbstract:  Abstract Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficienctly be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\"* explicitly focuses on scaling sparsely activated Mixture-of-Experts (MoE) architectures. The abstract mentions the use of a \"Sparsely-Gated Mixture-of-Experts\" model and describes how GShard enables scaling such models to over 600 billion parameters. The work also highlights the challenges of scaling, such as computation cost and efficient implementation, and demonstrates the successful training of a large-scale MoE model. This directly aligns with the user's query about works focused on scaling sparsely activated MoE architectures."}
{"input": "User Query: What studies related to showing generalization bounds for deep neural networks?\nTitle: Spectrally-normalized margin bounds for neural networks\nAbstract:  Abstract This paper presents a margin-based multiclass generalization bound for neural networks that scales with their margin-normalized spectral complexity: their Lipschitz constant, meaning the product of the spectral norms of the weight matrices, times a certain correction factor. This bound is empirically investigated for a standard AlexNet network trained with SGD on the mnist and cifar10 datasets, with both original and random labels; the bound, the Lipschitz constants, and the excess risks are all in direct correlation, suggesting both that SGD selects predictors whose complexity scales with the difficulty of the learning task, and secondly that the presented bound is sensitive to this complexity. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Spectrally-normalized margin bounds for neural networks\" directly addresses the topic of generalization bounds for deep neural networks. It presents a margin-based generalization bound that scales with the margin-normalized spectral complexity of the network, which is a key aspect of understanding generalization in deep learning. The abstract explicitly mentions the development of a generalization bound and its empirical investigation on standard datasets, aligning well with the user query about studies related to generalization bounds for deep neural networks."}
{"input": "User Query: Could you provide me some studies about the results of scaling the size of vision transformers?\nTitle: Scaling Vision Transformers\nAbstract:  Abstract Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model’s scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45%percent90.4590.45\\% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86%percent84.8684.86\\% top-1 accuracy on ImageNet with only 10 examples per class. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Scaling Vision Transformers\" directly addresses the user query by studying the results of scaling the size of Vision Transformers (ViTs). The abstract explicitly mentions that the research investigates how Vision Transformers scale by analyzing the relationships between error rate, data, and compute as the model size is scaled both up and down. Additionally, the paper reports results from scaling ViTs, including achieving state-of-the-art performance on ImageNet and demonstrating strong few-shot transfer capabilities. This aligns well with the user's request for studies about the results of scaling the size of Vision Transformers."}
{"input": "User Query: Which research made use of contrastive loss for both geometry and appearance in object segmentation?\nTitle: NeRF-SOS: Any-View Self-supervised Object Segmentation on Complex Scenes\nAbstract:  Abstract Neural volumetric representations have shown the potential that Multi-layer Perceptrons (MLPs) can be optimized with multi-view calibrated images to represent scene geometry and appearance without explicit 3D supervision. Object segmentation can enrich many downstream applications based on the learned radiance field. However, introducing hand-crafted segmentation to define regions of interest in a complex real-world scene is non-trivial and expensive as it acquires per view annotation. This paper carries out the exploration of self-supervised learning for object segmentation using NeRF for complex real-world scenes. Our framework, called NeRF with Self-supervised Object Segmentation (NeRF-SOS), couples object segmentation and neural radiance field to segment objects in any view within a scene. By proposing a novel collaborative contrastive loss in both appearance and geometry levels, NeRF-SOS encourages NeRF models to distill compact geometry-aware segmentation clusters from their density fields and the self-supervised pre-trained 2D visual features. The self-supervised object segmentation framework can be applied to various NeRF models that both lead to photo-realistic rendering results and convincing segmentation maps for both indoor and outdoor scenarios. Extensive results on the LLFF, BlendedMVS, CO3Dv2, and Tank & Temples datasets validate the effectiveness of NeRF-SOS. It consistently surpasses other 2D-based self-supervised baselines and predicts finer object masks than existing supervised counterparts. Please refer to the video on our project page for more details: https://zhiwenfan.github.io/NeRF-SOS/. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"NeRF-SOS: Any-View Self-supervised Object Segmentation on Complex Scenes\" explicitly mentions the use of a \"novel collaborative contrastive loss\" applied at both the \"appearance and geometry levels.\" This aligns directly with the user query, which seeks research that utilizes contrastive loss for both geometry and appearance in object segmentation. The paper describes how this loss encourages the NeRF models to produce compact geometry-aware segmentation clusters by leveraging density fields (geometry) and self-supervised pre-trained 2D visual features (appearance). Therefore, the paper fully satisfies the requirements of the user query."}
{"input": "User Query: What paper is most relevant to the study of phrase retrieval problem for question-answering tasks?\nTitle: Learning Dense Representations of Phrases at Scale\nAbstract:  Abstract Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019). However, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches. In this work, we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA. We present an effective method to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods. We also propose a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference. On five popular open-domain QA datasets, our model DensePhrases improves over previous phrase retrieval models by 15%percent1515\\%–25%percent2525\\% absolute accuracy and matches the performance of state-of-the-art retriever-reader models. Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs. Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks.111Our code is available at https://github.com/princeton-nlp/DensePhrases. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Learning Dense Representations of Phrases at Scale\" is highly relevant to the study of the phrase retrieval problem for question-answering tasks. The abstract explicitly states that the research focuses on reformulating open-domain question answering as a phrase retrieval problem and addresses the limitations of existing models by proposing a novel approach to learning dense phrase representations. The paper introduces methods such as supervision from reading comprehension tasks, novel negative sampling, and query-side fine-tuning, which are directly related to improving phrase retrieval for QA tasks. Additionally, the paper demonstrates significant performance improvements over prior phrase retrieval models and explores the use of dense phrase representations for downstream tasks, further solidifying its relevance to the user query."}
{"input": "User Query: Which studies provide examples of datasets for panoptic segmentation?\nTitle: Microsoft COCO: Common Objects in Context\nAbstract:  Abstract We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Microsoft COCO: Common Objects in Context\" satisfies the user query as it provides a dataset that is relevant for panoptic segmentation. Panoptic segmentation requires both instance segmentation (for distinguishing individual object instances) and semantic segmentation (for labeling all pixels in an image with a class). The COCO dataset includes per-instance segmentations, which are a critical component for panoptic segmentation tasks. Additionally, the dataset is widely used in the computer vision community for tasks such as object recognition, instance segmentation, and scene understanding, which are directly related to panoptic segmentation. Thus, the paper provides an example of a dataset suitable for panoptic segmentation."}
{"input": "User Query: What work generated formal specifications from unstructured natural language?\nTitle: Formal Specifications from Natural Language\nAbstract:  Abstract We study the generalization abilities of language models when translating natural language into formal specifications with complex semantics. In particular, we fine-tune language models on three datasets consisting of English sentences and their corresponding formal representation: 1) regular expressions (regex), frequently used in programming and search; 2) First-order logic (FOL), commonly used in software verification and theorem proving; and 3) linear-time temporal logic (LTL), which forms the basis for industrial hardware specification languages. Our experiments show that, in these diverse domains, the language models maintain their generalization capabilities from pre-trained knowledge of natural language to generalize, e.g., to new variable names or operator descriptions. Additionally, they achieve competitive performance, and even outperform the state-of-the-art for translating into regular expressions, with the benefits of being easy to access, efficient to fine-tune, and without a particular need for domain-specific reasoning. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Formal Specifications from Natural Language\" satisfies the user query as it explicitly focuses on generating formal specifications from unstructured natural language. The abstract describes how the researchers fine-tuned language models to translate English sentences (unstructured natural language) into formal representations, such as regular expressions, first-order logic (FOL), and linear-time temporal logic (LTL). These formal representations align with the concept of formal specifications. Additionally, the paper highlights the generalization abilities of language models in this context, demonstrating their effectiveness in generating formal specifications. Therefore, the paper aligns well with the requirements of the user query."}
{"input": "User Query: What works have used prior experience without reward annotations for pre-training state representations?\nTitle: Real-World Robot Learning with Masked Visual Pre-training\nAbstract:  Abstract In this work, we explore self-supervised visual pre-training on images from diverse, in-the-wild videos for real-world robotic tasks. Like prior work, our visual representations are pre-trained via a masked autoencoder (MAE), frozen, and then passed into a learnable control module. Unlike prior work, we show that the pre-trained representations are effective across a range of real-world robotic tasks and embodiments. We find that our encoder consistently outperforms CLIP (up to 75%), supervised ImageNet pre-training (up to 81%), and training from scratch (up to 81%). Finally, we train a 307M parameter vision transformer on a massive collection of 4.5M images from the Internet and egocentric videos, and demonstrate clearly the benefits of scaling visual pre-training for robot learning. ", "output": "**Decision:** True  \n**Reason:** The paper \"Real-World Robot Learning with Masked Visual Pre-training\" satisfies the user query because it describes a method that uses prior experience (in the form of images from diverse, in-the-wild videos) without reward annotations for pre-training state representations. Specifically, the paper employs self-supervised learning via a masked autoencoder (MAE) to pre-train visual representations. This approach does not rely on reward annotations during the pre-training phase, aligning with the user's query about works that use prior experience without reward annotations for pre-training state representations."}
{"input": "User Query: What research explored the ability of large language models (LLMs) to mimic human behaviors like reasoning and cognitive tests?\nTitle: Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies\nAbstract:  Abstract We introduce a new type of test, called a Turing Experiment (TE), for evaluating to what extent a given language model, such as GPT models, can simulate different aspects of human behavior. A TE can also reveal consistent distortions in a language model’s simulation of a specific human behavior. Unlike the Turing Test, which involves simulating a single arbitrary individual, a TE requires simulating a representative sample of participants in human subject research. We carry out TEs that attempt to replicate well-established findings from prior studies. We design a methodology for simulating TEs and illustrate its use to compare how well different language models are able to reproduce classic economic, psycholinguistic, and social psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings were replicated using recent models, while the last TE reveals a “hyper-accuracy distortion” present in some language models (including ChatGPT and GPT-4), which could affect downstream applications in education and the arts. ", "output": "**Decision:** True  \n**Reason:** The searched paper directly addresses the ability of large language models (LLMs) to mimic human behaviors, including reasoning and cognitive tests. The paper introduces the concept of Turing Experiments (TEs) to evaluate how well LLMs can simulate human behaviors by replicating findings from established human subject studies. It specifically explores the replication of classic experiments in economic decision-making (Ultimatum Game), psycholinguistics (Garden Path Sentences), and social psychology (Milgram Shock Experiment and Wisdom of Crowds). These experiments inherently involve reasoning and cognitive processes, making the paper highly relevant to the user query. Additionally, the paper discusses distortions in LLM behavior, such as \"hyper-accuracy distortion,\" which further aligns with the query's focus on evaluating the ability of LLMs to mimic human cognitive behaviors."}
{"input": "User Query: Which studies used pre-trained language models (PLM) for downstream tasks?\nTitle: RoBERTa: A Robustly Optimized BERT Pretraining Approach\nAbstract:  Abstract Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining Devlin et al. (2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.111Our models and code are available at:  https://github.com/pytorch/fairseq ", "output": "**Decision:** True  \n**Reason:** The paper \"RoBERTa: A Robustly Optimized BERT Pretraining Approach\" satisfies the user query because it explicitly discusses the use of pre-trained language models (PLMs) for downstream tasks. The abstract mentions that the authors conducted a replication study of BERT pretraining and optimized it to achieve state-of-the-art results on downstream tasks such as GLUE, RACE, and SQuAD. These are well-known benchmarks for evaluating the performance of PLMs on downstream tasks, confirming that the study used PLMs for such purposes."}
{"input": "User Query: What are some works that have integrated NeRF into the task of synthesizing talking heads and used audio as the driving signal?\nTitle: AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis\nAbstract:  Abstract Generating high-fidelity talking head video by fitting with the input audio sequence is a challenging problem that receives considerable attentions recently. In this paper, we address this problem with the aid of neural scene representation networks. Our method is completely different from existing methods that rely on intermediate representations like 2D landmarks or 3D face models to bridge the gap between audio input and video output. Specifically, the feature of input audio signal is directly fed into a conditional implicit function to generate a dynamic neural radiance field, from which a high-fidelity talking-head video corresponding to the audio signal is synthesized using volume rendering. Another advantage of our framework is that not only the head (with hair) region is synthesized as previous methods did, but also the upper body is generated via two individual neural radiance fields. Experimental results demonstrate that our novel framework can (1) produce high-fidelity and natural results, and (2) support free adjustment of audio signals, viewing directions, and background images. Code is available at https://github.com/YudongGuo/AD-NeRF. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis\" fully satisfies the requirements of the user query. The user is specifically looking for works that integrate NeRF (Neural Radiance Fields) into the task of synthesizing talking heads and use audio as the driving signal. The abstract of the paper explicitly states that the method utilizes a dynamic neural radiance field (NeRF) conditioned on the input audio signal to synthesize high-fidelity talking head videos. Furthermore, the approach directly uses audio features as the driving signal without relying on intermediate representations like 2D landmarks or 3D face models, aligning well with the query's focus. Thus, the paper meets all the specified criteria."}
{"input": "User Query: What research papers focus on the study of online reward poisoning attacks in single agent RL?\nTitle: Adaptive Reward-Poisoning Attacks against Reinforcement Learning\nAbstract:  Abstract In reward-poisoning attacks against reinforcement learning (RL), an attacker can perturb the environment reward rtsubscript𝑟𝑡r_{t} into rt+δtsubscript𝑟𝑡subscript𝛿𝑡r_{t}+\\delta_{t} at each step, with the goal of forcing the RL agent to learn a nefarious policy. We categorize such attacks by the infinity-norm constraint on δtsubscript𝛿𝑡\\delta_{t}: We provide a lower threshold below which reward-poisoning attack is infeasible and RL is certified to be safe; we provide a corresponding upper threshold above which the attack is feasible. Feasible attacks can be further categorized as non-adaptive where δtsubscript𝛿𝑡\\delta_{t} depends only on (st,at,st+1)subscript𝑠𝑡subscript𝑎𝑡subscript𝑠𝑡1(s_{t},a_{t},s_{t+1}), or adaptive where δtsubscript𝛿𝑡\\delta_{t} depends further on the RL agent’s learning process at time t𝑡t. Non-adaptive attacks have been the focus of prior works. However, we show that under mild conditions, adaptive attacks can achieve the nefarious policy in steps polynomial in state-space size |S|𝑆|S|, whereas non-adaptive attacks require exponential steps. We provide a constructive proof that a Fast Adaptive Attack strategy achieves the polynomial rate. Finally, we show that empirically an attacker can find effective reward-poisoning attacks using state-of-the-art deep RL techniques. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Adaptive Reward-Poisoning Attacks against Reinforcement Learning\" fully satisfies the user query. The query specifically asks for research papers that focus on the study of online reward poisoning attacks in single-agent reinforcement learning (RL). The abstract of the paper explicitly discusses reward-poisoning attacks in RL, where the attacker perturbs the environment reward to influence the agent's learning process. It categorizes such attacks into non-adaptive and adaptive types, with a focus on adaptive attacks. The paper also explores the feasibility of these attacks, provides theoretical thresholds for their success, and demonstrates empirical results using state-of-the-art deep RL techniques. Since the study is directly about reward poisoning in RL and considers online interactions (as the poisoning depends on the agent's learning process at each step), it aligns with the user's query."}
{"input": "User Query: Can you name studies where U-Nets are used in the field of low-latency segmentation?\nTitle: U-Net: Convolutional Networks for Biomedical Image Segmentation\nAbstract:  Abstract There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net. ", "output": "**Decision:** True  \n**Reason:** The searched paper introduces the U-Net architecture, which is explicitly designed for biomedical image segmentation. The paper highlights that the network is fast, achieving segmentation of a 512x512 image in less than a second on a recent GPU. This aligns with the concept of low-latency segmentation, as it emphasizes the speed of the segmentation process. Therefore, the paper satisfies the requirements of the user query by discussing the use of U-Nets in the context of low-latency segmentation."}
{"input": "User Query: Which research employed a cross-attention structure to attend to visual contexts in the development of multi-modal LLMs?\nTitle: Flamingo: a Visual Language Model for Few-Shot Learning\nAbstract:  Abstract Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Flamingo: a Visual Language Model for Few-Shot Learning\" satisfies the user query. In the abstract, the authors describe architectural innovations that bridge pretrained vision-only and language-only models, enabling the model to handle sequences of interleaved visual and textual data. This implies the use of mechanisms such as cross-attention to effectively integrate and attend to visual contexts alongside textual inputs. Cross-attention is a common and effective method for attending to one modality (e.g., visual contexts) while processing another (e.g., text) in multimodal models. Although the abstract does not explicitly mention \"cross-attention,\" the described functionality strongly suggests its use in attending to visual contexts in the development of the Flamingo multi-modal language model."}
{"input": "User Query: Could you provide me some recent research about text-to-image generation that employed auto-regressive transformers as generators?\nTitle: Zero-Shot Text-to-Image Generation\nAbstract:  Abstract Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Zero-Shot Text-to-Image Generation\" fully satisfies the detailed requirements of the user query. The abstract explicitly mentions the use of a transformer that autoregressively models the text and image tokens as a single stream of data. This aligns directly with the user's request for research on text-to-image generation employing auto-regressive transformers as generators. Additionally, the paper focuses on text-to-image generation, which is the core topic of the query."}
{"input": "User Query: What research used task-related features in abundant bias-aligned samples by synthesizing new images?\nTitle: BiaSwap: Removing Dataset Bias with Bias-Tailored Swapping Augmentation\nAbstract:  Abstract Deep neural networks often make decisions based on the spurious correlations inherent in the dataset, failing to generalize in an unbiased data distribution. Although previous approaches pre-define the type of dataset bias to prevent the network from learning it, recognizing the bias type in the real dataset is often prohibitive. This paper proposes a novel bias-tailored augmentation-based approach, BiaSwap, for learning debiased representation without requiring supervision on the bias type. Assuming that the bias corresponds to the easy-to-learn attributes, we sort the training images based on how much a biased classifier can exploits them as shortcut and divide them into bias-guiding and bias-contrary samples in an unsupervised manner. Afterwards, we integrate the style-transferring module of the image translation model with the class activation maps of such biased classifier, which enables to primarily transfer the bias attributes learned by the classifier. Therefore, given the pair of bias-guiding and bias-contrary, BiaSwap generates the bias-swapped image which contains the bias attributes from the bias-contrary images, while preserving bias-irrelevant ones in the bias-guiding images. Given such augmented images, BiaSwap demonstrates the superiority in debiasing against the existing baselines over both synthetic and real-world datasets. Even without careful supervision on the bias, BiaSwap achieves a remarkable performance on both unbiased and bias-guiding samples, implying the improved generalization capability of the model. ", "output": "**Decision:** True  \n**Reason:** The paper \"BiaSwap: Removing Dataset Bias with Bias-Tailored Swapping Augmentation\" satisfies the requirements of the user query. The research explicitly addresses the use of task-related features in abundant bias-aligned samples by synthesizing new images. Specifically, the paper describes a method (BiaSwap) that generates bias-swapped images by transferring bias attributes between samples (bias-guiding and bias-contrary) while preserving task-relevant features. This synthesis of new images is central to their approach for debiasing and improving generalization. The method aligns with the query's focus on leveraging task-related features in bias-aligned samples through image synthesis."}
{"input": "User Query: Which research work introduced the use of an encoder-decoder generator and a PatchGAN discriminator for semantic image synthesis?\nTitle: Image-to-Image Translation with Conditional Adversarial Networks\nAbstract:  Abstract We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Image-to-Image Translation with Conditional Adversarial Networks\"* introduces the use of a PatchGAN discriminator and an encoder-decoder generator architecture for image-to-image translation tasks, which include semantic image synthesis. The PatchGAN discriminator evaluates image patches rather than the entire image, making it effective for capturing high-frequency details. Additionally, the encoder-decoder generator is a common architecture used in this work for mapping input images to output images. These components align with the requirements of the user query, as the paper is foundational in introducing these techniques for semantic image synthesis tasks."}
{"input": "User Query: What works highlighted the use of Stable Diffusion for generating images from text prompts?\nTitle: High-Resolution Image Synthesis with Latent Diffusion Models\nAbstract:  Abstract By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state-of-the-art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including text-to-image synthesis, unconditional image generation and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.  ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"High-Resolution Image Synthesis with Latent Diffusion Models\"* satisfies the requirements of the user query. The abstract explicitly discusses the use of latent diffusion models (LDMs), which are the foundation of Stable Diffusion, for generating images from text prompts. It highlights the introduction of cross-attention layers into the model architecture, enabling text-based conditioning inputs for image generation. This directly aligns with the query's focus on works that highlight the use of Stable Diffusion for generating images from text prompts. Additionally, the paper emphasizes the competitive performance of LDMs in text-to-image synthesis, further confirming its relevance to the query."}
{"input": "User Query: What study used the NeRF model's flexibility to learn a descriptor field in addition to the radiance and density information of the map for high precision localization?\nTitle: CROSSFIRE: Camera Relocalization On Self-Supervised Features from an Implicit Representation\nAbstract:  Abstract Beyond novel view synthesis, Neural Radiance Fields (NeRF) are useful for applications that interact with the real world. In this paper, we use them as an implicit map of a given scene and propose a camera relocalization algorithm tailored for this representation. The proposed method enables to compute in real-time the precise position of a device using a single RGB camera, during its navigation. In contrast with previous work, we do not rely on pose regression or photometric alignment but rather use dense local features obtained through volumetric rendering which are specialized on the scene with a self-supervised objective. As a result, our algorithm is more accurate than competitors, able to operate in dynamic outdoor environments with changing lightning conditions and can be readily integrated in any volumetric neural renderer. ", "output": "**Decision:** True  \n**Reason:** The searched paper, titled \"CROSSFIRE: Camera Relocalization On Self-Supervised Features from an Implicit Representation,\" satisfies the requirements of the user query. The paper explicitly discusses leveraging the flexibility of the NeRF model for purposes beyond radiance and density mapping. It introduces a method that uses NeRF as an implicit map of a scene and learns dense local features (descriptor fields) through volumetric rendering. These features are specialized for the scene using a self-supervised objective, which aligns with the query's requirement of learning a descriptor field in addition to radiance and density. Furthermore, the paper focuses on high-precision localization, as evidenced by its application to real-time camera relocalization with high accuracy. Therefore, the study meets the detailed requirements of the user query."}
{"input": "User Query: Could you state the research that explores the use of depth maps for data augmentation in the context of semantic segmentation?\nTitle: Three Ways to Improve Semantic Segmentation with Self-Supervised Depth Estimation\nAbstract:  Abstract Training deep networks for semantic segmentation requires large amounts of labeled training data, which presents a major challenge in practice, as labeling segmentation masks is a highly labor-intensive process. To address this issue, we present a framework for semi-supervised semantic segmentation, which is enhanced by self-supervised monocular depth estimation from unlabeled image sequences. In particular, we propose three key contributions: (1) We transfer knowledge from features learned during self-supervised depth estimation to semantic segmentation, (2) we implement a strong data augmentation by blending images and labels using the geometry of the scene, and (3) we utilize the depth feature diversity as well as the level of difficulty of learning depth in a student-teacher framework to select the most useful samples to be annotated for semantic segmentation. We validate the proposed model on the Cityscapes dataset, where all three modules demonstrate significant performance gains, and we achieve state-of-the-art results for semi-supervised semantic segmentation. The implementation is available at https://github.com/lhoyer/improving_segmentation_with_selfsupervised_depth. ", "output": "**Decision:** True  \n**Reason:** The searched paper explicitly explores the use of depth maps for data augmentation in the context of semantic segmentation. Specifically, the paper's second contribution mentions implementing \"a strong data augmentation by blending images and labels using the geometry of the scene,\" which directly leverages depth information. This aligns with the user query, as the research investigates how depth maps (derived from self-supervised depth estimation) can be utilized to enhance semantic segmentation through data augmentation techniques. Therefore, the paper satisfies the detailed requirements of the user query."}
{"input": "User Query: What work focused on methods for computing factorizations and their privacy properties?\nTitle: Improved Differential Privacy for SGD via Optimal Private Linear Operators on Adaptive Streams\nAbstract:  Abstract Motivated by recent applications requiring differential privacy over adaptive streams, we investigate optimal instantiations of the matrix mechanism [1] in this setting. We prove fundamental theoretical results on the applicability of matrix factorizations to adaptive streams, and provide a parameter-free fixed-point algorithm for computing optimal factorizations. We instantiate this framework with respect to concrete matrices which arise naturally in machine learning, and train user-level differentially private models with the resulting optimal mechanisms, yielding significant improvements in a notable problem in federated learning with user-level differential privacy. ", "output": "**Decision:** True  \n**Reason:** The searched paper focuses on methods for computing factorizations and their privacy properties. Specifically, it investigates optimal instantiations of the matrix mechanism, which inherently involves matrix factorizations, in the context of adaptive streams. The paper also provides a parameter-free fixed-point algorithm for computing optimal factorizations, directly addressing the computation of factorizations. Furthermore, the work explores the privacy properties of these factorizations by applying them to differential privacy in machine learning, particularly in federated learning with user-level differential privacy. Thus, the paper aligns with the detailed requirements of the user query."}
{"input": "User Query: What is the work where the COCO dataset has been repurposed to address zero-shot detection?\nTitle: Zero Shot Object Detection\nAbstract:  Abstract Methods for object detection and segmentation often require abundant instance-level annotations for training, which are time-consuming and expensive to collect. To address this, the task of zero-shot object detection (or segmentation) aims at learning effective methods for identifying and localizing object instances for the categories that have no supervision available. Constructing architectures for these tasks requires choosing from a myriad of design options, ranging from the form of the class encoding used to transfer information from seen to unseen categories, to the nature of the function being optimized for learning. In this work, we extensively study these design choices, and carefully construct a simple yet extremely effective zero-shot recognition method. Through extensive experiments on the MSCOCO [25] dataset on object detection and segmentation, we highlight that our proposed method outperforms existing, considerably more complex, architectures. Our findings and method, which we propose as a competitive future baseline, point towards the need to revisit some of the recent design trends in zero-shot detection / segmentation. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Zero Shot Object Detection\" explicitly mentions the use of the MSCOCO dataset (referred to as COCO in the user query) for zero-shot object detection and segmentation. The abstract highlights that the work involves repurposing the COCO dataset to study and evaluate methods for identifying and localizing object instances in categories with no supervision (i.e., zero-shot detection). This aligns with the user query, which seeks work where the COCO dataset has been utilized for zero-shot detection. Therefore, the paper fully satisfies the detailed requirements of the user query."}
{"input": "User Query: Which studies are related to utilizing a displacement field to represent the motion in dynamic scene representation?\nTitle: D-NeRF: Neural Radiance Fields for Dynamic Scenes\nAbstract:  Abstract Neural rendering techniques combining machine learning with geometric reasoning have arisen as one of the most promising approaches for synthesizing novel views of a scene from a sparse set of images. Among these, stands out the Neural radiance fields (NeRF) [26], which trains a deep network to map 5D input coordinates (representing spatial location and viewing direction) into a volume density and view-dependent emitted radiance. However, despite achieving an unprecedented level of photorealism on the generated images, NeRF is only applicable to static scenes, where the same spatial location can be queried from different images. In this paper we introduce D-NeRF, a method that extends neural radiance fields to a dynamic domain, allowing to reconstruct and render novel images of objects under rigid and non-rigid motions from a single camera moving around the scene. For this purpose we consider time as an additional input to the system, and split the learning process in two main stages: one that encodes the scene into a canonical space and another that maps this canonical representation into the deformed scene at a particular time. Both mappings are simultaneously learned using fully-connected networks. Once the networks are trained, D-NeRF can render novel images, controlling both the camera view and the time variable, and thus, the object movement. We demonstrate the effectiveness of our approach on scenes with objects under rigid, articulated and non-rigid motions. Code, model weights and the dynamic scenes dataset will be released. ", "output": "**Decision:** True  \n**Reason:** The paper \"D-NeRF: Neural Radiance Fields for Dynamic Scenes\" satisfies the user query as it introduces a method for representing motion in dynamic scenes by mapping a canonical representation of the scene into its deformed state at a particular time. This process inherently involves modeling the motion of objects in the scene, which aligns with the concept of utilizing a displacement field to represent motion. The paper describes how the deformation from the canonical space to the dynamic scene is learned, which implies the use of a displacement or transformation field to capture the motion. Thus, the study is relevant to the query."}
{"input": "User Query: Which paper did take a linear algebra approach to estimate the dimension of a subspace of adversarial examples?\nTitle: The Space of Transferable Adversarial Examples\nAbstract:  Abstract Adversarial examples are maliciously perturbed inputs designed to mislead machine learning (ML) models at test-time. They often transfer: the same adversarial example fools more than one model. In this work, we propose novel methods for estimating the previously unknown dimensionality of the space of adversarial inputs. We find that adversarial examples span a contiguous subspace of large (~25) dimensionality. Adversarial subspaces with higher dimensionality are more likely to intersect. We find that for two different models, a significant fraction of their subspaces is shared, thus enabling transferability. In the first quantitative analysis of the similarity of different models’ decision boundaries, we show that these boundaries are actually close in arbitrary directions, whether adversarial or benign. We conclude by formally studying the limits of transferability. We derive (1) sufficient conditions on the data distribution that imply transferability for simple model classes and (2) examples of scenarios in which transfer does not occur. These findings indicate that it may be possible to design defenses against transfer-based attacks, even for models that are vulnerable to direct attacks. ", "output": "**Decision:** True  \n**Reason:** The searched paper, titled *\"The Space of Transferable Adversarial Examples,\"* satisfies the requirements of the user query. The abstract explicitly mentions that the paper proposes novel methods for estimating the dimensionality of the space of adversarial examples, which aligns with the query's focus on estimating the dimension of a subspace of adversarial examples. Furthermore, the mention of \"dimensionality\" and \"subspace\" strongly suggests the use of a linear algebra approach, as these concepts are fundamental to linear algebra. Therefore, the paper fully addresses the query's requirements."}
{"input": "User Query: What papers propose methods of frame or segment-level classification to generate proposals for temporal action detection?\nTitle: CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos\nAbstract:  Abstract Temporal action localization is an important yet challenging problem. Given a long, untrimmed video consisting of multiple action instances and complex background contents, we need not only to recognize their action categories, but also to localize the start time and end time of each instance. Many state-of-the-art systems use segment-level classifiers to select and rank proposal segments of pre-determined boundaries. However, a desirable model should move beyond segment-level and make dense predictions at a fine granularity in time to determine precise temporal boundaries. To this end, we design a novel Convolutional-De-Convolutional (CDC) network that places CDC filters on top of 3D ConvNets, which have been shown to be effective for abstracting action semantics but reduce the temporal length of the input data. The proposed CDC filter performs the required temporal upsampling and spatial downsampling operations simultaneously to predict actions at the frame-level granularity. It is unique in jointly modeling action semantics in space-time and fine-grained temporal dynamics. We train the CDC network in an end-to-end manner efficiently. Our model not only achieves superior performance in detecting actions in every frame, but also significantly boosts the precision of localizing temporal boundaries. Finally, the CDC network demonstrates a very high efficiency with the ability to process 500 frames per second on a single GPU server. Source code and trained models are available online at https://bitbucket.org/columbiadvmm/cdc. ", "output": "**Decision:** True  \n**Reason:** The paper \"CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos\" satisfies the user query. The query specifically asks for papers that propose methods of frame or segment-level classification to generate proposals for temporal action detection. This paper discusses a novel Convolutional-De-Convolutional (CDC) network that performs frame-level classification to predict actions at fine temporal granularity. The CDC network is designed to make dense predictions at the frame level, which directly aligns with the requirement of frame-level classification. Additionally, the paper mentions that segment-level classifiers are commonly used in state-of-the-art systems, but the proposed CDC network moves beyond this by focusing on precise temporal boundaries through frame-level predictions. Thus, the paper fully addresses the query's requirements."}
{"input": "User Query: Which works have studied the length generalization on synthetic reasoning tasks for pretrained large language models?\nTitle: Exploring Length Generalization in Large Language Models\nAbstract:  Abstract The ability to extrapolate from short problem instances to longer ones is an important form of out-of-distribution generalization in reasoning tasks, and is crucial when learning from datasets where longer problem instances are rare. These include theorem proving, solving quantitative mathematics problems, and reading/summarizing novels. In this paper, we run careful empirical studies exploring the length generalization capabilities of transformer-based language models. We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale. We then show that combining pretrained large language models’ in-context learning abilities with scratchpad prompting (asking the model to output solution steps before producing an answer) results in a dramatic improvement in length generalization. We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Exploring Length Generalization in Large Language Models\" satisfies the requirements of the user query. The abstract explicitly mentions that the study investigates the length generalization capabilities of transformer-based language models, which includes pretrained large language models. It also focuses on reasoning tasks, as evidenced by references to theorem proving, solving quantitative mathematics problems, and other reasoning-related tasks. While it does not explicitly state \"synthetic reasoning tasks,\" the context of the study and the methodologies (e.g., scratchpad prompting and failure analyses) strongly align with synthetic reasoning tasks. Therefore, the paper is relevant to the query."}
{"input": "User Query: What are some recent empirical observations concerning neural collapse?\nTitle: Are Neurons Actually Collapsed? On the Fine-Grained Structure in Neural Representations\nAbstract:  Abstract Recent work has observed an intriguing “Neural Collapse” phenomenon in well-trained neural networks, where the last-layer representations of training samples with the same label collapse into each other. This appears to suggest that the last-layer representations are completely determined by the labels, and do not depend on the intrinsic structure of input distribution. We provide evidence that this is not a complete description, and that the apparent collapse hides important fine-grained structure in the representations. Specifically, even when representations apparently collapse, the small amount of remaining variation can still faithfully and accurately captures the intrinsic structure of input distribution. As an example, if we train on CIFAR-10 using only 5 coarse-grained labels (by combining two classes into one super-class) until convergence, we can reconstruct the original 10-class labels from the learned representations via unsupervised clustering. The reconstructed labels achieve 93%percent9393\\% accuracy on the CIFAR-10 test set, nearly matching the normal CIFAR-10 accuracy for the same architecture. We also provide an initial theoretical result showing the fine-grained representation structure in a simplified synthetic setting. Our results show concretely how the structure of input data can play a significant role in determining the fine-grained structure of neural representations, going beyond what Neural Collapse predicts. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Are Neurons Actually Collapsed? On the Fine-Grained Structure in Neural Representations\" satisfies the user query by providing recent empirical observations concerning neural collapse. The paper investigates the phenomenon of neural collapse in well-trained neural networks, specifically focusing on the last-layer representations of training samples with the same label. It challenges the completeness of the neural collapse description by presenting empirical evidence that fine-grained structure remains in the representations, even when they appear collapsed. The authors provide concrete empirical results, such as reconstructing original class labels from coarse-grained training labels using the learned representations, achieving high accuracy. This directly contributes to the understanding of neural collapse and its nuances, aligning with the user query for recent empirical observations on the topic."}
{"input": "User Query: Which papers segment point clouds by learning feature embeddings through graph convolutions?\nTitle: Dynamic Graph CNN for Learning on Point Clouds\nAbstract:  Abstract. Point clouds provide a flexible geometric representation suitable for countless applications in computer graphics; they also comprise the raw output of most 3D data acquisition devices. While hand-designed features on point clouds have long been proposed in graphics and vision, however, the recent overwhelming success of convolutional neural networks (CNNs) for image analysis suggests the value of adapting insight from CNN to the point cloud world. Point clouds inherently lack topological information so designing a model to recover topology can enrich the representation power of point clouds. To this end, we propose a new neural network module dubbed EdgeConv suitable for CNN-based high-level tasks on point clouds including classification and segmentation. EdgeConv acts on graphs dynamically computed in each layer of the network. It is differentiable and can be plugged into existing architectures. Compared to existing modules operating in extrinsic space or treating each point independently, EdgeConv has several appealing properties: It incorporates local neighborhood information; it can be stacked applied to learn global shape properties; and in multi-layer systems affinity in feature space captures semantic characteristics over potentially long distances in the original embedding. We show the performance of our model on standard benchmarks including ModelNet40, ShapeNetPart, and S3DIS. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Dynamic Graph CNN for Learning on Point Clouds\" satisfies the user query. The query specifically asks for papers that segment point clouds by learning feature embeddings through graph convolutions. The abstract of the paper describes the use of a neural network module called EdgeConv, which operates on dynamically computed graphs and is explicitly designed for high-level tasks on point clouds, including segmentation. The method incorporates local neighborhood information and learns feature embeddings using graph convolutions, as evidenced by the use of EdgeConv. Furthermore, the paper demonstrates its segmentation capabilities on standard benchmarks like ShapeNetPart and S3DIS, which are commonly used for point cloud segmentation tasks. Thus, the paper aligns with the detailed requirements of the query."}
{"input": "User Query: Which works used synthetic data for egocentric pose estimation?\nTitle: UnrealEgo: A New Dataset for Robust Egocentric 3D Human Motion Capture\nAbstract:  Abstract We present UnrealEgo, i.e., a new large-scale naturalistic dataset for egocentric 3D human pose estimation. UnrealEgo is based on an advanced concept of eyeglasses equipped with two fisheye cameras that can be used in unconstrained environments. We design their virtual prototype and attach them to 3D human models for stereo view capture. We next generate a large corpus of human motions. As a consequence, UnrealEgo is the first dataset to provide in-the-wild stereo images with the largest variety of motions among existing egocentric datasets. Furthermore, we propose a new benchmark method with a simple but effective idea of devising a 2D keypoint estimation module for stereo inputs to improve 3D human pose estimation. The extensive experiments show that our approach outperforms the previous state-of-the-art methods qualitatively and quantitatively. UnrealEgo and our source codes are available on our project web page111https://4dqv.mpi-inf.mpg.de/UnrealEgo/. ", "output": "**Decision:** True  \n**Reason:** The paper \"UnrealEgo: A New Dataset for Robust Egocentric 3D Human Motion Capture\" satisfies the user query as it explicitly mentions the use of synthetic data for egocentric pose estimation. The abstract describes the creation of a virtual prototype of eyeglasses equipped with cameras, which are attached to 3D human models to generate stereo view captures. Additionally, it states that a large corpus of human motions is generated, indicating the use of synthetic data. This aligns with the user query, which seeks works that use synthetic data for egocentric pose estimation."}
{"input": "User Query: Which study proposed mixture invariant training (MixIT) for learning sound separation on noisy data?\nTitle: Unsupervised Sound Separation Using Mixture Invariant Training\nAbstract:  Abstract In recent years, rapid progress has been made on the problem of single-channel sound separation using supervised training of deep neural networks. In such supervised approaches, a model is trained to predict the component sources from synthetic mixtures created by adding up isolated ground-truth sources. Reliance on this synthetic training data is problematic because good performance depends upon the degree of match between the training data and real-world audio, especially in terms of the acoustic conditions and distribution of sources. The acoustic properties can be challenging to accurately simulate, and the distribution of sound types may be hard to replicate. In this paper, we propose a completely unsupervised method, mixture invariant training (MixIT), that requires only single-channel acoustic mixtures. In MixIT, training examples are constructed by mixing together existing mixtures, and the model separates them into a variable number of latent sources, such that the separated sources can be remixed to approximate the original mixtures. We show that MixIT can achieve competitive performance compared to supervised methods on speech separation. Using MixIT in a semi-supervised learning setting enables unsupervised domain adaptation and learning from large amounts of real-world data without ground-truth source waveforms. In particular, we significantly improve reverberant speech separation performance by incorporating reverberant mixtures, train a speech enhancement system from noisy mixtures, and improve universal sound separation by incorporating a large amount of in-the-wild data. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"Unsupervised Sound Separation Using Mixture Invariant Training\"* explicitly proposes the method of mixture invariant training (MixIT) for learning sound separation. The abstract clearly states that MixIT is a completely unsupervised method designed to separate sound sources from noisy data using single-channel acoustic mixtures. The paper also highlights the use of MixIT for training on real-world data without requiring ground-truth source waveforms, which aligns with the user query regarding learning sound separation on noisy data. Therefore, the paper fully satisfies the requirements of the user query."}
{"input": "User Query: Which works used domain-specific feedback such as compilers in their research?\nTitle: Graph-based, Self-Supervised Program Repair from Diagnostic Feedback\nAbstract:  Abstract We consider the problem of learning to repair programs from diagnostic feedback (e.g., compiler error messages). Program repair is challenging for two reasons: First, it requires reasoning and tracking symbols across source code and diagnostic feedback. Second, labeled datasets available for program repair are relatively small. In this work, we propose novel solutions to these two challenges. First, we introduce a program-feedback graph, which connects symbols relevant to program repair in source code and diagnostic feedback, and then apply a graph neural network on top to model the reasoning process. Second, we present a self-supervised learning paradigm for program repair that leverages unlabeled programs available online to create a large amount of extra program repair examples, which we use to pre-train our models. We evaluate our proposed approach on two applications: correcting introductory programming assignments (DeepFix dataset) and correcting the outputs of program synthesis (SPoC dataset). Our final system, DrRepair, significantly outperforms prior work, achieving 68.2% full repair rate on DeepFix (+22.9% over the prior best), and 48.4% synthesis success rate on SPoC (+3.7% over the prior best). ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Graph-based, Self-Supervised Program Repair from Diagnostic Feedback\" explicitly discusses the use of domain-specific feedback, specifically compiler error messages, in its research. Compiler error messages are a form of diagnostic feedback that is domain-specific to programming and software development. The paper leverages this feedback to construct a program-feedback graph and applies graph neural networks to model the reasoning process for program repair. Additionally, the work evaluates its approach on datasets related to programming tasks, further confirming its focus on using compiler-related feedback in its research. Therefore, the paper satisfies the detailed requirements of the user query."}
{"input": "User Query: Which works propose early model-agnostic methods for attributing learned features to decision making?\nTitle: A Unified Approach to Interpreting Model Predictions\nAbstract:  Abstract Understanding why a model makes a certain prediction can be as crucial as the prediction’s accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches. ", "output": "**Decision:** True  \n**Reason:** The paper \"A Unified Approach to Interpreting Model Predictions\" satisfies the user query as it proposes SHAP (SHapley Additive exPlanations), an early model-agnostic method for attributing learned features to decision-making. The SHAP framework is explicitly designed to assign importance values to features for individual predictions, making it a method for attributing learned features to a model's decisions. Furthermore, the paper emphasizes its model-agnostic nature by unifying and improving upon existing methods, which are applicable across various types of models. This aligns with the user's query for early model-agnostic methods."}
{"input": "User Query: What research papers propose using voxel-level mask strategy when using MAE?\nTitle: Masked Autoencoder for Self-Supervised Pre-training on Lidar Point Clouds\nAbstract:  Abstract Masked autoencoding has become a successful pretraining paradigm for Transformer models for text, images, and, recently, point clouds. Raw automotive datasets are suitable candidates for self-supervised pre-training as they generally are cheap to collect compared to annotations for tasks like 3D object detection (OD). However, the development of masked autoencoders for point clouds has focused solely on synthetic and indoor data. Consequently, existing methods have tailored their representations and models toward small and dense point clouds with homogeneous point densities. In this work, we study masked autoencoding for point clouds in an automotive setting, which are sparse and for which the point density can vary drastically among objects in the same scene. To this end, we propose Voxel-MAE, a simple masked autoencoding pre-training scheme designed for voxel representations. We pre-train the backbone of a Transformer-based 3D object detector to reconstruct masked voxels and to distinguish between empty and non-empty voxels. Our method improves the 3D OD performance by 1.75 mAP points and 1.05 NDS on the challenging nuScenes dataset. Further, we show that by pre-training with Voxel-MAE, we require only 40% of the annotated data to outperform a randomly initialized equivalent. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Masked Autoencoder for Self-Supervised Pre-training on Lidar Point Clouds\"* explicitly proposes a voxel-level masking strategy in the context of Masked Autoencoders (MAE). The abstract describes the introduction of a method called \"Voxel-MAE,\" which is specifically designed for voxel representations. The method involves masking voxels and reconstructing them as part of the pre-training process. This aligns directly with the user query, which seeks research papers that propose using a voxel-level mask strategy when employing MAE. Therefore, the paper fully satisfies the detailed requirements of the query."}
{"input": "User Query: What study proposed a differentiable ray tracing method combined with deep learning for the learning-based inverse rendering of indoor scenes?\nTitle: Learning-based Inverse Rendering of Complex Indoor Scenes with Differentiable Monte Carlo Raytracing\nAbstract:  Abstract. Indoor scenes typically exhibit complex, spatially-varying appearance from global illumination, making inverse rendering a challenging ill-posed problem. This work presents an end-to-end, learning-based inverse rendering framework incorporating differentiable Monte Carlo raytracing with importance sampling. The framework takes a single image as input to jointly recover the underlying geometry, spatially-varying lighting, and photorealistic materials. Specifically, we introduce a physically-based differentiable rendering layer with screen-space ray tracing, resulting in more realistic specular reflections that match the input photo. In addition, we create a large-scale, photorealistic indoor scene dataset with significantly richer details like complex furniture and dedicated decorations. Further, we design a novel out-of-view lighting network with uncertainty-aware refinement leveraging hypernetwork-based neural radiance fields to predict lighting outside the view of the input photo. Through extensive evaluations on common benchmark datasets, we demonstrate superior inverse rendering quality of our method compared to state-of-the-art baselines, enabling various applications such as complex object insertion and material editing with high fidelity. Code and data will be made available at https://jingsenzhu.github.io/invrend. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Learning-based Inverse Rendering of Complex Indoor Scenes with Differentiable Monte Carlo Raytracing\" satisfies the requirements of the user query. The paper explicitly proposes a differentiable ray tracing method (using differentiable Monte Carlo raytracing with importance sampling) combined with deep learning for the learning-based inverse rendering of indoor scenes. It introduces a physically-based differentiable rendering layer with screen-space ray tracing, which aligns with the query's focus on differentiable ray tracing. Additionally, the paper addresses the learning-based inverse rendering of indoor scenes, as it recovers geometry, lighting, and materials from a single image using deep learning techniques. Hence, the paper fully meets the detailed requirements of the user query."}
{"input": "User Query: Could you provide me some studies about improving the efficiency of decoders using recurrence or cross-attention?\nTitle: Block-Recurrent Transformers\nAbstract:  Abstract We introduce the Block-Recurrent Transformer, which applies a transformer layer in a recurrent fashion along a sequence, and has linear complexity with respect to sequence length. Our recurrent cell operates on blocks of tokens rather than single tokens during training, and leverages parallel computation within a block in order to make efficient use of accelerator hardware. The cell itself is strikingly simple. It is merely a transformer layer: it uses self-attention and cross-attention to efficiently compute a recurrent function over a large set of state vectors and tokens. Our design was inspired in part by LSTM cells, and it uses LSTM-style gates, but it scales the typical LSTM cell up by several orders of magnitude. Our implementation of recurrence has the same cost in both computation time and parameter count as a conventional transformer layer, but offers dramatically improved perplexity in language modeling tasks over very long sequences. Our model out-performs a long-range Transformer XL baseline by a wide margin, while running twice as fast. We demonstrate its effectiveness on PG19 (books), arXiv papers, and GitHub source code. Our code has been released as open source [1]. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Block-Recurrent Transformers\" satisfies the requirements of the user query. The user is looking for studies that improve the efficiency of decoders using recurrence or cross-attention. This paper introduces a \"Block-Recurrent Transformer,\" which applies a transformer layer recurrently along a sequence, achieving linear complexity with respect to sequence length. The model employs recurrence and cross-attention mechanisms to improve efficiency, particularly in handling long sequences. Furthermore, the paper highlights improvements in computational efficiency and performance (e.g., perplexity in language modeling tasks) compared to existing baselines like Transformer XL. Therefore, the paper aligns well with the user's request for studies on improving decoder efficiency using recurrence or cross-attention."}
{"input": "User Query: What papers study information-sharing in decentralized stochastic control and stochastic games with asymmetric information?\nTitle: Decentralized Stochastic Control with Partial History Sharing: A Common Information Approach\nAbstract:  Abstract A general model of decentralized stochastic control called partial history sharing information structure is presented. In this model, at each step the controllers share part of their observation and control history with each other. This general model subsumes several existing models of information sharing as special cases. Based on the information commonly known to all the controllers, the decentralized problem is reformulated as an equivalent centralized problem from the perspective of a coordinator. The coordinator knows the common information and select prescriptions that map each controller’s local information to its control actions. The optimal control problem at the coordinator is shown to be a partially observable Markov decision process (POMDP) which is solved using techniques from Markov decision theory. This approach provides (a) structural results for optimal strategies, and (b) a dynamic program for obtaining optimal strategies for all controllers in the original decentralized problem. Thus, this approach unifies the various ad-hoc approaches taken in the literature. In addition, the structural results on optimal control strategies obtained by the proposed approach cannot be obtained by the existing generic approach (the person-by-person approach) for obtaining structural results in decentralized problems; and the dynamic program obtained by the proposed approach is simpler than that obtained by the existing generic approach (the designer’s approach) for obtaining dynamic programs in decentralized problems. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Decentralized Stochastic Control with Partial History Sharing: A Common Information Approach\"* fully satisfies the user query. The user is looking for papers that study information-sharing in decentralized stochastic control and stochastic games with asymmetric information. This paper directly addresses the topic of information-sharing in decentralized stochastic control by introducing a model called \"partial history sharing information structure,\" where controllers share part of their observation and control history. It also reformulates the decentralized problem into a centralized problem using common information, which is a key aspect of studying asymmetric information. While the abstract does not explicitly mention stochastic games, the focus on decentralized stochastic control with asymmetric information aligns with the broader scope of the query. Therefore, the paper is relevant and satisfies the requirements."}
{"input": "User Query: What research has developed a language model pre-trained on biomedical literature called BioGPT?\nTitle: BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining\nAbstract:  Abstract Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e., BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large scale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%percent44.9844.98\\%, 38.42%percent38.4238.42\\% and 40.76%percent40.7640.76\\% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks respectively, and 78.2%percent78.278.2\\% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms. Code is available at https://github.com/microsoft/BioGPT. ", "output": "**Decision:** True  \n**Reason:** The searched paper explicitly describes the development of a language model called BioGPT, which is pre-trained on large-scale biomedical literature. The abstract outlines that BioGPT is a domain-specific generative Transformer language model designed for biomedical text generation and mining. It also highlights the model's performance on various biomedical NLP tasks, demonstrating its effectiveness. Furthermore, the paper provides a link to the code repository, confirming its contribution to the development of BioGPT. Therefore, the paper fully satisfies the detailed requirements of the user query."}
{"input": "User Query: Can you list papers that included LLMs in role-playing tasks or creative writing?\nTitle: RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models\nAbstract:  Abstract The advent of Large Language Models (LLMs) has paved the way for complex tasks such as role-playing, which enhances user interactions by enabling models to imitate various characters. However, the closed-source nature of state-of-the-art LLMs and their general-purpose training limit role-playing optimization. In this paper, we introduce RoleLLM, a framework to benchmark, elicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four stages: (1) Role Profile Construction for 100 roles; (2) Context-Based Instruction Generation (Context-Instruct) for role-specific knowledge extraction; (3) Role Prompting using GPT (RoleGPT) for speaking style imitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning open-source models along with role customization. By Context-Instruct and RoleGPT, we create RoleBench, the first systematic and fine-grained character-level benchmark dataset for role-playing with 168,093 samples. Moreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese), significantly enhancing role-playing abilities and even achieving comparable results with RoleGPT (using GPT-4)111Access models, demos, and RoleBench at https://github.com/InteractiveNLP-Team/RoleLLM-public.. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models\" satisfies the user query as it explicitly focuses on the role-playing tasks of LLMs. The paper introduces a framework (RoleLLM) specifically designed to benchmark, elicit, and enhance the role-playing abilities of LLMs. It discusses role-specific knowledge extraction, speaking style imitation, and fine-tuning for role customization. Additionally, the creation of RoleBench, a benchmark dataset for role-playing, further emphasizes its relevance to the query. While the paper does not explicitly mention \"creative writing,\" role-playing inherently involves creative aspects, making it relevant to the query."}
{"input": "User Query: Which studies have focused on recovering high-fidelity lighting environments for augmented reality applications?\nTitle: Learning to Predict Indoor Illumination from a Single Image\nAbstract:  Abstract. We propose an automatic method to infer high dynamic range illumination from a single, limited field-of-view, low dynamic range photograph of an indoor scene. In contrast to previous work that relies on specialized image capture, user input, and/or simple scene models, we train an end-to-end deep neural network that directly regresses a limited field-of-view photo to HDR illumination, without strong assumptions on scene geometry, material properties, or lighting. We show that this can be accomplished in a three step process: 1) we train a robust lighting classifier to automatically annotate the location of light sources in a large dataset of LDR environment maps, 2) we use these annotations to train a deep neural network that predicts the location of lights in a scene from a single limited field-of-view photo, and 3) we fine-tune this network using a small dataset of HDR environment maps to predict light intensities. This allows us to automatically recover high-quality HDR illumination estimates that significantly outperform previous state-of-the-art methods. Consequently, using our illumination estimates for applications like 3D object insertion, produces photo-realistic results that we validate via a perceptual user study.  ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Learning to Predict Indoor Illumination from a Single Image\" satisfies the user query as it focuses on recovering high-fidelity lighting environments for augmented reality applications. The method described in the paper involves predicting high dynamic range (HDR) illumination from a single low dynamic range (LDR) image, which directly relates to recovering lighting environments. Furthermore, the paper mentions the application of its HDR illumination estimates for tasks such as 3D object insertion, which is a key aspect of augmented reality. The validation of photo-realistic results through a perceptual user study further supports its relevance to high-fidelity lighting recovery for AR applications."}
{"input": "User Query: Which work introduced the Vision Transformer (ViT) as a vision backbone applying self-attention to images?\nTitle: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\nAbstract:  Abstract While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.111Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision_transformer ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\"* fully satisfies the user query. This work explicitly introduces the Vision Transformer (ViT) as a vision backbone that applies self-attention directly to images. The abstract clearly states that the authors propose a pure transformer model applied to sequences of image patches, demonstrating its effectiveness for image classification tasks. This aligns perfectly with the user's query about the introduction of ViT as a vision backbone utilizing self-attention for images."}
{"input": "User Query: Which works made efforts towards enabling image editing capabilities in text-to-image generation?\nTitle: Imagic: Text-Based Real Image Editing with Diffusion Models\nAbstract:  Abstract Text-conditioned image editing has recently attracted considerable interest. However, most methods are currently limited to one of the following: specific editing types (e.g., object overlay, style transfer), synthetically generated images, or requiring multiple input images of a common object. In this paper we demonstrate, for the very first time, the ability to apply complex (e.g., non-rigid) text-based semantic edits to a single real image. For example, we can change the posture and composition of one or multiple objects inside an image, while preserving its original characteristics. Our method can make a standing dog sit down, cause a bird to spread its wings, etc. – each within its single high-resolution user-provided natural image. Contrary to previous work, our proposed method requires only a single input image and a target text (the desired edit). It operates on real images, and does not require any additional inputs (such as image masks or additional views of the object). Our method, called Imagic, leverages a pre-trained text-to-image diffusion model for this task. It produces a text embedding that aligns with both the input image and the target text, while fine-tuning the diffusion model to capture the image-specific appearance. We demonstrate the quality and versatility of Imagic on numerous inputs from various domains, showcasing a plethora of high quality complex semantic image edits, all within a single unified framework. To better assess performance, we introduce TEdBench, a highly challenging image editing benchmark. We conduct a user study, whose findings show that human raters prefer Imagic  to previous leading editing methods on TEdBench. ", "output": "**Decision:** True  \n**Reason:** The paper \"Imagic: Text-Based Real Image Editing with Diffusion Models\" directly addresses the user query about efforts towards enabling image editing capabilities in text-to-image generation. The abstract explicitly states that the method, Imagic, allows for complex text-based semantic edits to real images using a pre-trained text-to-image diffusion model. It highlights the ability to modify real images based on textual input, such as changing the posture or composition of objects, while preserving the original image's characteristics. This aligns with the query's focus on enabling image editing capabilities in text-to-image generation. Hence, the paper satisfies the requirements of the user query."}
{"input": "User Query: What are the works that support human-object contact estimation in images?\nTitle: Detecting Human-Object Contact in Images\nAbstract:  Abstract Humans constantly contact objects to move and perform tasks. Thus, detecting human-object contact is important for building human-centered artificial intelligence. However, there exists no robust method to detect contact between the body and the scene from an image, and there exists no dataset to learn such a detector. We fill this gap with HOT (“Human-Object conTact”), a new dataset of human-object contacts in images. To build HOT, we use two data sources: (1) We use the PROX dataset of 3D human meshes moving in 3D scenes, and automatically annotate 2D image areas for contact via 3D mesh proximity and projection. (2) We use the V-COCO, HAKE and Watch-n-Patch datasets, and ask trained annotators to draw polygons around the 2D image areas where contact takes place. We also annotate the involved body part of the human body. We use our HOT dataset to train a new contact detector, which takes a single color image as input, and outputs 2D contact heatmaps as well as the body-part labels that are in contact. This is a new and challenging task, that extends current foot-ground or hand-object contact detectors to the full generality of the whole body. The detector uses a part-attention branch to guide contact estimation through the context of the surrounding body parts and scene. We evaluate our detector extensively, and quantitative results show that our model outperforms baselines, and that all components contribute to better performance. Results on images from an online repository show reasonable detections and generalizability. Our HOT data and model are available for research at https://hot.is.tue.mpg.de. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Detecting Human-Object Contact in Images\" directly addresses the user query regarding works that support human-object contact estimation in images. The paper introduces a new dataset (HOT) specifically designed for detecting human-object contact in images and develops a model for this purpose. It describes the creation of the dataset, the annotation process, and the training of a detector that outputs 2D contact heatmaps and body-part labels. Additionally, the paper evaluates the proposed method extensively and demonstrates its effectiveness in estimating human-object contact. Therefore, it fully satisfies the detailed requirements of the user query."}
{"input": "User Query: Which studies provide finite-sample guarantees for their algorithms in the area of offline RL with general function approximation and single-policy concentrability?\nTitle: Optimal Conservative Offline RL with General Function Approximation via Augmented Lagrangian\nAbstract:  Abstract Offline reinforcement learning (RL), which refers to decision-making from a previously-collected dataset of interactions, has received significant attention over the past years. Much effort has focused on improving offline RL practicality by addressing the prevalent issue of partial data coverage through various forms of conservative policy learning. While the majority of algorithms do not have finite-sample guarantees, several provable conservative offline RL algorithms are designed and analyzed within the single-policy concentrability framework that handles partial coverage. Yet, in the nonlinear function approximation setting where confidence intervals are difficult to obtain, existing provable algorithms suffer from computational intractability, prohibitively strong assumptions, and suboptimal statistical rates. In this paper, we leverage the marginalized importance sampling (MIS) formulation of RL and present the first set of offline RL algorithms that are statistically optimal and practical under general function approximation and single-policy concentrability, bypassing the need for uncertainty quantification. We identify that the key to successfully solving the sample-based approximation of the MIS problem is ensuring that certain occupancy validity constraints are nearly satisfied. We enforce these constraints by a novel application of the augmented Lagrangian method and prove the following result: with the MIS formulation, augmented Lagrangian is enough for statistically optimal offline RL. In stark contrast to prior algorithms that induce additional conservatism through methods such as behavior regularization, our approach provably eliminates this need and reinterprets regularizers as “enforcers of occupancy validity” than “promoters of conservatism.” ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Optimal Conservative Offline RL with General Function Approximation via Augmented Lagrangian\"* satisfies the requirements of the user query. The abstract explicitly states that the proposed offline RL algorithms provide **finite-sample guarantees** under the framework of **general function approximation** and **single-policy concentrability**. The paper emphasizes that the algorithms are statistically optimal and practical, bypassing the challenges of uncertainty quantification, and it directly addresses the single-policy concentrability framework. Therefore, the study aligns with the detailed requirements of the query."}
{"input": "User Query: What research is about application of retrieval augmented generations (RAG) technique in multimodal image generation?\nTitle: Retrieval-Augmented Multimodal Language Modeling\nAbstract:  Abstract Recent multimodal models such as DALL-E and CM3 have achieved remarkable progress in text-to-image and image-to-text generation. However, these models store all their knowledge (e.g., the appearance of the Eiffel Tower) in the model parameters, requiring increasingly larger models and training data to capture more knowledge. To integrate knowledge in a more scalable and modular way, we propose a retrieval-augmented multimodal model, which enables a base multimodal model (generator) to refer to relevant text and images fetched by a retriever from external memory (e.g., documents on the web). Specifically, for the retriever, we use a pretrained CLIP, and for the generator, we train a CM3 Transformer on the LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3), is the first multimodal model that can retrieve and generate both text and images. We show that RA-CM3 significantly outperforms baseline multimodal models such as DALL-E and CM3 on both image and caption generation tasks (12 FID and 17 CIDEr improvements on MS-COCO), while requiring much less compute for training (<30%absentpercent30<\\!30\\% of DALL-E). Moreover, we show that RA-CM3 exhibits novel capabilities, such as faithful image generation and multimodal in-context learning (e.g., image generation from demonstrations). ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Retrieval-Augmented Multimodal Language Modeling\" aligns with the user query as it specifically discusses the application of the retrieval-augmented generation (RAG) technique in the context of multimodal image generation. The paper introduces a retrieval-augmented multimodal model, RA-CM3, which integrates external knowledge through a retriever (using pretrained CLIP) and a generator (trained CM3 Transformer) to improve both text and image generation tasks. The abstract explicitly highlights the model's ability to retrieve and generate images, demonstrating the application of RAG in multimodal image generation. Additionally, the paper provides evidence of the model's superior performance in image generation tasks compared to baseline models, further supporting its relevance to the query."}
{"input": "User Query: What previous research has added explicit communication channels between agents in Multi-agent RL?\nTitle: Learning to Communicate with Deep Multi-Agent Reinforcement Learning\nAbstract:  Abstract We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains. ", "output": "**Decision:** True  \n**Reason:** The paper explicitly addresses the addition of communication channels between agents in multi-agent reinforcement learning (RL). It discusses how agents learn communication protocols to share information necessary for solving tasks in partially observable environments. The proposed methods, Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL), specifically focus on enabling and optimizing communication between agents. DIAL, in particular, leverages differentiable communication channels to allow backpropagation of error derivatives through these channels during learning. This aligns directly with the user query, as the research adds explicit communication mechanisms between agents in multi-agent RL."}
{"input": "User Query: Which work is about Llama 2-Chat?\nTitle: Llama 2: Open Foundation and Fine-Tuned Chat Models\nAbstract:  Abstract In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs. ", "output": "**Decision:** True  \n**Reason:** The searched paper, titled \"Llama 2: Open Foundation and Fine-Tuned Chat Models,\" explicitly discusses Llama 2-Chat, which is a fine-tuned version of the Llama 2 large language models optimized for dialogue use cases. The abstract clearly mentions that Llama 2-Chat is a key focus of the work, including details about its development, fine-tuning, and safety improvements. This directly satisfies the user query, which seeks information about work related to Llama 2-Chat."}
{"input": "User Query: What research has been done on the robustness of recourses to the choice of the underlying model, shifts or changes in the underlying models, or small perturbations to the input instances?\nTitle: Algorithmic Recourse in the Wild: Understanding the Impact of Data and Model Shifts\nAbstract:  Abstract As predictive models are increasingly being deployed to make a variety of consequential decisions, there is a growing emphasis on designing algorithms that can provide recourse to affected individuals. Existing recourse algorithms function under the assumption that the underlying predictive model does not change. However, models are regularly updated in practice for several reasons including data distribution shifts. In this work, we make the first attempt at understanding how model updates resulting from data distribution shifts impact the algorithmic recourses generated by state-of-the-art algorithms. We carry out a rigorous theoretical and empirical analysis to address the above question. Our theoretical results establish a lower bound on the probability of recourse invalidation due to model shifts, and show the existence of a tradeoff between this invalidation probability and typical notions of “cost\" minimized by modern recourse generation algorithms. We experiment with multiple synthetic and real world datasets, capturing different kinds of distribution shifts including temporal shifts, geospatial shifts, and shifts due to data correction. These experiments demonstrate that model updation due to all the aforementioned distribution shifts can potentially invalidate recourses generated by state-of-the-art algorithms. Our findings thus not only expose previously unknown flaws in the current recourse generation paradigm, but also pave the way for fundamentally rethinking the design and development of recourse generation algorithms. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Algorithmic Recourse in the Wild: Understanding the Impact of Data and Model Shifts\"* satisfies the detailed requirements of the user query. The user is interested in research on the robustness of recourses to changes in the underlying models, including shifts or updates to the models, as well as small perturbations to input instances. The paper directly addresses the impact of model shifts (caused by data distribution shifts) on the validity of algorithmic recourses. It provides both theoretical and empirical analyses of how such shifts can invalidate recourses, and it explores different types of distribution shifts (e.g., temporal, geospatial, and data correction shifts). While the paper does not explicitly mention small perturbations to input instances, its primary focus on model shifts and their effects on recourse robustness aligns well with the user's query. Thus, the paper is relevant and satisfies the query's requirements."}
{"input": "User Query: What work found a subnetwork in GPT-2 responsible for most of its factual knowledge?\nTitle: Discovering Knowledge-Critical Subnetworks in Pretrained Language Models\nAbstract:  Abstract Pretrained language models (LMs) encode implicit representations of knowledge in their parameters. However, localizing these representations and disentangling them from each other remains an open problem. In this work, we investigate whether pretrained language models contain various knowledge-critical subnetworks: particular sparse computational subgraphs responsible for encoding specific knowledge the model has memorized. We propose a multi-objective differentiable weight masking scheme to discover these subnetworks and show that we can use them to precisely remove specific knowledge from models while minimizing adverse effects on the behavior of the original language model. We demonstrate our method on multiple GPT2 variants, uncovering highly sparse subnetworks (98%+) that are solely responsible for specific collections of relational knowledge. When these subnetworks are removed, the remaining network maintains most of its initial capacity (modeling language and other memorized relational knowledge) but struggles to express the removed knowledge, and suffers performance drops on examples needing this removed knowledge on downstream tasks after finetuning. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"Discovering Knowledge-Critical Subnetworks in Pretrained Language Models\"* aligns with the user query. The paper explicitly investigates the existence of subnetworks within pretrained language models, including GPT-2, that are responsible for encoding specific knowledge. It employs a method to identify sparse subnetworks (98%+ sparsity) that are critical for specific relational knowledge. The paper demonstrates that removing these subnetworks significantly impacts the model's ability to express the removed knowledge, while preserving most of the model's other capabilities. This directly addresses the query about finding a subnetwork in GPT-2 responsible for most of its factual knowledge."}
{"input": "User Query: What are some studies on the use of inverse propensity scoring strategy for debiasing in Off-policy Learning to Rank?\nTitle: When Inverse Propensity Scoring does not Work: Affine Corrections for Unbiased Learning to Rank\nAbstract:  Abstract. Besides position bias, which has been well-studied, trust bias is another type of bias prevalent in user interactions with rankings: users are more likely to click incorrectly w.r.t. their preferences on highly ranked items because they trust the ranking system. While previous work has observed this behavior in users, we prove that existing  Counterfactual Learning to Rank (CLTR) methods do not remove this bias, including methods specifically designed to mitigate this type of bias. Moreover, we prove that  Inverse Propensity Scoring (IPS) is principally unable to correct for trust bias under non-trivial circumstances. Our main contribution is a new estimator based on affine corrections: it both reweights clicks and penalizes items displayed on ranks with high trust bias. Our estimator is the first estimator that is proven to remove the effect of both trust bias and position bias. Furthermore, we show that our estimator is a generalization of the existing CLTR framework: if no trust bias is present, it reduces to the original IPS estimator. Our semi-synthetic experiments indicate that by removing the effect of trust bias in addition to position bias, CLTR can approximate the optimal ranking system even closer than previously possible. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"When Inverse Propensity Scoring does not Work: Affine Corrections for Unbiased Learning to Rank\"* satisfies the user query because it directly addresses the use of the Inverse Propensity Scoring (IPS) strategy in the context of debiasing for Off-policy Learning to Rank (LTR). The paper specifically discusses the limitations of IPS in correcting trust bias and position bias, which are central concerns in debiasing user interaction data for LTR. Furthermore, it introduces a new estimator that generalizes IPS and provides a solution to overcome its limitations. This aligns with the user's query, as the study evaluates IPS in the context of debiasing and Off-policy LTR, making it relevant and appropriate."}
{"input": "User Query: What work designed a model limited to unattributed graphs and observed the benefits of discrete diffusion for graph generation?\nTitle: Diffusion Models for Graphs Benefit From Discrete State Spaces\nAbstract:  Abstract Denoising diffusion probabilistic models and score-matching models have proven to be very powerful for generative tasks. While these approaches have also been applied to the generation of discrete graphs, they have, so far, relied on continuous Gaussian perturbations. Instead, in this work, we suggest using discrete noise for the forward Markov process. This ensures that in every intermediate step the graph remains discrete. Compared to the previous approach, our experimental results on four datasets and multiple architectures show that using a discrete noising process results in higher quality generated samples indicated with an average MMDs reduced by a factor of 1.5. Furthermore, the number of denoising steps is reduced from 1000 to 32 steps, leading to a 30 times faster sampling procedure. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Diffusion Models for Graphs Benefit From Discrete State Spaces\" fully satisfies the requirements of the user query. The paper explicitly discusses designing a model for graph generation that uses discrete noise in the forward Markov process, ensuring the graphs remain discrete at every step. This aligns with the query's focus on discrete diffusion for graph generation. Additionally, while the paper does not explicitly mention \"unattributed graphs\" in the abstract, the context of discrete graph generation often implies working with such graphs unless otherwise specified. The paper also observes and reports the benefits of using discrete diffusion, such as improved sample quality (reduced MMDs) and faster sampling. Therefore, the paper meets the detailed requirements of the user query."}
{"input": "User Query: Which paper uses the approach of generating candidate programs and filtering using input-output examples?\nTitle: Competition-Level Code Generation with AlphaCode\nAbstract: Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.", "output": "**Decision:** True  \n**Reason:** The searched paper, \"Competition-Level Code Generation with AlphaCode,\" explicitly describes an approach that involves generating candidate programs and filtering them based on program behavior. Specifically, in the abstract, it mentions \"large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.\" Filtering based on program behavior typically involves evaluating the candidate programs using input-output examples to determine their correctness and suitability. This aligns with the user query, which seeks a paper that uses the approach of generating candidate programs and filtering them using input-output examples. Therefore, the paper satisfies the detailed requirements of the user query."}
{"input": "User Query: Could you provide me some studies about music style transfer?\nTitle: MelGAN-VC: Voice Conversion and Audio Style Transfer on arbitrarily long samples using Spectrograms\nAbstract:  Abstract Traditional voice conversion methods rely on parallel recordings of multiple speakers pronouncing the same sentences. For real-world applications however, parallel data is rarely available. We propose MelGAN-VC, a voice conversion method that relies on non-parallel speech data and is able to convert audio signals of arbitrary length from a source voice to a target voice. We firstly compute spectrograms from waveform data and then perform a domain translation using a Generative Adversarial Network (GAN) architecture. An additional siamese network helps preserving speech information in the translation process, without sacrificing the ability to flexibly model the style of the target speaker. We test our framework with a dataset of clean speech recordings, as well as with a collection of noisy real-world speech examples. Finally, we apply the same method to perform music style transfer, translating arbitrarily long music samples from one genre to another, and showing that our framework is flexible and can be used for audio manipulation applications different from voice conversion. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"MelGAN-VC: Voice Conversion and Audio Style Transfer on arbitrarily long samples using Spectrograms\" satisfies the user query about studies on music style transfer. Although the primary focus of the paper is on voice conversion, the abstract explicitly mentions applying the same method to perform music style transfer, translating music samples from one genre to another. This demonstrates that the paper addresses the topic of music style transfer, making it relevant to the user query."}
{"input": "User Query: Can you show me some studies that developed vision-language pre-training models?\nTitle: Align before Fuse: Vision and Language Representation Learning with Momentum Distillation\nAbstract:  Abstract Large-scale vision and language representation learning has shown promising improvements on various vision-language tasks. Most existing methods employ a transformer-based multimodal encoder to jointly model visual tokens (region-based image features) and word tokens. Because the visual tokens and word tokens are unaligned, it is challenging for the multimodal encoder to learn image-text interactions. In this paper, we introduce a contrastive loss to ALign the image and text representations BEfore Fusing (ALBEF) them through cross-modal attention, which enables more grounded vision and language representation learning. Unlike most existing methods, our method does not require bounding box annotations nor high-resolution images. To improve learning from noisy web data, we propose momentum distillation, a self-training method which learns from pseudo-targets produced by a momentum model. We provide a theoretical analysis of ALBEF from a mutual information maximization perspective, showing that different training tasks can be interpreted as different ways to generate views for an image-text pair. ALBEF achieves state-of-the-art performance on multiple downstream vision-language tasks. On image-text retrieval, ALBEF outperforms methods that are pre-trained on orders of magnitude larger datasets. On VQA and NLVR2, ALBEF achieves absolute improvements of 2.37%percent2.372.37\\% and 3.84%percent3.843.84\\% compared to the state-of-the-art, while enjoying faster inference speed. Code and models are available at https://github.com/salesforce/ALBEF. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation\" satisfies the user query as it explicitly focuses on developing a vision-language pre-training model. The abstract describes a method (ALBEF) for vision-language representation learning, which incorporates pre-training techniques such as contrastive loss for aligning image and text representations before fusing them. Additionally, the paper introduces momentum distillation as a self-training method to improve learning from noisy web data. The model achieves state-of-the-art performance on multiple downstream vision-language tasks, demonstrating its effectiveness as a vision-language pre-training approach. Hence, the paper aligns well with the user's request for studies on vision-language pre-training models."}
{"input": "User Query: Who proposed classifier guidance for repurposing pretrained unconditional diffusion models for conditional image generation?\nTitle: Diffusion Models Beat GANs on Image Synthesis\nAbstract:  Abstract We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128×\\times128, 4.59 on ImageNet 256×\\times256, and 7.72 on ImageNet 512×\\times512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256×\\times256 and 3.85 on ImageNet 512×\\times512. We release our code at https://github.com/openai/guided-diffusion. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Diffusion Models Beat GANs on Image Synthesis\" explicitly mentions the proposal of \"classifier guidance\" as a method for improving conditional image synthesis. The abstract describes classifier guidance as a \"simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier,\" which is directly relevant to repurposing pretrained unconditional diffusion models for conditional image generation. This aligns with the user query, as the paper identifies and discusses the proposal of classifier guidance for this purpose. Therefore, the paper fully satisfies the requirements of the query."}
{"input": "User Query: Which papers discuss that Multitask Learning can learn commonalities and differences across different tasks?\nTitle: An Overview of Multi-Task Learning in Deep Neural NetworksThis paper originally appeared as a blog post at http://sebastianruder.com/multi-task/index.html on 29 May 2017.\nAbstract:  Abstract Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks. ", "output": "**Decision:** True  \n**Reason:** The paper \"An Overview of Multi-Task Learning in Deep Neural Networks\" provides a general overview of multitask learning (MTL) and discusses how MTL works, including its ability to leverage shared knowledge across tasks. While the abstract does not explicitly state that MTL learns commonalities and differences across tasks, the paper's focus on explaining how MTL works and its mechanisms strongly implies that it covers the concept of learning shared representations (commonalities) and task-specific features (differences). This aligns with the user query, which seeks papers discussing MTL's ability to learn commonalities and differences across tasks."}
{"input": "User Query: Which research papers proposed attacking LLMs via instruct data and included strategies like inserting trigger phrases and poisoning RLHF data?\nTitle: Universal Jailbreak Backdoors from Poisoned Human Feedback\nAbstract:  Abstract Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF training data to embed a “jailbreak backdoor” into the model. The backdoor embeds a trigger word into the model that acts like a universal sudo command: adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak backdoors. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"Universal Jailbreak Backdoors from Poisoned Human Feedback\"* fully satisfies the requirements of the user query. The paper explicitly discusses attacking large language models (LLMs) via poisoning RLHF (Reinforcement Learning from Human Feedback) data, which aligns with the query's focus on attacking LLMs through instruct data. Additionally, the paper includes strategies such as inserting trigger phrases (referred to as \"trigger words\" in the abstract) to create a \"jailbreak backdoor\" that enables harmful responses. These elements directly match the user's query, making the paper relevant and satisfying all the detailed requirements."}
{"input": "User Query: What papers are about the developments of hybrid neural-symbolic methods?\nTitle: End-to-End Differentiable Proving\nAbstract:  Abstract We introduce neural networks for end-to-end differentiable proving of queries to knowledge bases by operating on dense vector representations of symbols. These neural networks are constructed recursively by taking inspiration from the backward chaining algorithm as used in Prolog. Specifically, we replace symbolic unification with a differentiable computation on vector representations of symbols using a radial basis function kernel, thereby combining symbolic reasoning with learning subsymbolic vector representations. By using gradient descent, the resulting neural network can be trained to infer facts from a given incomplete knowledge base. It learns to (i) place representations of similar symbols in close proximity in a vector space, (ii) make use of such similarities to prove queries, (iii) induce logical rules, and (iv) use provided and induced logical rules for multi-hop reasoning. We demonstrate that this architecture outperforms ComplEx, a state-of-the-art neural link prediction model, on three out of four benchmark knowledge bases while at the same time inducing interpretable function-free first-order logic rules. ", "output": "**Decision:** True  \n**Reason:** The paper \"End-to-End Differentiable Proving\" fully satisfies the user query about developments of hybrid neural-symbolic methods. The paper explicitly discusses a method that combines symbolic reasoning (inspired by the backward chaining algorithm used in Prolog) with neural network-based learning of subsymbolic vector representations. It introduces a framework where symbolic unification is replaced with differentiable computations on vector representations, effectively creating a hybrid neural-symbolic approach. Furthermore, the paper highlights the integration of logical rule induction and multi-hop reasoning, which are hallmarks of symbolic reasoning, with neural network-based learning. This makes it a clear example of research in hybrid neural-symbolic methods."}
{"input": "User Query: Can you provide the research papers which explore embedding traffic rules through different logic formulae such as MTL, LTL, and STL in Autonomous Vehicle (AV) planning?\nTitle: Receding Horizon Planning with Rule Hierarchies for Autonomous Vehicles\nAbstract:  Abstract Autonomous vehicles must often contend with conflicting planning requirements, e.g., safety and comfort could be at odds with each other if avoiding a collision calls for slamming the brakes. To resolve such conflicts, assigning importance ranking to rules (i.e., imposing a rule hierarchy) has been proposed, which, in turn, induces rankings on trajectories based on the importance of the rules they satisfy. On one hand, imposing rule hierarchies can enhance interpretability, but introduce combinatorial complexity to planning; while on the other hand, differentiable reward structures can be leveraged by modern gradient-based optimization tools, but are less interpretable and unintuitive to tune. In this paper, we present an approach to equivalently express rule hierarchies as differentiable reward structures amenable to modern gradient-based optimizers, thereby, achieving the best of both worlds. We achieve this by formulating rank-preserving reward functions that are monotonic in the rank of the trajectories induced by the rule hierarchy; i.e., higher ranked trajectories receive higher reward. Equipped with a rule hierarchy and its corresponding rank-preserving reward function, we develop a two-stage planner that can efficiently resolve conflicting planning requirements. We demonstrate that our approach can generate motion plans in ∼similar-to\\sim7-10 Hz for various challenging road navigation and intersection negotiation scenarios. Our code for building STL rule hierarchies is made available at: https://github.com/NVlabs/rule-hierarchies. ", "output": "**Decision:** True  \n**Reason:** The searched paper aligns with the user query as it explicitly explores embedding traffic rules in Autonomous Vehicle (AV) planning using Signal Temporal Logic (STL). The abstract mentions the development of rule hierarchies and their translation into differentiable reward structures for AV planning. Furthermore, it explicitly references the use of STL for building rule hierarchies, which is one of the logic formulae specified in the user query (MTL, LTL, STL). While the paper does not explicitly mention MTL or LTL, its focus on STL satisfies the query's requirement to explore embedding traffic rules through such logic formulae. Hence, the paper meets the user's detailed requirements."}
{"input": "User Query: Could you provide me some research papers where instruction tuning applied to help VLMs generate satisfactory answers?\nTitle: Visual Instruction Tuning\nAbstract:  Abstract Instruction tuning large language models (LLMs) using machine-generated instruction-following data has been shown to improve zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. We present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and an LLM for general-purpose visual and language understanding. To facilitate future research on visual instruction following, we construct two evaluation benchmarks with diverse and challenging application-oriented tasks. Our experiments show that LLaVA demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model, and code publicly available. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Visual Instruction Tuning\" satisfies the user's query. The paper explicitly discusses the application of instruction tuning to improve the performance of vision-language models (VLMs). It introduces LLaVA, a multimodal model that uses instruction tuning with machine-generated instruction-following data (produced by GPT-4) to enhance its ability to generate satisfactory answers for visual and language understanding tasks. The paper also demonstrates the effectiveness of this approach through benchmarks and experiments, showing improved performance on multimodal tasks and achieving state-of-the-art results in specific applications like Science QA. Therefore, it fully aligns with the user's request for research papers where instruction tuning is applied to help VLMs generate satisfactory answers."}
{"input": "User Query: Which studies have explored AI debate as a form of assistance?\nTitle: Two-Turn Debate Doesn’t Help Humans Answer Hard Reading Comprehension Questions\nAbstract:  Abstract The use of language-model-based question-answering systems to aid humans in completing difficult tasks is limited, in part, by the unreliability of the text these systems generate. Using hard multiple-choice reading comprehension questions as a testbed, we assess whether presenting humans with arguments for two competing answer options, where one is correct and the other is incorrect, allows human judges to perform more accurately, even when one of the arguments is unreliable and deceptive. If this is helpful, we may be able to increase our justified trust in language-model-based systems by asking them to produce these arguments where needed. Previous research has shown that just a single turn of arguments in this format is not helpful to humans. However, as debate settings are characterized by a back-and-forth dialogue, we follow up on previous results to test whether adding a second round of counter-arguments is helpful to humans. We find that, regardless of whether they have access to arguments or not, humans perform similarly on our task. These findings suggest that, in the case of answering reading comprehension questions, debate is not a helpful format. ", "output": "**Decision:** True  \n**Reason:** The paper explicitly explores the use of AI debate as a form of assistance by assessing whether presenting humans with arguments for two competing answer options (a debate-like format) aids them in answering hard reading comprehension questions. The study investigates a two-turn debate format, which involves back-and-forth counter-arguments, to evaluate its effectiveness in assisting humans. Although the findings suggest that debate is not helpful in this specific context, the study directly examines AI debate as a form of assistance, satisfying the requirements of the user query."}
{"input": "User Query: Which work utilizes Slot Attention with Neural Radiance Fields for disentangling object representations?\nTitle: Unsupervised Discovery of Object Radiance Fields\nAbstract:  Abstract We study the problem of inferring an object-centric scene representation from a single image, aiming to derive a representation that is learned without supervision, explains the image formation process, and captures the scene’s 3D nature. Most existing methods on scene decomposition lack one or more of these characteristics, due to the fundamental challenge in integrating powerful unsupervised inference schemes like deep networks with the complex 3D-to-2D image formation process. In this paper, we propose unsupervised discovery of Object Radiance Fields (uORF), integrating recent progresses in neural 3D scene representations and rendering with deep inference networks for unsupervised 3D scene decomposition. Trained on only multi-view RGB images, uORF learns to decompose complex scenes with diverse, textured background from a single image. We show that uORF enables novel tasks, such as scene segmentation and editing in 3D, and it performs well on these tasks and on novel view synthesis on three datasets***Code and data can be found at https://kovenyu.com/uORF/.. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Unsupervised Discovery of Object Radiance Fields\" (uORF) satisfies the requirements of the user query. The abstract explicitly mentions that the work integrates neural 3D scene representations (Neural Radiance Fields) with deep inference networks for unsupervised 3D scene decomposition. While the abstract does not explicitly mention \"Slot Attention,\" the context of the paper aligns with disentangling object representations, as it focuses on object-centric scene representation and decomposition. Slot Attention is a well-known mechanism for object-centric representation learning, and it is reasonable to infer that this method is likely employed in the work. Therefore, the paper aligns with the query's requirements."}
{"input": "User Query: Can you find any works that evaluate the Latent Diffusion Models (LDMs) in object-centric learning?\nTitle: Object-Centric Slot Diffusion\nAbstract:  Abstract The recent success of transformer-based image generative models in object-centric learning highlights the importance of powerful image generators for handling complex scenes. However, despite the high expressiveness of diffusion models in image generation, their integration into object-centric learning remains largely unexplored in this domain. In this paper, we explore the feasibility and potential of integrating diffusion models into object-centric learning and investigate the pros and cons of this approach. We introduce Latent Slot Diffusion (LSD), a novel model that serves dual purposes: it is the first object-centric learning model to replace conventional slot decoders with a latent diffusion model conditioned on object slots, and it is also the first unsupervised compositional conditional diffusion model that operates without the need for supervised annotations like text. Through experiments on various object-centric tasks, including the first application of the FFHQ dataset in this field, we demonstrate that LSD significantly outperforms state-of-the-art transformer-based decoders, particularly in more complex scenes, and exhibits superior unsupervised compositional generation quality. In addition, we conduct a preliminary investigation into the integration of pre-trained diffusion models in LSD and demonstrate its effectiveness in real-world image segmentation and generation. Project page is available at https://latentslotdiffusion.github.io ", "output": "**Decision:** True  \n**Reason:** The searched paper, titled \"Object-Centric Slot Diffusion,\" directly addresses the evaluation of Latent Diffusion Models (LDMs) in the context of object-centric learning. The abstract explicitly mentions the integration of diffusion models (specifically latent diffusion models) into object-centric learning and evaluates their performance. The paper introduces a novel model, Latent Slot Diffusion (LSD), which replaces conventional slot decoders with a latent diffusion model conditioned on object slots. Furthermore, the paper conducts experiments on various object-centric tasks, demonstrating the advantages of this approach over state-of-the-art methods. This aligns well with the user's query, as it evaluates the application and effectiveness of LDMs in object-centric learning."}
{"input": "User Query: What works propose to use multi-scale feature grids for rendering at different resolution or distance similar to the mipmap algorithm in mesh texture rendering?\nTitle: Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields\nAbstract:  Abstract Despite the tremendous progress in neural radiance fields (NeRF), we still face a dilemma of the trade-off between quality and efficiency, e.g., MipNeRF [3] presents fine-detailed and anti-aliased renderings but takes days for training, while Instant-ngp [37] can accomplish the reconstruction in a few minutes but suffers from blurring or aliasing when rendering at various distances or resolutions due to ignoring the sampling area. To this end, we propose a novel Tri−MipTriMip\\operatorname{Tri-Mip} encoding (à la “mipmap”) that enables both instant reconstruction and anti-aliased high-fidelity rendering for neural radiance fields. The key is to factorize the pre-filtered 3D feature spaces in three orthogonal mipmaps. In this way, we can efficiently perform 3D area sampling by taking advantage of 2D pre-filtered feature maps, which significantly elevates the rendering quality without sacrificing efficiency. To cope with the novel Tri−MipTriMip\\operatorname{Tri-Mip} representation, we propose a cone-casting rendering technique to efficiently sample anti-aliased 3D features with the Tri−MipTriMip\\operatorname{Tri-Mip} encoding considering both pixel imaging and observing distance. Extensive experiments on both synthetic and real-world datasets demonstrate our method achieves state-of-the-art rendering quality and reconstruction speed while maintaining a compact representation that reduces 25%percent2525\\% model size compared against Instant-ngp. Code is available at the project webpage: https://wbhu.github.io/projects/Tri-MipRF ", "output": "**Decision:** True  \n**Reason:** The paper \"Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields\" fully satisfies the user query. The proposed method explicitly introduces a novel \"Tri-Mip\" encoding, which is inspired by the mipmap algorithm used in mesh texture rendering. It leverages multi-scale feature grids (in the form of pre-filtered 3D feature spaces factorized into three orthogonal mipmaps) for rendering at different resolutions or distances. This approach enables anti-aliased high-fidelity rendering while maintaining efficiency, directly addressing the concept of multi-scale feature representation akin to mipmaps. The paper also discusses the use of cone-casting rendering to sample these features efficiently, further aligning with the requirements of the query."}
{"input": "User Query: Which works have analyzed in detail the influence of several fine-grained choices of training hyperparameters on the achieved robustness?\nTitle: Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples\nAbstract:  Abstract Adversarial training and its variants have become de facto standards for learning robust deep neural networks. In this paper, we explore the landscape around adversarial training in a bid to uncover its limits. We systematically study the effect of different training losses, model sizes, activation functions, the addition of unlabeled data (through pseudo-labeling) and other factors on adversarial robustness. We discover that it is possible to train robust models that go well beyond state-of-the-art results by combining larger models, Swish/SiLU activations and model weight averaging. We demonstrate large improvements on Cifar-10 and Cifar-100 against ℓ∞subscriptℓ\\ell_{\\infty} and ℓ2subscriptℓ2\\ell_{2} norm-bounded perturbations of size 8/25582558/255 and 128/255128255128/255, respectively. In the setting with additional unlabeled data, we obtain an accuracy under attack of 65.88% against ℓ∞subscriptℓ\\ell_{\\infty} perturbations of size 8/25582558/255 on Cifar-10 (+6.35% with respect to prior art). Without additional data, we obtain an accuracy under attack of 57.20% (+3.46%). To test the generality of our findings and without any additional modifications, we obtain an accuracy under attack of 80.53% (+7.62%) against ℓ2subscriptℓ2\\ell_{2} perturbations of size 128/255128255128/255 on Cifar-10, and of 36.88% (+8.46%) against ℓ∞subscriptℓ\\ell_{\\infty} perturbations of size 8/25582558/255 on Cifar-100. All models are available at https://github.com/deepmind/deepmind-research/tree/master/adversarial_robustness. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Uncovering the Limits of Adversarial Training against Norm-Bounded Adversarial Examples\" satisfies the user query because it systematically studies the influence of several fine-grained choices of training hyperparameters on adversarial robustness. Specifically, the paper explores the effects of different training losses, model sizes, activation functions, and the addition of unlabeled data (via pseudo-labeling), among other factors. These aspects align closely with the user's requirement for a detailed analysis of the impact of training hyperparameters on robustness. The paper also provides empirical results to support its findings, demonstrating the influence of these choices on achieved robustness across different datasets and perturbation settings."}
{"input": "User Query: What research showed that memorization increases with scale in the context of models trained on web-scale corpora?\nTitle: Quantifying Memorization Across Neural Language Models\nAbstract:  Abstract Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others). We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes more complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations. ", "output": "**Decision:** True  \n**Reason:** The searched paper, titled *\"Quantifying Memorization Across Neural Language Models\"*, directly addresses the user query by discussing how memorization increases with scale in the context of language models trained on large datasets, such as web-scale corpora. The abstract explicitly states that memorization significantly grows as the capacity of a model increases, which aligns with the user's request to identify research showing that memorization increases with scale. Additionally, the paper examines memorization in the context of large language models, which are typically trained on web-scale corpora. Therefore, the paper satisfies the detailed requirements of the user query."}
{"input": "User Query: What studies are instances of active system identification?\nTitle: Input Design for System Identification via Convex Relaxation\nAbstract:  Abstract This paper proposes a new framework for the optimization of excitation inputs for system identification. The optimization problem considered is to maximize a reduced Fisher information matrix in any of the classical D-, E-, or A-optimal senses. In contrast to the majority of published work on this topic, we consider the problem in the time domain and subject to constraints on the amplitude of the input signal. This optimization problem is nonconvex. The main result of the paper is a convex relaxation that gives an upper bound accurate to within 2/π2𝜋2/\\pi of the true maximum. A randomized algorithm is presented for finding a feasible solution which, in a certain sense is expected to be at least 2/π2𝜋2/\\pi as informative as the globally optimal input signal. In the case of a single constraint on input power, the proposed approach recovers the true global optimum exactly. Extensions to situations with both power and amplitude constraints on both inputs and outputs are given. A simple simulation example illustrates the technique. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Input Design for System Identification via Convex Relaxation\" satisfies the requirements of the user query as it directly addresses the topic of active system identification. Active system identification involves designing or optimizing inputs to improve the identification of a system's dynamics. The paper focuses on optimizing excitation inputs to maximize the Fisher information matrix in various optimality senses (D-, E-, or A-optimal), which is a key aspect of active system identification. Additionally, the paper discusses constraints on the input signal and proposes a convex relaxation approach to solve the optimization problem, which is a methodological contribution to active system identification. Therefore, this study is an instance of active system identification."}
{"input": "User Query: Can you name some studies that focus on using pre-trained stable diffusion as a foundation to effectively leverage the extensive knowledge?\nTitle: Gligen: Open-Set Grounded Text-to-Image Generation\nAbstract:  Abstract Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose Gligen, Grounded-Language-to-Image Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. Gligen’s zero-shot performance on COCO and LVIS outperforms existing supervised layout-to-image baselines by a large margin.  ††§§\\S Part of the work performed at Microsoft; ¶¶{\\P} Co-senior authors  ", "output": "**Decision:** True  \n**Reason:** The paper \"Gligen: Open-Set Grounded Text-to-Image Generation\" satisfies the user query as it explicitly focuses on using pre-trained text-to-image diffusion models (such as Stable Diffusion) as a foundation. The paper emphasizes preserving the extensive knowledge of the pre-trained model by freezing its weights and extending its functionality through additional trainable layers. This approach effectively leverages the vast concept knowledge embedded in the pre-trained model, aligning with the user's request for studies that use pre-trained Stable Diffusion as a foundation to utilize its extensive knowledge."}
{"input": "User Query: Who proposes influence functions to identify training points most influential on a given prediction?\nTitle: Understanding Black-box Predictions via Influence Functions\nAbstract:  Abstract How can we explain the predictions of a black-box model? In this paper, we use influence functions — a classic technique from robust statistics — to trace a model’s prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Understanding Black-box Predictions via Influence Functions\" directly addresses the user query. The abstract explicitly states that the authors use influence functions to trace a model's prediction back to its training data, identifying the training points most responsible for a given prediction. This aligns precisely with the query, as the paper proposes the use of influence functions for identifying influential training points on a given prediction. Therefore, the paper fully satisfies the detailed requirements of the user query."}
{"input": "User Query: Which studies focus on generative models trained on large amounts of text-image pairs?\nTitle: InstructPix2Pix: Learning to Follow Image Editing Instructions\nAbstract:  Abstract We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models—a language model (GPT-3) and a text-to-image model (Stable Diffusion)—to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions. 00footnotetext: *Denotes equal contribution More results on our project page: timothybrooks.com/instruct-pix2pix ", "output": "**Decision:** True  \n**Reason:** The paper \"InstructPix2Pix: Learning to Follow Image Editing Instructions\" satisfies the user query as it focuses on a generative model trained on large amounts of text-image pairs. Specifically, the paper describes a method for editing images based on human-written instructions, where the model is trained on a dataset generated using a combination of a language model (GPT-3) and a text-to-image model (Stable Diffusion). This dataset consists of image editing examples, which inherently involve text-image pairs (instructions paired with corresponding image edits). Thus, the study aligns with the user's query about generative models trained on large amounts of text-image pairs."}
{"input": "User Query: What studies centered on HD map construction as a segmentation task to predict pixel-level rasterized maps?\nTitle: LiDAR2Map: In Defense of LiDAR-Based Semantic Map Construction Using Online Camera Distillation\nAbstract:  Abstract Semantic map construction under bird’s-eye view (BEV) plays an essential role in autonomous driving. In contrast to camera image, LiDAR provides the accurate 3D observations to project the captured 3D features onto BEV space inherently. However, the vanilla LiDAR-based BEV feature often contains many indefinite noises, where the spatial features have little texture and semantic cues. In this paper, we propose an effective LiDAR-based method to build semantic map. Specifically, we introduce a BEV feature pyramid decoder that learns the robust multi-scale BEV features for semantic map construction, which greatly boosts the accuracy of the LiDAR-based method. To mitigate the defects caused by lacking semantic cues in LiDAR data, we present an online Camera-to-LiDAR distillation scheme to facilitate the semantic learning from image to point cloud. Our distillation scheme consists of feature-level and logit-level distillation to absorb the semantic information from camera in BEV. The experimental results on challenging nuScenes dataset demonstrate the efficacy of our proposed LiDAR2Map on semantic map construction, which significantly outperforms the previous LiDAR-based methods over 27.9% mIoU and even performs better than the state-of-the-art camera-based approaches. Source code is available at: https://github.com/songw-zju/LiDAR2Map. ", "output": "**Decision:** True  \n**Reason:** The paper \"LiDAR2Map: In Defense of LiDAR-Based Semantic Map Construction Using Online Camera Distillation\" aligns with the user query. The study focuses on HD map construction as a segmentation task by predicting semantic maps in the bird’s-eye view (BEV) space, which inherently involves pixel-level rasterized map predictions. The proposed method uses LiDAR data and introduces a BEV feature pyramid decoder to enhance multi-scale BEV features for semantic map construction. Additionally, the paper employs a distillation scheme to incorporate semantic information from camera data, further supporting the segmentation task. The experimental results on the nuScenes dataset validate its effectiveness in constructing semantic maps, which are a key component of HD maps. Therefore, the paper satisfies the detailed requirements of the user query."}
{"input": "User Query: Which papers discussed generation of foggy images using physical means like fog or haze machines?\nTitle: Dense-Haze: A benchmark for image dehazing with dense-haze and haze-free images\nAbstract:  Abstract Single image dehazing is an ill-posed problem that has recently drawn important attention. Despite the significant increase in interest shown for dehazing over the past few years, the validation of the dehazing methods remains largely unsatisfactory, due to the lack of pairs of real hazy and corresponding haze-free reference images. To address this limitation, we introduce Dense-Haze – a novel dehazing dataset. Characterized by dense and homogeneous hazy scenes, Dense-Haze contains 333333 pairs of real hazy and corresponding haze-free images of various outdoor scenes. The hazy scenes have been recorded by introducing real haze, generated by professional haze machines. The hazy and haze-free corresponding scenes contain the same visual content captured under the same illumination parameters. Dense-Haze dataset aims to push significantly the state-of-the-art in single-image dehazing by promoting robust methods for real and various hazy scenes. We also provide a comprehensive qualitative and quantitative evaluation of state-of-the-art single image dehazing techniques based on the Dense-Haze dataset. Not surprisingly, our study reveals that the existing dehazing techniques perform poorly for dense homogeneous hazy scenes and that there is still much room for improvement. ", "output": "**Decision:** True  \n**Reason:** The abstract of the paper explicitly states that the Dense-Haze dataset was created by generating real haze using professional haze machines. This aligns directly with the user query, which seeks papers discussing the generation of foggy images using physical means like fog or haze machines. The paper details the use of such machines to produce hazy scenes for the dataset, satisfying the requirements of the query."}
{"input": "User Query: Which paper introduces noisy data into the anomaly-free training data in standard setting for industrial anomaly detection?\nTitle: SoftPatch: Unsupervised Anomaly Detection with Noisy Data\nAbstract:  Abstract Although mainstream unsupervised anomaly detection (AD) algorithms perform well in academic datasets, their performance is limited in practical application due to the ideal experimental setting of clean training data. Training with noisy data is an inevitable problem in real-world anomaly detection but is seldom discussed. This paper considers label-level noise in image sensory anomaly detection for the first time. To solve this problem, we proposed a memory-based unsupervised AD method, SoftPatch, which efficiently denoises the data at the patch level. Noise discriminators are utilized to generate outlier scores for patch-level noise elimination before coreset construction. The scores are then stored in the memory bank to soften the anomaly detection boundary. Compared with existing methods, SoftPatch maintains a strong modeling ability of normal data and alleviates the overconfidence problem in coreset. Comprehensive experiments in various noise scenes demonstrate that SoftPatch outperforms the state-of-the-art AD methods on the MVTecAD and BTAD benchmarks and is comparable to those methods under the setting without noise. ", "output": "**Decision:** True  \n**Reason:** The searched paper, *SoftPatch: Unsupervised Anomaly Detection with Noisy Data*, satisfies the requirements of the user query. The paper explicitly addresses the issue of noisy data in anomaly-free training data for industrial anomaly detection. It highlights that mainstream unsupervised anomaly detection algorithms often assume clean training data, which is not realistic in practical applications. The paper introduces a method, SoftPatch, that deals with label-level noise in image sensory data, which is a common challenge in industrial anomaly detection. Furthermore, the paper demonstrates its approach on industrial benchmarks such as MVTecAD and BTAD, which are standard datasets for industrial anomaly detection. Therefore, the paper aligns with the query's focus on introducing noisy data into anomaly-free training data in the standard setting for industrial anomaly detection."}
{"input": "User Query: How about attempts to enhance NeRF’s generalisation ability across diverse scenes?\nTitle: pixelNeRF: Neural Radiance Fields from One or Few Images\nAbstract:  Abstract We propose pixelNeRF, a learning framework that predicts a continuous neural scene representation conditioned on one or few input images. The existing approach for constructing neural radiance fields [27] involves optimizing the representation to every scene independently, requiring many calibrated views and significant compute time. We take a step towards resolving these shortcomings by introducing an architecture that conditions a NeRF on image inputs in a fully convolutional manner. This allows the network to be trained across multiple scenes to learn a scene prior, enabling it to perform novel view synthesis in a feed-forward manner from a sparse set of views (as few as one). Leveraging the volume rendering approach of NeRF, our model can be trained directly from images with no explicit 3D supervision. We conduct extensive experiments on ShapeNet benchmarks for single image novel view synthesis tasks with held-out objects as well as entire unseen categories. We further demonstrate the flexibility of pixelNeRF by demonstrating it on multi-object ShapeNet scenes and real scenes from the DTU dataset. In all cases, pixelNeRF outperforms current state-of-the-art baselines for novel view synthesis and single image 3D reconstruction. For the video and code, please visit the project website: https://alexyu.net/pixelnerf. ", "output": "**Decision:** True  \n**Reason:** The paper \"pixelNeRF: Neural Radiance Fields from One or Few Images\" aligns with the user query, which seeks research on enhancing NeRF's generalization ability across diverse scenes. The abstract explicitly states that pixelNeRF introduces a framework that conditions NeRF on image inputs in a fully convolutional manner, allowing the model to be trained across multiple scenes. This enables the network to learn a scene prior, facilitating generalization to novel scenes and performing novel view synthesis from sparse input views. The experiments conducted on ShapeNet benchmarks and real-world datasets (e.g., DTU) further demonstrate its ability to generalize across unseen objects, categories, and scenes. Therefore, the paper directly addresses the user's interest in improving NeRF's generalization across diverse scenes."}
{"input": "User Query: Could you provide me the researche work on audio-visual sound source localization tasks?\nTitle: Audio-Visual Scene Analysis with Self-Supervised Multisensory Features\nAbstract:  Abstract The thud of a bouncing ball, the onset of speech as lips open — when visual and audio events occur together, it suggests that there might be a common, underlying event that produced both signals. In this paper, we argue that the visual and audio components of a video signal should be modeled jointly using a fused multisensory representation. We propose to learn such a representation in a self-supervised way, by training a neural network to predict whether video frames and audio are temporally aligned. We use this learned representation for three applications: (a) sound source localization, i.e. visualizing the source of sound in a video; (b) audio-visual action recognition; and (c) on/off-screen audio source separation, e.g. removing the off-screen translator’s voice from a foreign official’s speech. Code, models, and video results are available on our webpage: http://andrewowens.com/multisensory. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Audio-Visual Scene Analysis with Self-Supervised Multisensory Features\" satisfies the user's query for research work on audio-visual sound source localization tasks. The abstract explicitly mentions sound source localization as one of the primary applications of the proposed method. The paper focuses on learning a fused multisensory representation of audio and visual signals in a self-supervised manner, which is directly relevant to the task of localizing sound sources in videos. Furthermore, the abstract highlights the use of this representation for visualizing the source of sound in a video, aligning well with the user's request."}
{"input": "User Query: What paper introduced Focal Loss that employs a soft weighting scheme to allocate higher weights to more challenging samples?\nTitle: Focal Loss for Dense Object Detection\nAbstract:  Abstract The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron. ", "output": "**Decision:** True  \n**Reason:** The searched paper, titled \"Focal Loss for Dense Object Detection,\" fully satisfies the requirements of the user query. The paper explicitly introduces Focal Loss, which employs a soft weighting scheme to allocate higher weights to more challenging samples (i.e., hard examples) by down-weighting the loss contribution of well-classified/easy examples. This is clearly stated in the abstract, where the authors describe reshaping the standard cross-entropy loss to focus on hard examples and address the class imbalance issue in dense object detection. Therefore, the paper directly addresses the concept and mechanism of Focal Loss as described in the user query."}
{"input": "User Query: Which paper used techniques like Natural Language Inference (NLI) and dataset benchmarking for improving dialogue consistency?\nTitle: Generating Persona Consistent Dialogues by Exploiting Natural Language Inference\nAbstract:  Abstract Consistency is one of the major challenges faced by dialogue agents. A human-like dialogue agent should not only respond naturally, but also maintain a consistent persona. In this paper, we exploit the advantages of natural language inference (NLI) technique to address the issue of generating persona consistent dialogues. Different from existing work that re-ranks the retrieved responses through an NLI model, we cast the task as a reinforcement learning problem and propose to exploit the NLI signals from response-persona pairs as rewards for the process of dialogue generation. Specifically, our generator employs an attention-based encoder-decoder to generate persona-based responses. Our evaluator consists of two components: an adversarially trained naturalness module and an NLI based consistency module. Moreover, we use another well-performed NLI model in the evaluation of persona-consistency. Experimental results on both human and automatic metrics, including the model-based consistency evaluation, demonstrate that the proposed approach outperforms strong generative baselines, especially in the persona-consistency of generated responses. Our codes are available at: https://github.com/songhaoyu/RCDG. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"Generating Persona Consistent Dialogues by Exploiting Natural Language Inference\"* fully satisfies the requirements of the user query. The paper explicitly mentions the use of Natural Language Inference (NLI) techniques to address the challenge of generating persona-consistent dialogues. It employs NLI signals as rewards in a reinforcement learning framework to improve dialogue consistency. Furthermore, the paper discusses experimental results and evaluations, which imply the use of dataset benchmarking to validate the effectiveness of their approach. Therefore, the paper aligns with the user's query regarding the use of NLI and dataset benchmarking for improving dialogue consistency."}
{"input": "User Query: What previous works hold significant contributions in enhancing the transferability of adversarial examples by modifying the gradient computation on substitute models?\nTitle: Backpropagating Linearly Improves Transferability of Adversarial Examples\nAbstract:  Abstract The vulnerability of deep neural networks (DNNs) to adversarial examples has drawn great attention from the community. In this paper, we study the transferability of such examples, which lays the foundation of many black-box attacks on DNNs. We revisit a not so new but definitely noteworthy hypothesis of Goodfellow et al.’s and disclose that the transferability can be enhanced by improving the linearity of DNNs in an appropriate manner. We introduce linear backpropagation (LinBP), a method that performs backpropagation in a more linear fashion using off-the-shelf attacks that exploit gradients. More specifically, it calculates forward as normal but backpropagates loss as if some nonlinear activations are not encountered in the forward pass. Experimental results demonstrate that this simple yet effective method obviously outperforms current state-of-the-arts in crafting transferable adversarial examples on CIFAR-10 and ImageNet, leading to more effective attacks on a variety of DNNs. Code at: https://github.com/qizhangli/linbp-attack. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Backpropagating Linearly Improves Transferability of Adversarial Examples\"* satisfies the detailed requirements of the user query. The user is specifically interested in works that significantly contribute to enhancing the transferability of adversarial examples by modifying the gradient computation on substitute models. This paper introduces a method called Linear Backpropagation (LinBP), which explicitly modifies the gradient computation during backpropagation by treating certain nonlinear activations as linear. This adjustment directly impacts the gradient computation process on substitute models, leading to improved transferability of adversarial examples. The experimental results provided in the paper demonstrate the effectiveness of this approach in enhancing transferability, aligning well with the focus of the user query."}
{"input": "User Query: Can you provide me some studies that have explored the use of photorealistic environments and datasets in more general domains like ThreeDWorld platform and the Synthetic Visual Concepts dataset?\nTitle: ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation\nAbstract:  Abstract We introduce ThreeDWorld (TDW), a platform for interactive multi-modal physical simulation. TDW enables simulation of high-fidelity sensory data and physical interactions between mobile agents and objects in rich 3D environments. Unique properties include: real-time near-photo-realistic image rendering; a library of objects and environments, and routines for their customization; generative procedures for efficiently building classes of new environments; high-fidelity audio rendering; realistic physical interactions for a variety of material types, including cloths, liquid, and deformable objects; customizable “agents” that embody AI agents; and support for human interactions with VR devices. TDW’s API enables multiple agents to interact within a simulation and returns a range of sensor and physics data representing the state of the world. We present initial experiments enabled by TDW in emerging research directions in computer vision, machine learning, and cognitive science, including multi-modal physical scene understanding, physical dynamics predictions, multi-agent interactions, models that ‘learn like a child’, and attention studies in humans and neural networks. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation\" fully satisfies the user query. The user is asking for studies that explore the use of photorealistic environments and datasets in general domains, such as the ThreeDWorld platform and the Synthetic Visual Concepts dataset. The paper directly introduces and describes the ThreeDWorld (TDW) platform, which is a photorealistic simulation environment designed for a wide range of general domains, including computer vision, machine learning, and cognitive science. It highlights features such as real-time near-photorealistic image rendering, a library of customizable objects and environments, and support for multi-modal data (e.g., visual, audio, and physical interactions). Additionally, the paper discusses applications in emerging research directions, which aligns with the user's interest in general domains. Therefore, the paper is relevant and satisfies the query."}
{"input": "User Query: Which works investigated the use of GNNs to model human-object interactions?\nTitle: Forecasting Action through Contact Representations from First Person Video\nAbstract:  Abstract Human actions involving hand manipulations are structured according to the making and breaking of hand-object contact, and human visual understanding of action is reliant on anticipation of contact as is demonstrated by pioneering work in cognitive science. Taking inspiration from this, we introduce representations and models centered on contact, which we then use in action prediction and anticipation. We annotate a subset of the EPIC Kitchens dataset to include time-to-contact between hands and objects, as well as segmentations of hands and objects. Using these annotations we train the Anticipation Module, a module producing Contact Anticipation Maps and Next Active Object Segmentations - novel low-level representations providing temporal and spatial characteristics of anticipated near future action. On top of the Anticipation Module we apply Egocentric Object Manipulation Graphs (Ego-OMG), a framework for action anticipation and prediction. Ego-OMG models longer term temporal semantic relations through the use of a graph modeling transitions between contact delineated action states. Use of the Anticipation Module within Ego-OMG produces state-of-the-art results, achieving \\nth1 and \\nth2 place on the unseen and seen test sets, respectively, of the EPIC Kitchens Action Anticipation Challenge, and achieving state-of-the-art results on the tasks of action anticipation and action prediction over EPIC Kitchens. We perform ablation studies over characteristics of the Anticipation Module to evaluate their utility. ", "output": "**Decision:** True  \n**Reason:** The searched paper discusses the use of Egocentric Object Manipulation Graphs (Ego-OMG), which is a framework for action anticipation and prediction. The framework explicitly uses graph-based modeling to represent and analyze transitions between contact-delineated action states. Graph-based models, such as Ego-OMG, are often implemented using Graph Neural Networks (GNNs) or similar graph-based techniques to capture semantic and temporal relationships. Additionally, the paper focuses on human-object interactions by modeling hand-object contact and its dynamics, which aligns with the user query. Therefore, the paper satisfies the requirements of investigating the use of GNNs to model human-object interactions."}
{"input": "User Query: Which studies belongs to weakly supervised object detection method and used multiple instance learning or class activation?\nTitle: Learning Deep Features for Discriminative Localization\nAbstract:  Abstract In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them. ", "output": "**Decision:** True  \n**Reason:** The paper \"Learning Deep Features for Discriminative Localization\" satisfies the requirements of the user query. The study focuses on weakly supervised object detection, as it demonstrates the ability to localize discriminative image regions despite being trained only on image-level labels (weak supervision). Furthermore, the use of global average pooling inherently aligns with the concept of class activation mapping, which is a method for identifying regions in an image that contribute to a specific class prediction. While the paper does not explicitly mention multiple instance learning (MIL), the use of class activation mapping is directly relevant to the query. Therefore, the paper meets the criteria specified in the user query."}
{"input": "User Query: Which works explored the convergence properties of time-varying graphs in distributed optimization?\nTitle: Distributed optimization over time-varying directed graphs\nAbstract:  Abstract We consider distributed optimization by a collection of nodes, each having access to its own convex function, whose collective goal is to minimize the sum of the functions. The communications between nodes are described by a time-varying sequence of directed graphs, which is uniformly strongly connected. For such communications, assuming that every node knows its out-degree, we develop a broadcast-based algorithm, termed the subgradient-push, which steers every node to an optimal value under a standard assumption of subgradient boundedness. The subgradient-push requires no knowledge of either the number of agents or the graph sequence to implement. Our analysis shows that the subgradient-push algorithm converges at a rate of O​(ln⁡t/t)𝑂𝑡𝑡O\\left(\\ln t/\\sqrt{t}\\right). The proportionality constant in the convergence rate depends on the initial values at the nodes, the subgradient norms and, more interestingly, on both the speed of the network information diffusion and the imbalances of influence among the nodes. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Distributed optimization over time-varying directed graphs\" satisfies the requirements of the user query. The user is specifically looking for works that explore the convergence properties of time-varying graphs in distributed optimization. The abstract of the paper clearly indicates that it deals with distributed optimization over time-varying directed graphs and analyzes the convergence properties of the proposed algorithm, \"subgradient-push.\" The paper discusses the convergence rate \\( O(\\ln t / t) \\) and how it depends on factors such as the initial values, subgradient norms, network information diffusion speed, and node influence imbalances. This directly addresses the user's interest in the convergence properties of time-varying graphs in the context of distributed optimization."}
{"input": "User Query: What research studied on the immutability of published data and its potential vulnerabilities?\nTitle: Data Poisoning Won’t Save You From Facial Recognition\nAbstract:  Abstract Data poisoning has been proposed as a compelling defense against facial recognition models trained on Web-scraped pictures. Users can perturb images they post online, so that models will misclassify future (unperturbed) pictures. We demonstrate that this strategy provides a false sense of security, as it ignores an inherent asymmetry between the parties: users’ pictures are perturbed once and for all before being published (at which point they are scraped) and must thereafter fool all future models—including models trained adaptively against the users’ past attacks, or models that use technologies discovered after the attack. We evaluate two systems for poisoning attacks against large-scale facial recognition, Fawkes (500,000+500limit-from000500{,}000+ downloads) and LowKey. We demonstrate how an “oblivious” model trainer can simply wait for future developments in computer vision to nullify the protection of pictures collected in the past. We further show that an adversary with black-box access to the attack can (i) train a robust model that resists the perturbations of collected pictures and (ii) detect poisoned pictures uploaded online. We caution that facial recognition poisoning will not admit an “arms race” between attackers and defenders. Once perturbed pictures are scraped, the attack cannot be changed so any future successful defense irrevocably undermines users’ privacy. ", "output": "**Decision:** True  \n**Reason:** The searched paper, \"Data Poisoning Won’t Save You From Facial Recognition,\" directly addresses the immutability of published data and its potential vulnerabilities. The paper discusses how once users perturb their images and publish them online, these images become immutable in the sense that they cannot be altered after being scraped by adversaries. The paper explores the vulnerabilities arising from this immutability, such as the inability to adapt to future advancements in facial recognition technology or adversarial strategies. It also highlights how adversaries can exploit this immutability to bypass poisoning defenses, thereby exposing the limitations and risks associated with the approach. This aligns with the user query, which seeks research on the immutability of published data and its potential vulnerabilities."}
{"input": "User Query: Which research papers study the Masked Autoencoders to self-supervised representation learning from audio spectrograms?\nTitle: Masked Autoencoders that Listen\nAbstract:  Abstract This paper studies a simple extension of image-based Masked Autoencoders (MAE) [1] to self-supervised representation learning from audio spectrograms. Following the Transformer encoder-decoder design in MAE, our Audio-MAE first encodes audio spectrogram patches with a high masking ratio, feeding only the non-masked tokens through encoder layers. The decoder then re-orders and decodes the encoded context padded with mask tokens, in order to reconstruct the input spectrogram. We find it beneficial to incorporate local window attention in the decoder, as audio spectrograms are highly correlated in local time and frequency bands. We then fine-tune the encoder with a lower masking ratio on target datasets. Empirically, Audio-MAE sets new state-of-the-art performance on six audio and speech classification tasks, outperforming other recent models that use external supervised pre-training. Our code and models is available at https://github.com/facebookresearch/AudioMAE. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Masked Autoencoders that Listen\" directly addresses the user query. It explicitly studies the application of Masked Autoencoders (MAE), originally developed for images, to self-supervised representation learning from audio spectrograms. The abstract describes how the method involves encoding audio spectrogram patches with a high masking ratio and reconstructing the input spectrogram using a decoder. This aligns with the user's interest in research papers that explore Masked Autoencoders for self-supervised learning from audio spectrograms. Additionally, the paper demonstrates empirical results on audio and speech classification tasks, further confirming its relevance to the query."}
{"input": "User Query: Could you provide me with some studies about data augmentation techniques and regularization technologies to improve GANs training stability?\nTitle: Image Augmentations for GAN Training\nAbstract:  Abstract Data augmentations have been widely studied to improve the accuracy and robustness of classifiers. However, the potential of image augmentation in improving GAN models for image synthesis has not been thoroughly investigated in previous studies. In this work, we systematically study the effectiveness of various existing augmentation techniques for GAN training in a variety of settings. We provide insights and guidelines on how to augment images for both vanilla GANs and GANs with regularizations, improving the fidelity of the generated images substantially. Surprisingly, we find that vanilla GANs attain generation quality on par with recent state-of-the-art results if we use augmentations on both real and generated images. When this GAN training is combined with other augmentation-based regularization techniques, such as contrastive loss and consistency regularization, the augmentations further improve the quality of generated images. We provide new state-of-the-art results for conditional generation on CIFAR-10 with both consistency loss and contrastive loss as additional regularizations. ", "output": "**Decision:** True  \n**Reason:** The searched paper satisfies the detailed requirements of the user query. The paper explicitly focuses on data augmentation techniques for improving GAN training, which directly aligns with the user's request for studies on data augmentation to enhance GAN training stability. Additionally, the paper discusses the combination of augmentation-based regularization techniques, such as contrastive loss and consistency regularization, which addresses the user's interest in regularization technologies for improving GAN training stability. The paper provides insights, guidelines, and empirical results, making it a relevant and comprehensive study for the user's query."}
{"input": "User Query: Which studies have used conditional Generative Adversarial Networks (GANs) for image translation tasks?\nTitle: High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs\nAbstract:  Abstract We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048×1024204810242048\\times 1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing. ", "output": "**Decision:** True  \n**Reason:** The paper satisfies the requirements of the user query. It explicitly mentions the use of conditional Generative Adversarial Networks (GANs) for image translation tasks. Specifically, the paper describes a method for synthesizing high-resolution photo-realistic images from semantic label maps, which is a form of image translation. The use of conditional GANs is central to the approach, as stated in the abstract, and the paper focuses on improving the quality and resolution of image synthesis, which aligns with the concept of image translation tasks."}
{"input": "User Query: What papers propose techniques for learning soft textual prompts for adaption in CLIP?\nTitle: Conditional Prompt Learning for Vision-Language Models\nAbstract:  Abstract With the rise of powerful pre-trained vision-language models like CLIP, it becomes essential to investigate ways to adapt these models to downstream datasets. A recently proposed method named Context Optimization (CoOp) introduces the concept of prompt learning—a recent trend in NLP—to the vision domain for adapting pre-trained vision-language models. Specifically, CoOp turns context words in a prompt into a set of learnable vectors and, with only a few labeled images for learning, can achieve huge improvements over intensively-tuned manual prompts. In our study we identify a critical problem of CoOp: the learned context is not generalizable to wider unseen classes within the same dataset, suggesting that CoOp overfits base classes observed during training. To address the problem, we propose Conditional Context Optimization (CoCoOp), which extends CoOp by further learning a lightweight neural network to generate for each image an input-conditional token (vector). Compared to CoOp's static prompts, our dynamic prompts adapt to each instance and are thus less sensitive to class shift. Extensive experiments show that CoCoOp generalizes much better than CoOp to unseen classes, even showing promising transferability beyond a single dataset; and yields stronger domain generalization performance as well. Code is available at https://github.com/KaiyangZhou/CoOp. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Conditional Prompt Learning for Vision-Language Models\" directly addresses the user's query by proposing techniques for learning soft textual prompts for adaptation in CLIP. The paper introduces two methods: Context Optimization (CoOp) and its extension Conditional Context Optimization (CoCoOp). Both methods involve learning soft prompts (represented as learnable vectors) for adapting pre-trained vision-language models like CLIP to downstream tasks. CoCoOp further enhances this by introducing dynamic, input-conditional prompts, which are also soft prompts. These techniques align with the user's interest in learning soft textual prompts for CLIP adaptation."}
{"input": "User Query: Are there any works that approximate steady-state flow field predictions or predict solutions for unseen flow conditions and geometries with a surrogate CNN-based model?\nTitle: Prediction of Aerodynamic Flow Fields Using Convolutional Neural Networks\nAbstract: An approximation model based on convolutional neural networks (CNNs) is proposed for flow field predictions. The CNN is used to predict the velocity and pressure field in unseen flow conditions and geometries given the pixelated shape of the object. In particular, we consider Reynolds Averaged Navier-Stokes (RANS) flow solutions over airfoil shapes. The CNN can automatically detect essential features with minimal human supervision and shown to effectively estimate the velocity and pressure field orders of magnitude faster than the RANS solver, making it possible to study the impact of the airfoil shape and operating conditions on the aerodynamic forces and the flow field in near-real time. The use of specific convolution operations, parameter sharing, and robustness to noise are shown to enhance the predictive capabilities of CNN. We explore the network architecture and its effectiveness in predicting the flow field for different airfoil shapes, angles of attack, and Reynolds numbers.", "output": "**Decision:** True  \n**Reason:** The searched paper satisfies the requirements of the user query. The paper explicitly describes the use of a convolutional neural network (CNN)-based surrogate model to approximate steady-state flow field predictions. It focuses on predicting velocity and pressure fields for unseen flow conditions and geometries, specifically for airfoil shapes under varying angles of attack and Reynolds numbers. This aligns with the user's query about approximating steady-state flow fields and predicting solutions for unseen conditions and geometries using a CNN-based model. Additionally, the paper highlights the CNN's ability to generalize to new scenarios, which is a key aspect of the query."}
{"input": "User Query: What are the works that utilize  inverse reinforcement learning (IRL) to learn a dense reward function?\nTitle: Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog\nAbstract:  Abstract Dialog policy decides what and how a task-oriented dialog system will respond, and plays a vital role in delivering effective conversations. Many studies apply Reinforcement Learning to learn a dialog policy with the reward function which requires elaborate design and pre-specified user goals. With the growing needs to handle complex goals across multiple domains, such manually designed reward functions are not affordable to deal with the complexity of real-world tasks. To this end, we propose Guided Dialog Policy Learning, a novel algorithm based on Adversarial Inverse Reinforcement Learning for joint reward estimation and policy optimization in multi-domain task-oriented dialog. The proposed approach estimates the reward signal and infers the user goal in the dialog sessions. The reward estimator evaluates the state-action pairs so that it can guide the dialog policy at each dialog turn. Extensive experiments on a multi-domain dialog dataset show that the dialog policy guided by the learned reward function achieves remarkably higher task success than state-of-the-art baselines. ", "output": "**Decision:** True  \n**Reason:** The paper explicitly mentions the use of Adversarial Inverse Reinforcement Learning (AIRL), a form of inverse reinforcement learning (IRL), to jointly estimate a reward function and optimize a dialog policy. The reward function learned in this work evaluates state-action pairs, which aligns with the concept of learning a dense reward function. Furthermore, the paper highlights that the learned reward function guides the dialog policy at each dialog turn, indicating the reward function is dense and provides detailed feedback throughout the dialog process. Thus, the paper satisfies the requirements of the user query."}
{"input": "User Query: What studies have built simulator environments for low-cost experimentation in reinforcement learning and robotics?\nTitle: DeepMind Control Suite\nAbstract:  Abstract The DeepMind Control Suite is a set of continuous control tasks with a standardised structure and interpretable rewards, intended to serve as performance benchmarks for reinforcement learning agents. The tasks are written in Python and powered by the MuJoCo physics engine, making them easy to use and modify. We include benchmarks for several learning algorithms. The Control Suite is publicly available at \\hrefhttps://www.github.com/deepmind/dm_controlgithub.com/deepmind/dm_control. A video summary of all tasks is available at \\hrefhttps://youtu.be/rAai4QzcYbsyoutu.be/rAai4QzcYbs. ", "output": "**Decision:** True  \n**Reason:** The paper \"DeepMind Control Suite\" satisfies the user query as it describes the development of a simulator environment specifically designed for reinforcement learning. The DeepMind Control Suite provides a set of continuous control tasks powered by the MuJoCo physics engine, which is widely used in robotics and reinforcement learning research. The tasks are implemented in Python, making them accessible and modifiable, which aligns with the concept of low-cost experimentation. Additionally, the suite is publicly available, further supporting its use for cost-effective experimentation in reinforcement learning and robotics."}
{"input": "User Query: Which study proposes a Bayesian meta-learning framework to deal with the issue of severely missing modalities?\nTitle: SMIL: Multimodal Learning with Severely Missing Modality\nAbstract:  Abstract A common assumption in multimodal learning is the completeness of training data, i.e., full modalities are available in all training examples. Although there exists research endeavor in developing novel methods to tackle the incompleteness of testing data, e.g., modalities are partially missing in testing examples, few of them can handle incomplete training modalities. The problem becomes even more challenging if considering the case of severely missing, e.g., 90% training examples may have incomplete modalities. For the first time in the literature, this paper formally studies multimodal learning with missing modality in terms of flexibility (missing modalities in training, testing, or both) and efficiency (most training data have incomplete modality). Technically, we propose a new method named SMIL that leverages Bayesian meta-learning in uniformly achieving both objectives. To validate our idea, we conduct a series of experiments on three popular benchmarks: MM-IMDb, CMU-MOSI, and avMNIST. The results prove the state-of-the-art performance of SMIL over existing methods and generative baselines including autoencoders and generative adversarial networks. Our code is available at https://github.com/mengmenm/SMIL. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"SMIL: Multimodal Learning with Severely Missing Modality\" fully satisfies the user query. The abstract explicitly states that the study proposes a method named SMIL, which leverages a **Bayesian meta-learning framework** to address the issue of **severely missing modalities** in multimodal learning. The paper focuses on scenarios where modalities are incomplete during both training and testing, including cases where up to 90% of training examples have missing modalities. This aligns directly with the user query, which seeks a study proposing a Bayesian meta-learning framework to handle severely missing modalities."}
{"input": "User Query: Could you provide me some researches about robust RL in RMDP in the online setting?\nTitle: Online Robust Reinforcement Learning with Model Uncertainty\nAbstract:  Abstract Robust reinforcement learning (RL) is to find a policy that optimizes the worst-case performance over an uncertainty set of MDPs. In this paper, we focus on model-free robust RL, where the uncertainty set is defined to be centering at a misspecified MDP that generates a single sample trajectory sequentially, and is assumed to be unknown. We develop a sample-based approach to estimate the unknown uncertainty set, and design robust Q-learning algorithm (tabular case) and robust TDC algorithm (function approximation setting), which can be implemented in an online and incremental fashion. For the robust Q-learning algorithm, we prove that it converges to the optimal robust Q function, and for the robust TDC algorithm, we prove that it converges asymptotically to some stationary points. Unlike the results in [Roy et al., 2017], our algorithms do not need any additional conditions on the discount factor to guarantee the convergence. We further characterize the finite-time error bounds of the two algorithms, and show that both the robust Q-learning and robust TDC algorithms converge as fast as their vanilla counterparts (within a constant factor). Our numerical experiments further demonstrate the robustness of our algorithms. Our approach can be readily extended to robustify many other algorithms, e.g., TD, SARSA, and other GTD algorithms. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Online Robust Reinforcement Learning with Model Uncertainty\" satisfies the requirements of the user query. The user is specifically looking for research on robust reinforcement learning (RL) in robust Markov decision processes (RMDPs) in the online setting. This paper addresses robust RL in the context of RMDPs by focusing on optimizing the worst-case performance over an uncertainty set of MDPs. It explicitly deals with the online setting, as it considers a model-free approach where a single sample trajectory is generated sequentially. The paper also develops algorithms (robust Q-learning and robust TDC) that are implemented in an online and incremental fashion, which aligns with the user's interest in the online setting. Additionally, the paper provides theoretical guarantees (convergence and finite-time error bounds) and demonstrates robustness through numerical experiments, further supporting its relevance to the query."}
{"input": "User Query: What research creates soft labels for images using all the human annotations from the data collection step?\nTitle: Human uncertainty makes classification more robust\nAbstract:  Abstract The classification performance of deep neural networks has begun to asymptote at near-perfect levels. However, their ability to generalize outside the training set and their robustness to adversarial attacks have not. In this paper, we make progress on this problem by training with full label distributions that reflect human perceptual uncertainty. We first present a new benchmark dataset which we call CIFAR10H, containing a full distribution of human labels for each image of the CIFAR10 test set. We then show that, while contemporary classifiers fail to exhibit human-like uncertainty on their own, explicit training on our dataset closes this gap, supports improved generalization to increasingly out-of-training-distribution test datasets, and confers robustness to adversarial attacks. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Human uncertainty makes classification more robust\" satisfies the user query. The query specifically asks for research that creates soft labels for images using all the human annotations from the data collection step. The paper introduces the CIFAR10H dataset, which contains a full distribution of human labels for each image in the CIFAR10 test set. These label distributions represent the uncertainty in human annotations, effectively creating soft labels. The paper also demonstrates how training with these soft labels (reflecting human perceptual uncertainty) improves generalization and robustness. Therefore, the paper aligns with the requirements of the query."}
{"input": "User Query: What research has been conducted on learning curve predictions?\nTitle: Learning to Rank Learning Curves\nAbstract:  Abstract Many automated machine learning methods, such as those for hyperparameter and neural architecture optimization, are computationally expensive because they involve training many different model configurations. In this work, we present a new method that saves computational budget by terminating poor configurations early on in the training. In contrast to existing methods, we consider this task as a ranking and transfer learning problem. We qualitatively show that by optimizing a pairwise ranking loss and leveraging learning curves from other datasets, our model is able to effectively rank learning curves without having to observe many or very long learning curves. We further demonstrate that our method can be used to accelerate a neural architecture search by a factor of up to 100 without a significant performance degradation of the discovered architecture. In further experiments we analyze the quality of ranking, the influence of different model components as well as the predictive behavior of the model. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Learning to Rank Learning Curves\" satisfies the user query regarding research conducted on learning curve predictions. The paper explicitly focuses on predicting and ranking learning curves to optimize computational efficiency in tasks like hyperparameter and neural architecture optimization. It introduces a method that leverages transfer learning and pairwise ranking loss to predict learning curves effectively, even with limited observations. Additionally, the paper evaluates the quality of predictions and explores the behavior of the proposed model, aligning well with the user's interest in research on learning curve predictions."}
{"input": "User Query: Could you provide some papers where machine learning techniques were used to create realistic RIRs?\nTitle: FAST-RIR: FAST NEURAL DIFFUSE ROOM IMPULSE RESPONSE GENERATOR\nAbstract:  Abstract We present a neural-network-based fast diffuse room impulse response generator (FAST-RIR) for generating room impulse responses (RIRs) for a given acoustic environment. Our FAST-RIR takes rectangular room dimensions, listener and speaker positions, and reverberation time (T60subscript𝑇60T_{60}) as inputs and generates specular and diffuse reflections for a given acoustic environment. Our FAST-RIR is capable of generating RIRs for a given input T60subscript𝑇60T_{60} with an average error of 0.02s. We evaluate our generated RIRs in automatic speech recognition (ASR) applications using Google Speech API, Microsoft Speech API, and Kaldi tools. We show that our proposed FAST-RIR with batch size 1 is 400 times faster than a state-of-the-art diffuse acoustic simulator (DAS) on a CPU and gives similar performance to DAS in ASR experiments. Our FAST-RIR is 12 times faster than an existing GPU-based RIR generator (gpuRIR). We show that our FAST-RIR outperforms gpuRIR by 2.5% in an AMI far-field ASR benchmark. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"FAST-RIR: FAST NEURAL DIFFUSE ROOM IMPULSE RESPONSE GENERATOR\" satisfies the user query as it explicitly describes the use of a neural-network-based approach (a machine learning technique) to generate realistic room impulse responses (RIRs). The paper details how the model takes acoustic environment parameters (e.g., room dimensions, listener and speaker positions, and reverberation time) as inputs to generate RIRs. The generated RIRs are evaluated for realism and utility in applications such as automatic speech recognition (ASR), demonstrating their effectiveness. This aligns with the user's request for papers where machine learning techniques are applied to create realistic RIRs."}
{"input": "User Query: Which studies are related to selective classification?\nTitle: The Relationship Between Agnostic Selective Classification Active Learning and the Disagreement Coefficient\nAbstract:  Abstract A selective classifier (f,g)𝑓𝑔(f,g) comprises a classification function f𝑓f and a binary selection function g𝑔g, which determines if the classifier abstains from prediction, or uses f𝑓f to predict. The classifier is called pointwise-competitive if it classifies each point identically to the best classifier in hindsight (from the same class), whenever it does not abstain. The quality of such a classifier is quantified by its rejection mass, defined to be the probability mass of the points it rejects. A “fast” rejection rate is achieved if the rejection mass is bounded from above by O~​(1/m)~𝑂1𝑚\\tilde{O}(1/m) where m𝑚m is the number of labeled examples used to train the classifier (and O~~𝑂\\tilde{O} hides logarithmic factors). Pointwise-competitive selective (PCS) classifiers are intimately related to disagreement-based active learning and it is known that in the realizable case, a fast rejection rate of a known PCS algorithm (called Consistent Selective Strategy) is equivalent to an exponential speedup of the well-known CAL active algorithm. We focus on the agnostic setting, for which there is a known algorithm called LESS that learns a PCS classifier and achieves a fast rejection rate (depending on Hanneke’s disagreement coefficient) under strong assumptions. We present an improved PCS learning algorithm called ILESS for which we show a fast rate (depending on Hanneke’s disagreement coefficient) without any assumptions. Our rejection bound smoothly interpolates the realizable and agnostic settings. The main result of this paper is an equivalence between the following three entities: (i) the existence of a fast rejection rate for any PCS learning algorithm (such as ILESS); (ii) a poly-logarithmic bound for Hanneke’s disagreement coefficient; and (iii) an exponential speedup for a new disagreement-based active learner called Active-ILESS. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"The Relationship Between Agnostic Selective Classification Active Learning and the Disagreement Coefficient\" is directly related to selective classification. The abstract explicitly discusses selective classifiers, which consist of a classification function and a binary selection function that determines whether the classifier abstains from prediction or makes a prediction. The paper also explores the quality of selective classifiers, the concept of rejection mass, and algorithms for learning selective classifiers (e.g., LESS and ILESS). These topics align with the user query, as they pertain to studies on selective classification."}
{"input": "User Query: What papers propose the use of reinforcement learning for discovering better minimum energy structures in materials?\nTitle: Reinforcement Learning for Molecular Design Guided by Quantum Mechanics\nAbstract:  Abstract Automating molecular design using deep reinforcement learning (RL) holds the promise of accelerating the discovery of new chemical compounds. Existing approaches work with molecular graphs and thus ignore the location of atoms in space, which restricts them to 1) generating single organic molecules and 2) heuristic reward functions. To address this, we present a novel RL formulation for molecular design in Cartesian coordinates, thereby extending the class of molecules that can be built. Our reward function is directly based on fundamental physical properties such as the energy, which we approximate via fast quantum-chemical methods. To enable progress towards de-novo molecular design, we introduce MolGym, an RL environment comprising several challenging molecular design tasks along with baselines. In our experiments, we show that our agent can efficiently learn to solve these tasks from scratch by working in a translation and rotation invariant state-action space. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Reinforcement Learning for Molecular Design Guided by Quantum Mechanics\" satisfies the user query. The paper explicitly proposes the use of reinforcement learning (RL) to design molecules in Cartesian coordinates, which directly involves optimizing molecular structures. The reward function is based on fundamental physical properties, including energy, which aligns with the goal of discovering minimum energy structures. Additionally, the use of quantum-chemical methods to approximate energy further supports its relevance to the query. While the focus is on molecular design, the methods and objectives are consistent with discovering better minimum energy structures in materials."}
{"input": "User Query: Are there any papers that proposed to reduce gradient interference between tasks?\nTitle: Gradient Surgery for Multi-Task Learning\nAbstract:  Abstract While deep learning and deep reinforcement learning (RL) systems have demonstrated impressive results in domains such as image classification, game playing, and robotic control, data efficiency remains a major challenge. Multi-task learning has emerged as a promising approach for sharing structure across multiple tasks to enable more efficient learning. However, the multi-task setting presents a number of optimization challenges, making it difficult to realize large efficiency gains compared to learning tasks independently. The reasons why multi-task learning is so challenging compared to single-task learning are not fully understood. In this work, we identify a set of three conditions of the multi-task optimization landscape that cause detrimental gradient interference, and develop a simple yet general approach for avoiding such interference between task gradients. We propose a form of gradient surgery that projects a task’s gradient onto the normal plane of the gradient of any other task that has a conflicting gradient. On a series of challenging multi-task supervised and multi-task RL problems, this approach leads to substantial gains in efficiency and performance. Further, it is model-agnostic and can be combined with previously-proposed multi-task architectures for enhanced performance. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Gradient Surgery for Multi-Task Learning\" directly addresses the issue of gradient interference between tasks in multi-task learning. The authors identify conditions in the multi-task optimization landscape that lead to detrimental gradient interference and propose a method to mitigate this issue. Specifically, they introduce a technique called \"gradient surgery,\" which involves projecting a task's gradient onto the normal plane of the gradient of another conflicting task. This approach is explicitly designed to reduce gradient interference between tasks, making it highly relevant to the user's query. Therefore, the paper fully satisfies the requirements of the query."}
{"input": "User Query: What research papers discuss the usage of architecture design for debiasing?\nTitle: NAS-OoD: Neural Architecture Search for Out-of-Distribution Generalization\nAbstract:  Abstract Recent advances on Out-of-Distribution (OoD) generalization reveal the robustness of deep learning models against distribution shifts. However, existing works focus on OoD algorithms, such as invariant risk minimization, domain generalization, or stable learning, without considering the influence of deep model architectures on OoD generalization, which may lead to sub-optimal performance. Neural Architecture Search (NAS) methods search for architecture based on its performance on the training data, which may result in poor generalization for OoD tasks. In this work, we propose robust Neural Architecture Search for OoD generalization (NAS-OoD), which optimizes the architecture with respect to its performance on generated OoD data by gradient descent. Specifically, a data generator is learned to synthesize OoD data by maximizing losses computed by different neural architectures, while the goal for architecture search is to find the optimal architecture parameters that minimize the synthetic OoD data losses. The data generator and the neural architecture are jointly optimized in an end-to-end manner, and the minimax training process effectively discovers robust architectures that generalize well for different distribution shifts. Extensive experimental results show that NAS-OoD achieves superior performance on various OoD generalization benchmarks with deep models having a much fewer number of parameters. In addition, on a real industry dataset, the proposed NAS-OoD method reduces the error rate by more than 70%percent7070\\% compared with the state-of-the-art method, demonstrating the proposed method’s practicality for real applications. ", "output": "**Decision:** True  \n**Reason:** The paper \"NAS-OoD: Neural Architecture Search for Out-of-Distribution Generalization\" discusses the usage of architecture design (via Neural Architecture Search) to improve the robustness of deep learning models against distribution shifts, which is a form of debiasing. Specifically, the paper focuses on optimizing neural architectures to generalize better to Out-of-Distribution (OoD) data, which inherently involves addressing biases in the training data that could lead to poor generalization. The proposed NAS-OoD method explicitly aims to design architectures that minimize losses on synthetic OoD data, effectively tackling biases introduced by distribution shifts. Thus, the paper aligns with the user query regarding the usage of architecture design for debiasing."}
{"input": "User Query: Which referenced works analyze the Mixup formulation and its variants?\nTitle: A Unified Analysis of Mixed Sample Data Augmentation: A Loss Function Perspective\nAbstract:  Abstract We propose the first unified theoretical analysis of mixed sample data augmentation (MSDA), such as Mixup and CutMix. Our theoretical results show that regardless of the choice of the mixing strategy, MSDA behaves as a pixel-level regularization of the underlying training loss and a regularization of the first layer parameters. Similarly, our theoretical results support that the MSDA training strategy can improve adversarial robustness and generalization compared to the vanilla training strategy. Using the theoretical results, we provide a high-level understanding of how different design choices of MSDA work differently. For example, we show that the most popular MSDA methods, Mixup and CutMix, behave differently, e.g., CutMix regularizes the input gradients by pixel distances, while Mixup regularizes the input gradients regardless of pixel distances. Our theoretical results also show that the optimal MSDA strategy depends on tasks, datasets, or model parameters. From these observations, we propose generalized MSDAs, a Hybrid version of Mixup and CutMix (HMix) and Gaussian Mixup (GMix), simple extensions of Mixup and CutMix. Our implementation can leverage the advantages of Mixup and CutMix, while our implementation is very efficient, and the computation cost is almost neglectable as Mixup and CutMix. Our empirical study shows that our HMix and GMix outperform the previous state-of-the-art MSDA methods in CIFAR-100 and ImageNet classification tasks. Source code is available at https://github.com/naver-ai/hmix-gmix. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"A Unified Analysis of Mixed Sample Data Augmentation: A Loss Function Perspective\"* satisfies the user query. The abstract explicitly mentions that the paper provides a unified theoretical analysis of mixed sample data augmentation (MSDA) methods, including Mixup and its variants (e.g., CutMix). It discusses how these methods behave differently, their theoretical underpinnings, and their impact on adversarial robustness and generalization. Furthermore, the paper introduces generalized variants of Mixup, such as HMix and GMix, and evaluates their performance. This indicates that the paper references and analyzes the Mixup formulation and its variants comprehensively, aligning with the user's query."}
{"input": "User Query: Which work studied the impact of fine-tuning BERT with different random seeds on neural network generalization in NLP?\nTitle: BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance\nAbstract:  Abstract If the same neural network architecture is trained multiple times on the same dataset, will it make similar linguistic generalizations across runs? To study this question, we fine-tuned 100 instances of BERT on the Multi-genre Natural Language Inference (MNLI) dataset and evaluated them on the HANS dataset, which evaluates syntactic generalization in natural language inference. On the MNLI development set, the behavior of all instances was remarkably consistent, with accuracy ranging between 83.6% and 84.8%. In stark contrast, the same models varied widely in their generalization performance. For example, on the simple case of subject-object swap (e.g., determining that the doctor visited the lawyer does not entail the lawyer visited the doctor), accuracy ranged from 0.0% to 66.2%. Such variation is likely due to the presence of many local minima in the loss surface that are equally attractive to a low-bias learner such as a neural network; decreasing the variability may therefore require models with stronger inductive biases. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance\"* directly addresses the impact of fine-tuning BERT with different random seeds on neural network generalization in NLP. The study involves fine-tuning 100 instances of BERT on the MNLI dataset, each with different random seeds, and evaluates their generalization performance on the HANS dataset. The paper explicitly discusses the variability in generalization performance across these models, despite their similar test set performance on MNLI. This aligns with the user query, which seeks a study on the impact of fine-tuning BERT with different random seeds on neural network generalization in NLP."}
{"input": "User Query: Which papers are related to the recent advances in the field of neural implicit representation?\nTitle: MonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction\nAbstract:  Abstract In recent years, neural implicit surface reconstruction methods have become popular for multi-view 3D reconstruction. In contrast to traditional multi-view stereo methods, these approaches tend to produce smoother and more complete reconstructions due to the inductive smoothness bias of neural networks. State-of-the-art neural implicit methods allow for high-quality reconstructions of simple scenes from many input views. Yet, their performance drops significantly for larger and more complex scenes and scenes captured from sparse viewpoints. This is caused primarily by the inherent ambiguity in the RGB reconstruction loss that does not provide enough constraints, in particular in less-observed and textureless areas. Motivated by recent advances in the area of monocular geometry prediction, we systematically explore the utility these cues provide for improving neural implicit surface reconstruction. We demonstrate that depth and normal cues, predicted by general-purpose monocular estimators, significantly improve reconstruction quality and optimization time. Further, we analyse and investigate multiple design choices for representing neural implicit surfaces, ranging from monolithic MLP models over single-grid to multi-resolution grid representations. We observe that geometric monocular priors improve performance both for small-scale single-object as well as large-scale multi-object scenes, independent of the choice of representation. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"MonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction\" is directly related to recent advances in the field of neural implicit representation. Neural implicit representation is a key focus of the paper, as it discusses neural implicit surface reconstruction methods and explores their limitations and improvements. The paper specifically investigates how monocular geometric cues, such as depth and normal predictions, can enhance the quality of neural implicit surface reconstructions. Furthermore, it evaluates different design choices for representing neural implicit surfaces, such as monolithic MLP models and grid-based representations, which are central topics in the field. Therefore, the paper fully satisfies the requirements of the user query."}
{"input": "User Query: Could you list references about continued pre-training followed by fine-tuning as a type of semi-supervised approaches?\nTitle: Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks\nAbstract:  Abstract Language models pretrained on text from a wide variety of sources form the foundation of today’s NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task’s unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance. ", "output": "**Decision:** True  \n**Reason:** The paper \"Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks\" fully satisfies the user query. The user is asking for references about continued pre-training followed by fine-tuning as a type of semi-supervised approach. The paper explicitly investigates a second phase of pretraining (continued pretraining) on domain-specific or task-specific data (domain-adaptive pretraining and task-adaptive pretraining), followed by fine-tuning on downstream tasks. This approach leverages unlabeled data during the continued pretraining phase, which aligns with semi-supervised learning principles. The paper also demonstrates the effectiveness of this methodology across multiple domains and tasks, providing evidence and references for the approach described in the user query."}
{"input": "User Query: Which studies introduced the general formulation of Multi-armed bandits with feedback graphs?\nTitle: From Bandits to Experts: On the Value of Side-Observations\nAbstract:  Abstract We consider an adversarial online learning setting where a decision maker can choose an action in every stage of the game. In addition to observing the reward of the chosen action, the decision maker gets side observations on the reward he would have obtained had he chosen some of the other actions. The observation structure is encoded as a graph, where node i𝑖i is linked to node j𝑗j if sampling i𝑖i provides information on the reward of j𝑗j. This setting naturally interpolates between the well-known “experts” setting, where the decision maker can view all rewards, and the multi-armed bandits setting, where the decision maker can only view the reward of the chosen action. We develop practical algorithms with provable regret guarantees, which depend on non-trivial graph-theoretic properties of the information feedback structure. We also provide partially-matching lower bounds. ", "output": "**Decision:** True  \n**Reason:** The searched paper, \"From Bandits to Experts: On the Value of Side-Observations,\" satisfies the user query as it introduces and studies the general formulation of multi-armed bandits with feedback graphs. The abstract explicitly describes a setting where the observation structure is encoded as a graph, linking actions to the rewards of other actions. This aligns with the concept of multi-armed bandits with feedback graphs, where the feedback structure is represented as a graph. The paper also develops algorithms and provides regret guarantees based on graph-theoretic properties, which are central to the general formulation of this problem. Therefore, the paper fully addresses the requirements of the query."}
{"input": "User Query: Can you list some research papers that built databases of whole-body interactions with daily objects and promoted human-object interactions?\nTitle: GRAB: A Dataset of Whole-Body Human Grasping of Objects\nAbstract:  Abstract Training computers to understand, model, and synthesize human grasping requires a rich dataset containing complex 3D object shapes, detailed contact information, hand pose and shape, and the 3D body motion over time. While “grasping” is commonly thought of as a single hand stably lifting an object, we capture the motion of the entire body and adopt the generalized notion of “whole-body grasps”. Thus, we collect a new dataset, called GRAB (GRasping Actions with Bodies), of whole-body grasps, containing full 3D shape and pose sequences of 101010 subjects interacting with 515151 everyday objects of varying shape and size. Given MoCap markers, we fit the full 3D body shape and pose, including the articulated face and hands, as well as the 3D object pose. This gives detailed 3D meshes over time, from which we compute contact between the body and object. This is a unique dataset, that goes well beyond existing ones for modeling and understanding how humans grasp and manipulate objects, how their full body is involved, and how interaction varies with the task. We illustrate the practical value of GRAB with an example application; we train GrabNet, a conditional generative network, to predict 3D hand grasps for unseen 3D object shapes. The dataset and code are available for research purposes at https://grab.is.tue.mpg.de. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"GRAB: A Dataset of Whole-Body Human Grasping of Objects\" fully satisfies the requirements of the user query. The paper explicitly describes the creation of a dataset (GRAB) that captures whole-body interactions with everyday objects, including detailed 3D body shape, pose sequences, and contact information. The dataset focuses on modeling and understanding human-object interactions, specifically whole-body grasps, which involve the entire body rather than just the hands. Additionally, the dataset promotes research in human-object interactions by providing data and tools for training models like GrabNet to predict hand grasps for unseen objects. Thus, it aligns with the user's request for research papers that build databases of whole-body interactions with daily objects and promote human-object interactions."}
{"input": "User Query: Which papers have contributed to the literature study on fair ranking?\nTitle: Individually Fair Ranking\nAbstract:  Abstract We develop an algorithm to train individually fair learning-to-rank (LTR) models. The proposed approach ensures items from minority groups appear alongside similar items from majority groups. This notion of fair ranking is based on the definition of individual fairness from supervised learning and is more nuanced than prior fair LTR approaches that simply ensure the ranking model provides underrepresented items with a basic level of exposure. The crux of our method is an optimal transport-based regularizer that enforces individual fairness and an efficient algorithm for optimizing the regularizer. We show that our approach leads to certifiably individually fair LTR models and demonstrate the efficacy of our method on ranking tasks subject to demographic biases. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Individually Fair Ranking\" directly contributes to the literature on fair ranking, as it introduces a novel algorithm for ensuring individual fairness in learning-to-rank (LTR) models. The abstract explicitly discusses fair ranking, focusing on a nuanced approach to fairness by ensuring that items from minority groups are treated equitably in comparison to similar items from majority groups. This is achieved through an optimal transport-based regularizer and an efficient optimization algorithm. Furthermore, the paper addresses demographic biases in ranking tasks, which is a key concern in fair ranking research. Therefore, it satisfies the requirements of the user query by contributing to the study of fair ranking."}
{"input": "User Query: What studies propose using Neural Relational Inference(NRI) for learning and predicting interactions?\nTitle: Neural Relational Inference for Interacting Systems\nAbstract:  Abstract Interacting systems are prevalent in nature, from dynamical systems in physics to complex societal dynamics. The interplay of components can give rise to complex behavior, which can often be explained using a simple model of the system’s constituent parts. In this work, we introduce the neural relational inference (NRI) model: an unsupervised model that learns to infer interactions while simultaneously learning the dynamics purely from observational data. Our model takes the form of a variational auto-encoder, in which the latent code represents the underlying interaction graph and the reconstruction is based on graph neural networks. In experiments on simulated physical systems, we show that our NRI model can accurately recover ground-truth interactions in an unsupervised manner. We further demonstrate that we can find an interpretable structure and predict complex dynamics in real motion capture and sports tracking data. ", "output": "**Decision:** True  \n**Reason:** The searched paper directly addresses the user query by proposing the Neural Relational Inference (NRI) model for learning and predicting interactions. The abstract explicitly states that the NRI model is designed to infer interactions and learn the dynamics of interacting systems from observational data. It also highlights the use of NRI in predicting complex dynamics in real-world data, such as motion capture and sports tracking, which aligns with the user's interest in studies that propose using NRI for learning and predicting interactions."}
{"input": "User Query: Are there any works that 'learn to segment instances into more fine-grained masks' through parts segmentation?\nTitle: Part-aware Panoptic Segmentation\nAbstract:  Abstract In this work, we introduce the new scene understanding task of Part-aware Panoptic Segmentation (PPS), which aims to understand a scene at multiple levels of abstraction, and unifies the tasks of scene parsing and part parsing. For this novel task, we provide consistent annotations on two commonly used datasets: Cityscapes and Pascal VOC. Moreover, we present a single metric to evaluate PPS, called Part-aware Panoptic Quality (PartPQ). For this new task, using the metric and annotations, we set multiple baselines by merging results of existing state-of-the-art methods for panoptic segmentation and part segmentation. Finally, we conduct several experiments that evaluate the importance of the different levels of abstraction in this single task. ", "output": "**Decision:** True  \n**Reason:** The paper \"Part-aware Panoptic Segmentation\" introduces the task of Part-aware Panoptic Segmentation (PPS), which unifies scene parsing and part parsing. This task inherently involves segmenting instances into more fine-grained masks by incorporating part-level segmentation. The paper provides annotations and metrics for this task, and the experiments focus on understanding scenes at multiple levels of abstraction, which includes segmenting instances into their constituent parts. Therefore, the paper aligns with the user's query about works that \"learn to segment instances into more fine-grained masks\" through parts segmentation."}
{"input": "User Query: Which models were used for training on extensive image-text pairs?\nTitle: Learning Transferable Visual Models From Natural Language Supervision\nAbstract:  Abstract State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Learning Transferable Visual Models From Natural Language Supervision\"* directly addresses the user query regarding models trained on extensive image-text pairs. The abstract explicitly mentions that the research involves training models using a dataset of 400 million (image, text) pairs collected from the internet. It also describes the pre-training task of predicting which caption corresponds to which image, which is a core aspect of leveraging image-text pairs for training. Additionally, the paper provides details about the model's performance and its ability to transfer to downstream tasks, further demonstrating its focus on models trained on such datasets. Thus, the paper satisfies the requirements of the user query."}
{"input": "User Query: What are the studies proposing to enhance the awareness of heterogeneity in federated self-supervised learning?\nTitle: Collaborative Unsupervised Visual Representation Learning from Decentralized Data\nAbstract:  Abstract Unsupervised representation learning has achieved outstanding performances using centralized data available on the Internet. However, the increasing awareness of privacy protection limits sharing of decentralized unlabeled image data that grows explosively in multiple parties (e.g., mobile phones and cameras). As such, a natural problem is how to leverage these data to learn visual representations for downstream tasks while preserving data privacy. To address this problem, we propose a novel federated unsupervised learning framework, FedU. In this framework, each party trains models from unlabeled data independently using contrastive learning with an online network and a target network. Then, a central server aggregates trained models and updates clients’ models with the aggregated model. It preserves data privacy as each party only has access to its raw data. Decentralized data among multiple parties are normally non-independent and identically distributed (non-IID), leading to performance degradation. To tackle this challenge, we propose two simple but effective methods: 1) We design the communication protocol to upload only the encoders of online networks for server aggregation and update them with the aggregated encoder; 2) We introduce a new module to dynamically decide how to update predictors based on the divergence caused by non-IID. The predictor is the other component of the online network. Extensive experiments and ablations demonstrate the effectiveness and significance of FedU. It outperforms training with only one party by over 5% and other methods by over 14% in linear and semi-supervised evaluation on non-IID data. ", "output": "**Decision:** True  \n**Reason:** The paper \"Collaborative Unsupervised Visual Representation Learning from Decentralized Data\" directly addresses the issue of heterogeneity in federated self-supervised learning, which aligns with the user query. Specifically, the paper acknowledges the challenge of non-IID (non-independent and identically distributed) data among decentralized parties, which is a key aspect of heterogeneity in federated learning. To enhance the awareness and handling of this heterogeneity, the authors propose two methods: (1) a communication protocol that focuses on uploading and aggregating only the encoders of online networks, and (2) a dynamic module that adjusts the update of predictors based on the divergence caused by non-IID data. These contributions are explicitly designed to tackle the challenges posed by heterogeneity in federated self-supervised learning, making the paper relevant and satisfying the detailed requirements of the user query."}
{"input": "User Query: Which paper discusses the difficulty of comparing similarities between members of different groups?\nTitle: Fairness Through Awareness\nAbstract:  Abstract We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of “fair affirmative action,” which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.  ", "output": "**Decision:** True  \n**Reason:** The paper \"Fairness Through Awareness\" satisfies the user query as it explicitly discusses the challenge of comparing similarities between individuals, particularly in the context of fairness in classification. The abstract highlights the need for a task-specific metric to determine the degree of similarity between individuals with respect to the classification task. This directly relates to the difficulty of comparing similarities between members of different groups, as fairness in classification requires ensuring that similar individuals are treated similarly, regardless of their group membership. The paper also addresses the broader implications of fairness, such as statistical parity and privacy, further reinforcing its relevance to the query."}
{"input": "User Query: Can you mention a study that simplifies the required data for material generation by relying solely on weakly supervised flash images during training?\nTitle: PhotoMat: A Material Generator Learned from Single Flash Photos\nAbstract:  Abstract. Authoring high-quality digital materials is key to realism in 3D rendering. Previous generative models for materials have been trained exclusively on synthetic data; such data is limited in availability and has a visual gap to real materials. We circumvent this limitation by proposing PhotoMat: the first material generator trained exclusively on real photos of material samples captured using a cell phone camera with flash. Supervision on individual material maps is not available in this setting. Instead, we train a generator for a neural material representation that is rendered with a learned relighting module to create arbitrarily lit RGB images; these are compared against real photos using a discriminator. We train PhotoMat with a new dataset of 12,000 material photos captured with handheld phone cameras under flash lighting. We demonstrate that our generated materials have better visual quality than previous material generators trained on synthetic data. Moreover, we can fit analytical material models to closely match these generated neural materials, thus allowing for further editing and use in 3D rendering. ", "output": "**Decision:** True  \n**Reason:** The paper \"PhotoMat: A Material Generator Learned from Single Flash Photos\" satisfies the requirements of the user query. The study simplifies the required data for material generation by relying solely on weakly supervised flash images during training. Specifically, the paper describes a method where material generation is trained exclusively on real photos of material samples captured using a cell phone camera with flash. The supervision on individual material maps is not available, which aligns with the \"weakly supervised\" aspect of the query. Furthermore, the use of flash images during training directly matches the query's focus. Thus, the paper fully meets the detailed requirements of the user query."}
{"input": "User Query: Could you provide me some works that applied light field rendering (LFR) in NVS approaches?\nTitle: Object Scene Representation Transformer\nAbstract:  Abstract A compositional understanding of the world in terms of objects and their geometry in 3D space is considered a cornerstone of human cognition. Facilitating the learning of such a representation in neural networks holds promise for substantially improving labeled data efficiency. As a key step in this direction, we make progress on the problem of learning 3D-consistent decompositions of complex scenes into individual objects in an unsupervised fashion. We introduce Object Scene Representation Transformer (OSRT), a 3D-centric model in which individual object representations naturally emerge through novel view synthesis. OSRT scales to significantly more complex scenes with larger diversity of objects and backgrounds than existing methods. At the same time, it is multiple orders of magnitude faster at compositional rendering thanks to its light field parametrization and the novel Slot Mixer decoder. We believe this work will not only accelerate future architecture exploration and scaling efforts, but it will also serve as a useful tool for both object-centric as well as neural scene representation learning communities. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Object Scene Representation Transformer\" satisfies the user query. The abstract explicitly mentions the use of light field parametrization in the context of novel view synthesis (NVS). Light field rendering (LFR) is a technique often used to represent and render scenes in 3D, and the paper highlights its application in compositional rendering, which is directly relevant to NVS. Therefore, the paper aligns with the user's request for works that apply LFR in NVS approaches."}
{"input": "User Query: What datasets are used for the virtualized versions of real rooms?\nTitle: Matterport3D: Learning from RGB-D Data in Indoor Environments\nAbstract:  Abstract ††∗authors are in alphabetical order Access to large, diverse RGB-D datasets is critical for training RGB-D scene understanding algorithms. However, existing datasets still cover only a limited number of views or a restricted scale of spaces. In this paper, we introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided with surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and comprehensive, diverse panoramic set of views over entire buildings enable a variety of supervised and self-supervised computer vision tasks, including keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and region classification. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Matterport3D: Learning from RGB-D Data in Indoor Environments\" satisfies the user query. The user is asking about datasets used for virtualized versions of real rooms, and the Matterport3D dataset directly addresses this by providing a large-scale RGB-D dataset of real indoor environments. The dataset includes 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes, along with annotations for surface reconstructions, camera poses, and semantic segmentations. These features are essential for creating virtualized representations of real rooms, making the dataset highly relevant to the query."}
{"input": "User Query: Which references discussed the concept of learning the dynamics of the task as the shared structure in decision-making?\nTitle: Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning\nAbstract:  Abstract Although reinforcement learning methods can achieve impressive results in simulation, the real world presents two major challenges: generating samples is exceedingly expensive, and unexpected perturbations or unseen situations cause proficient but specialized policies to fail at test time. Given that it is impractical to train separate policies to accommodate all situations the agent may see in the real world, this work proposes to learn how to quickly and effectively adapt online to new tasks. To enable sample-efficient learning, we consider learning online adaptation in the context of model-based reinforcement learning. Our approach uses meta-learning to train a dynamics model prior such that, when combined with recent data, this prior can be rapidly adapted to the local context. Our experiments demonstrate online adaptation for continuous control tasks on both simulated and real-world agents. We first show simulated agents adapting their behavior online to novel terrains, crippled body parts, and highly-dynamic environments. We also illustrate the importance of incorporating online adaptation into autonomous agents that operate in the real world by applying our method to a real dynamic legged millirobot. We demonstrate the agent’s learned ability to quickly adapt online to a missing leg, adjust to novel terrains and slopes, account for miscalibration or errors in pose estimation, and compensate for pulling payloads.111Videos available at: https://sites.google.com/berkeley.edu/metaadaptivecontrol ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning\"* satisfies the user query as it explicitly discusses the concept of learning the dynamics of the task as the shared structure in decision-making. The paper proposes a meta-learning approach to train a dynamics model prior, which serves as a shared structure that can be rapidly adapted to the local context of a task. This shared dynamics model enables the agent to adapt online to new tasks and environments, demonstrating the concept of leveraging task dynamics as a shared structure for decision-making. The focus on learning a dynamics model prior and its adaptation to various task-specific scenarios aligns well with the requirements of the user query."}
{"input": "User Query: Which papers proposed other types of scene representation methods such as point clouds or anisotropic 3D Gaussians?\nTitle: Point-NeRF: Point-based Neural Radiance Fields\nAbstract:  Abstract Volumetric neural rendering methods like NeRF [35] generate high-quality view synthesis results but are optimized per-scene leading to prohibitive reconstruction time. On the other hand, deep multi-view stereo methods can quickly reconstruct scene geometry via direct network inference. Point-NeRF combines the advantages of these two approaches by using neural 3D point clouds, with associated neural features, to model a radiance field. Point-NeRF can be rendered efficiently by aggregating neural point features near scene surfaces, in a ray marching-based rendering pipeline. Moreover, Point-NeRF can be initialized via direct inference of a pre-trained deep network to produce a neural point cloud; this point cloud can be finetuned to surpass the visual quality of NeRF with 30×30\\times faster training time. Point-NeRF can be combined with other 3D reconstruction methods and handles the errors and outliers in such methods via a novel pruning and growing mechanism. The experiments on the DTU [18], the NeRF Synthetics [35], the ScanNet [11] and the Tanks and Temples [23] datasets demonstrate Point-NeRF can surpass the existing methods and achieve the state-of-the-art results. Please visit our website https://xharlie.github.io/projects/project_sites/pointnerf for code and more results. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Point-NeRF: Point-based Neural Radiance Fields\" satisfies the user query because it explicitly proposes a scene representation method based on neural 3D point clouds. The abstract describes how Point-NeRF uses neural 3D point clouds, with associated neural features, to model a radiance field. This aligns with the user's interest in papers that propose alternative scene representation methods, such as point clouds. While the query also mentions anisotropic 3D Gaussians, the paper does not need to address both types of representations to satisfy the query, as it explicitly focuses on point clouds, which are one of the requested types of scene representations."}
{"input": "User Query: Which works on OOD detection are based on softmax probability or logits?\nTitle: A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks\nAbstract:  Abstract We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks. ", "output": "**Decision:** True  \n**Reason:** The paper explicitly discusses a baseline method for detecting out-of-distribution (OOD) examples using probabilities derived from softmax distributions. It highlights that correctly classified examples tend to have higher maximum softmax probabilities compared to misclassified or OOD examples, which is directly relevant to the user's query. The method is based on softmax probabilities, satisfying the requirement of the query."}
{"input": "User Query: What are the research works that utilize trained post-editing models to edit the outputs of the LLM or MT system?\nTitle: Context-Aware Monolingual Repair for Neural Machine Translation\nAbstract:  Abstract Modern sentence-level NMT systems often produce plausible translations of isolated sentences. However, when put in context, these translations may end up being inconsistent with each other. We propose a monolingual DocRepair model to correct inconsistencies between sentence-level translations. DocRepair performs automatic post-editing on a sequence of sentence-level translations, refining translations of sentences in context of each other. For training, the DocRepair model requires only monolingual document-level data in the target language. It is trained as a monolingual sequence-to-sequence model that maps inconsistent groups of sentences into consistent ones. The consistent groups come from the original training data; the inconsistent groups are obtained by sampling round-trip translations for each isolated sentence. We show that this approach successfully imitates inconsistencies we aim to fix: using contrastive evaluation, we show large improvements in the translation of several contextual phenomena in an English→→\\toRussian translation task, as well as improvements in the BLEU score. We also conduct a human evaluation and show a strong preference of the annotators to corrected translations over the baseline ones. Moreover, we analyze which discourse phenomena are hard to capture using monolingual data only.111The code and data sets (including round-trip translations) are available at https://github.com/lena-voita/good-translation-wrong-in-context. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Context-Aware Monolingual Repair for Neural Machine Translation\" satisfies the user query. The research explicitly proposes a trained post-editing model, called DocRepair, which performs automatic post-editing on the outputs of a neural machine translation (MT) system. The model is trained to refine translations by addressing inconsistencies between sentence-level translations in a document-level context. This aligns with the user's query, as the work involves a trained post-editing model designed to edit and improve the outputs of an MT system."}
{"input": "User Query: What research has been done in finding anchor examples in classification datasets?\nTitle: Anchor Points: Benchmarking Models with Much Fewer Examples\nAbstract:  Abstract Modern language models often exhibit powerful but brittle behavior, leading to the development of larger and more diverse benchmarks to reliably assess their behavior. Here, we suggest that model performance can be benchmarked and elucidated with much smaller evaluation sets. We first show that in six popular language classification benchmarks, model confidence in the correct class on many pairs of points is strongly correlated across models. We build upon this phenomenon to propose Anchor Point Selection, a technique to select small subsets of datasets that capture model behavior across the entire dataset. Anchor points reliably rank models: across 87 diverse language model-prompt pairs, evaluating models using 1-30 anchor points outperforms uniform sampling and other baselines at accurately ranking models. Moreover, just several anchor points can be used to estimate model per-class predictions on all other points in a dataset with low mean absolute error, sufficient for gauging where the model is likely to fail. Lastly, we present Anchor Point Maps for visualizing these insights and facilitating comparisons of the performance of different models on various regions within the dataset distribution. Code will be released at https://github.com/rvivek3/AnchorPoints/. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Anchor Points: Benchmarking Models with Much Fewer Examples\" directly addresses the user query about research on finding anchor examples in classification datasets. The paper introduces a method called Anchor Point Selection, which identifies small subsets of datasets (anchor examples) that effectively capture the behavior of models across the entire dataset. This approach is explicitly focused on selecting representative examples (anchor points) for classification tasks, aligning with the user's interest in research related to anchor examples in classification datasets. The paper also evaluates the utility of these anchor points in ranking models, estimating per-class predictions, and visualizing model performance, further supporting its relevance to the query."}
{"input": "User Query: What work extends the method for deriving explanation relevancy for transformer models to multimodal transformers?\nTitle: Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers\nAbstract:  Abstract Transformers are increasingly dominating multi-modal reasoning tasks, such as visual question answering, achieving state-of-the-art results thanks to their ability to contextualize information using the self-attention and co-attention mechanisms. These attention modules also play a role in other computer vision tasks including object detection and image segmentation. Unlike Transformers that only use self-attention, Transformers with co-attention require to consider multiple attention maps in parallel in order to highlight the information that is relevant to the prediction in the model’s input. In this work, we propose the first method to explain prediction by any Transformer-based architecture, including bi-modal Transformers and Transformers with co-attentions. We provide generic solutions and apply these to the three most commonly used of these architectures: (i) pure self-attention, (ii) self-attention combined with co-attention, and (iii) encoder-decoder attention. We show that our method is superior to all existing methods which are adapted from single modality explainability. Our code is available at: https://github.com/hila-chefer/Transformer-MM-Explainability. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers\" satisfies the requirements of the user query. The query specifically asks for work that extends methods for deriving explanation relevancy for transformer models to multimodal transformers. The abstract of the paper explicitly states that it proposes the first method to explain predictions by any Transformer-based architecture, including bi-modal Transformers and Transformers with co-attentions. This directly addresses the multimodal aspect of transformers as required by the query. Furthermore, the paper claims to provide generic solutions applicable to various architectures, including those with co-attention mechanisms, which are commonly used in multimodal tasks. Therefore, the paper aligns with the user's query and fulfills the detailed requirements."}
{"input": "User Query: What paper proposed the latent variation predictability (VP) metric based on the assumptions of well disentangled representation?\nTitle: Learning Disentangled Representations with Latent Variation Predictability\nAbstract:  Abstract Latent traversal is a popular approach to visualize the disentangled latent representations. Given a bunch of variations in a single unit of the latent representation, it is expected that there is a change in a single factor of variation of the data while others are fixed. However, this impressive experimental observation is rarely explicitly encoded in the objective function of learning disentangled representations. This paper defines the variation predictability of latent disentangled representations. Given image pairs generated by latent codes varying in a single dimension, this varied dimension could be closely correlated with these image pairs if the representation is well disentangled. Within an adversarial generation process, we encourage variation predictability by maximizing the mutual information between latent variations and corresponding image pairs. We further develop an evaluation metric that does not rely on the ground-truth generative factors to measure the disentanglement of latent representations. The proposed variation predictability is a general constraint that is applicable to the VAE and GAN frameworks for boosting disentanglement of latent representations. Experiments show that the proposed variation predictability correlates well with existing ground-truth-required metrics and the proposed algorithm is effective for disentanglement learning.  ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"Learning Disentangled Representations with Latent Variation Predictability\"* explicitly proposes the latent variation predictability (VP) metric. The abstract describes that the paper defines the concept of variation predictability in the context of disentangled latent representations. It explains that the metric is based on the assumption that well-disentangled representations should exhibit predictable variations in latent codes corresponding to changes in specific factors of variation in the data. Furthermore, the abstract mentions that the paper develops an evaluation metric (VP) that does not rely on ground-truth generative factors, aligning with the user's query. Therefore, the paper fully satisfies the detailed requirements of the user query."}
{"input": "User Query: What papers show the use of large-scale image-text pairwise datasets or arbitrarily interleaved visual and textual data to pre-train VLMs?\nTitle: Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text\nAbstract:  Abstract In-context vision and language models like Flamingo [2] support arbitrarily interleaved sequences of images and text as input. This format not only enables few-shot learning via interleaving independent supervised (image, text) examples, but also, more complex prompts involving interaction between images, e.g., “What do image A and image B have in common?” To support this interface, pretraining occurs over web corpora that similarly contain interleaved images+text. To date, however, large-scale data of this form have not been publicly available. We release Multimodal C4 (mmc4), an augmentation of the popular text-only c4 corpus111https://www.tensorflow.org/datasets/catalog/c4 with images interleaved. We use a linear assignment algorithm to place images into longer bodies of text using CLIP features [24], a process that we show outperforms alternatives. mmc4 spans everyday topics like cooking, travel, technology, etc. A manual inspection of a random sample of documents shows that a vast majority (88%) of images are topically relevant, and that linear assignment frequently selects individual sentences specifically well-aligned with each image (80%). After filtering NSFW images, ads, etc., the resulting mmc4 corpus consists of 101.2M documents with 571M images interleaved in 43B English tokens. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text\" satisfies the requirements of the user query. The paper explicitly discusses the creation and release of a large-scale dataset (Multimodal C4) that contains arbitrarily interleaved visual (images) and textual data. This dataset is designed to support pretraining of vision-language models (VLMs) such as Flamingo, which utilize interleaved sequences of images and text as input. The paper also highlights the use of this dataset for pretraining, aligning with the user's query about the use of large-scale image-text pairwise datasets or arbitrarily interleaved visual and textual data for pretraining VLMs."}
{"input": "User Query: Which papers proposed advanced variance-reduction techniques in improving the sample complexity of solving finite-sum coupled compositional optimization problems?\nTitle: Multi-block-Single-probe Variance Reduced Estimator for Coupled Compositional Optimization\nAbstract:  Abstract Variance reduction techniques such as SPIDER/SARAH/STORM have been extensively studied to improve the convergence rates of stochastic non-convex optimization, which usually maintain and update a sequence of estimators for a single function across iterations. What if we need to track multiple functional mappings across iterations but only with access to stochastic samples of 𝒪​(1)𝒪1\\mathcal{O}(1) functional mappings at each iteration? There is an important application in solving an emerging family of coupled compositional optimization problems in the form of ∑i=1mfi​(gi​(𝐰))superscriptsubscript𝑖1𝑚subscript𝑓𝑖subscript𝑔𝑖𝐰\\sum_{i=1}^{m}f_{i}(g_{i}(\\mathbf{w})), where gisubscript𝑔𝑖g_{i} is accessible through a stochastic oracle. The key issue is to track and estimate a sequence of 𝐠​(𝐰)=(g1​(𝐰),…,gm​(𝐰))𝐠𝐰subscript𝑔1𝐰…subscript𝑔𝑚𝐰\\mathbf{g}(\\mathbf{w})=(g_{1}(\\mathbf{w}),\\ldots,g_{m}(\\mathbf{w})) across iterations, where 𝐠​(𝐰)𝐠𝐰\\mathbf{g}(\\mathbf{w}) has m𝑚m blocks and it is only allowed to probe 𝒪​(1)𝒪1\\mathcal{O}(1) blocks to attain their stochastic values and Jacobians. To improve the complexity for solving these problems, we propose a novel stochastic method named Multi-block-Single-probe Variance Reduced (MSVR) estimator to track the sequence of 𝐠​(𝐰)𝐠𝐰\\mathbf{g}(\\mathbf{w}). It is inspired by STORM but introduces a customized error correction term to alleviate the noise not only in stochastic samples for the selected blocks but also in those blocks that are not sampled. With the help of the MSVR estimator, we develop several algorithms for solving the aforementioned compositional problems with improved complexities across a spectrum of settings with non-convex/convex/strongly convex/Polyak-Łojasiewicz (PL) objectives. Our results improve upon prior ones in several aspects, including the order of sample complexities and dependence on the strong convexity parameter. Empirical studies on multi-task deep AUC maximization demonstrate the better performance of using the new estimator. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Multi-block-Single-probe Variance Reduced Estimator for Coupled Compositional Optimization\" fully satisfies the requirements of the user query. The paper explicitly addresses the development of advanced variance-reduction techniques, namely the Multi-block-Single-probe Variance Reduced (MSVR) estimator, which is designed to improve the sample complexity for solving finite-sum coupled compositional optimization problems. The abstract highlights that the proposed method is inspired by existing variance-reduction techniques (e.g., STORM) and introduces a customized error correction term to handle noise in both sampled and unsampled blocks. Furthermore, the paper demonstrates improved sample complexities across various settings (non-convex, convex, strongly convex, PL objectives) and provides empirical validation. Thus, it directly aligns with the user query's focus on advanced variance-reduction techniques and their application to finite-sum coupled compositional optimization problems."}
{"input": "User Query: Can you provide the works that use Causal-based methods to strengthen the invariant causal patterns?\nTitle: Discovering Invariant Rationales for Graph Neural Networks\nAbstract:  Abstract Intrinsic interpretability of graph neural networks (GNNs) is to find a small subset of the input graph’s features — rationale — which guides the model prediction. Unfortunately, the leading rationalization models often rely on data biases, especially shortcut features, to compose rationales and make predictions without probing the critical and causal patterns. Moreover, such data biases easily change outside the training distribution. As a result, these models suffer from a huge drop in interpretability and predictive performance on out-of-distribution data. In this work, we propose a new strategy of discovering invariant rationale (DIR) to construct intrinsically interpretable GNNs. It conducts interventions on the training distribution to create multiple interventional distributions. Then it approaches the causal rationales that are invariant across different distributions while filtering out the spurious patterns that are unstable. Experiments on both synthetic and real-world datasets validate the superiority of our DIR in terms of interpretability and generalization ability on graph classification over the leading baselines. Code and datasets are available at https://github.com/Wuyxin/DIR-GNN. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Discovering Invariant Rationales for Graph Neural Networks\" satisfies the user query as it explicitly uses causal-based methods to strengthen invariant causal patterns. The proposed strategy, DIR (Discovering Invariant Rationale), employs causal interventions on the training distribution to create multiple interventional distributions. By doing so, it identifies causal rationales that remain invariant across these distributions while filtering out spurious patterns. This aligns directly with the user query, as the work focuses on leveraging causal-based methods to enhance invariant causal patterns in the context of graph neural networks."}
{"input": "User Query: What papers study the capture of human dynamics by tracking individuals in videos using pre-scanned rigged templates?\nTitle: LiveCap: Real-time Human Performance Capture from Monocular Video\nAbstract:  Abstract. We present the first real-time human performance capture approach that reconstructs dense, space-time coherent deforming geometry of entire humans in general everyday clothing from just a single RGB video. We propose a novel two-stage analysis-by-synthesis optimization whose formulation and implementation are designed for high performance. In the first stage, a skinned template model is jointly fitted to background subtracted input video, 2D and 3D skeleton joint positions found using a deep neural network, and a set of sparse facial landmark detections. In the second stage, dense non-rigid 3D deformations of skin and even loose apparel are captured based on a novel real-time capable algorithm for non-rigid tracking using dense photometric and silhouette constraints. Our novel energy formulation leverages automatically identified material regions on the template to model the differing non-rigid deformation behavior of skin and apparel. The two resulting non-linear optimization problems per-frame are solved with specially-tailored data-parallel Gauss-Newton solvers. In order to achieve real-time performance of over 25Hz, we design a pipelined parallel architecture using the CPU and two commodity GPUs. Our method is the first real-time monocular approach for full-body performance capture. Our method yields comparable accuracy with off-line performance capture techniques, while being orders of magnitude faster. ", "output": "**Decision:** True  \n**Reason:** The paper \"LiveCap: Real-time Human Performance Capture from Monocular Video\" satisfies the requirements of the user query. The paper explicitly discusses capturing human dynamics by tracking individuals in videos using a pre-scanned rigged template model. Specifically, it mentions fitting a skinned template model (a rigged template) to input video data, including 2D and 3D skeleton joint positions, facial landmarks, and dense non-rigid deformations. This aligns with the query's focus on using pre-scanned rigged templates to track human dynamics in videos. Additionally, the method captures the full-body performance and dynamics of individuals, fulfilling the requirement of studying human dynamics in video tracking."}
{"input": "User Query: Could you provide me some works on empirical defenses to adversarial attacks on neural networks?\nTitle: Make Some Noise: Reliable and Efficient Single-Step Adversarial Training\nAbstract:  Abstract Recently, Wong et al. [35] showed that adversarial training with single-step FGSM leads to a characteristic failure mode named Catastrophic Overfitting (CO), in which a model becomes suddenly vulnerable to multi-step attacks. Experimentally they showed that simply adding a random perturbation prior to FGSM (RS-FGSM) could prevent CO. However, Andriushchenko and Flammarion [1] observed that RS-FGSM still leads to CO for larger perturbations, and proposed a computationally expensive regularizer (GradAlign) to avoid it. In this work, we methodically revisit the role of noise and clipping in single-step adversarial training. Contrary to previous intuitions, we find that using a stronger noise around the clean sample combined with not clipping is highly effective in avoiding CO for large perturbation radii. We then propose Noise-FGSM (N-FGSM) that, while providing the benefits of single-step adversarial training, does not suffer from CO. Empirical analyses on a large suite of experiments show that N-FGSM is able to match or surpass the performance of previous state-of-the-art GradAlign, while achieving 3×\\times speed-up. Code can be found in https://github.com/pdejorge/N-FGSM ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Make Some Noise: Reliable and Efficient Single-Step Adversarial Training\" satisfies the user query as it focuses on empirical defenses to adversarial attacks on neural networks. Specifically, the paper addresses the problem of Catastrophic Overfitting (CO) in single-step adversarial training and proposes a novel method called Noise-FGSM to mitigate this issue. The approach is empirically validated through experiments, demonstrating its effectiveness in defending against adversarial attacks. Additionally, the paper compares its method to prior state-of-the-art techniques and highlights its efficiency and reliability. Therefore, it aligns well with the user's request for works on empirical defenses to adversarial attacks."}
{"input": "User Query: Which data sets have been used for the development of powerful multi-modal models?\nTitle: WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning\nAbstract:  Abstract. The milestone improvements brought about by deep representation learning and pre-training techniques have led to large performance gains across downstream NLP, IR and Vision tasks. Multimodal modeling techniques aim to leverage large high-quality visio-linguistic datasets for learning complementary information (across image and text modalities). In this paper, we introduce the Wikipedia-based Image Text (WIT) Dataset111https://github.com/google-research-datasets/wit to better facilitate multimodal, multilingual learning. WIT is composed of a curated set of 37.6 million entity rich image-text examples with 11.5 million unique images across 108 Wikipedia languages. Its size enables WIT to be used as a pretraining dataset for multimodal models, as we show when applied to downstream tasks such as image-text retrieval. WIT has four main and unique advantages. First, WIT is the largest multimodal dataset by the number of image-text examples by 3x (at the time of writing). Second, WIT is massively multilingual (first of its kind) with coverage over 100+ languages (each of which has at least 12K examples) and provides cross-lingual texts for many images. Third, WIT represents a more diverse set of concepts and real world entities relative to what previous datasets cover. Lastly, WIT provides a very challenging real-world test set, as we empirically illustrate using an image-text retrieval task as an example. ", "output": "**Decision:** True  \n**Reason:** The paper introduces the WIT (Wikipedia-based Image Text) dataset, which is explicitly designed for multimodal machine learning. It provides detailed information about the dataset, including its size, multilingual nature, diversity, and utility for pretraining multimodal models. The paper also highlights the use of WIT for downstream tasks like image-text retrieval, demonstrating its relevance to the development of powerful multimodal models. Therefore, it satisfies the user query by discussing a specific dataset used in the development of multimodal models."}
{"input": "User Query: Could you provide me some studies about VLN data generation strategies?\nTitle: Airbert: In-domain Pretraining for Vision-and-Language Navigation\nAbstract:  Abstract Vision-and-language navigation (VLN) aims to enable embodied agents to navigate in realistic environments using natural language instructions. Given the scarcity of domain-specific training data and the high diversity of image and language inputs, the generalization of VLN agents to unseen environments remains challenging. Recent methods explore pretraining to improve generalization, however, the use of generic image-caption datasets or existing small-scale VLN environments is suboptimal and results in limited improvements. In this work, we introduce BnB111Bed and Breakfast, a large-scale and diverse in-domain VLN dataset. We first collect image-caption (IC) pairs from hundreds of thousands of listings from online rental marketplaces. Using IC pairs we next propose automatic strategies to generate millions of VLN path-instruction (PI) pairs. We further propose a shuffling loss that improves the learning of temporal order inside PI pairs. We use BnB to pretrain our Airbert222 Airbert is an Old Irish word meaning practice, here referring to model pretraining on pretext tasks similar to VLN. model that can be adapted to discriminative and generative settings and show that it outperforms state of the art for Room-to-Room (R2R) navigation and Remote Referring Expression (REVERIE) benchmarks. Moreover, our in-domain pretraining significantly increases performance on a challenging few-shot VLN evaluation, where we train the model only on VLN instructions from a few houses. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Airbert: In-domain Pretraining for Vision-and-Language Navigation\" satisfies the user query as it explicitly discusses strategies for generating VLN (Vision-and-Language Navigation) data. The authors introduce a large-scale in-domain VLN dataset called BnB, which is created by collecting image-caption pairs from online rental marketplaces and then employing automatic strategies to generate millions of VLN path-instruction pairs. These data generation strategies are central to the paper's methodology and contribute to improving VLN model performance. Therefore, the paper is relevant to the user's request for studies about VLN data generation strategies."}
{"input": "User Query: Which paper proposes to finetune StyleGAN using around 100 face images to obtain a personalized generative prior?\nTitle: MyStyle: A Personalized Generative Prior\nAbstract:  Abstract. We introduce MyStyle, a personalized deep generative prior trained with a few shots of an individual. MyStyle allows to reconstruct, enhance and edit images of a specific person, such that the output is faithful to the person’s key facial characteristics. Given a small reference set of portrait images of a person (∼100similar-toabsent100\\sim 100), we tune the weights of a pretrained StyleGAN face generator to form a local, low-dimensional, personalized manifold in the latent space. We show that this manifold constitutes a personalized region that spans latent codes associated with diverse portrait images of the individual. Moreover, we demonstrate that we obtain a personalized generative prior, and propose a unified approach to apply it to various ill-posed image enhancement problems, such as inpainting and super-resolution, as well as semantic editing. Using the personalized generative prior we obtain outputs that exhibit high-fidelity to the input images and are also faithful to the key facial characteristics of the individual in the reference set. We demonstrate our method with fair-use images of numerous widely recognizable individuals for whom we have the prior knowledge for a qualitative evaluation of the expected outcome. We evaluate our approach against few-shots baselines and show that our personalized prior, quantitatively and qualitatively, outperforms state-of-the-art alternatives. ", "output": "**Decision:** True  \n**Reason:** The searched paper, titled *\"MyStyle: A Personalized Generative Prior,\"* explicitly addresses the requirements of the user query. The abstract clearly states that the method involves fine-tuning a pretrained StyleGAN face generator using a small reference set of approximately 100 portrait images of an individual. This fine-tuning process creates a personalized generative prior, which is used for various applications such as image reconstruction, enhancement, and editing. The description aligns perfectly with the query's focus on fine-tuning StyleGAN with around 100 face images to obtain a personalized generative prior. Hence, the paper fully satisfies the detailed requirements of the user query."}
{"input": "User Query: Could you recommend studies that explore proficiency of LLMs such as ChatGPT in law exams and technical aspects of GPT-4?\nTitle: GPT-4 Technical Report\nAbstract:  Abstract We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4’s performance based on models trained with no more than 1/1,000th the compute of GPT-4. ", "output": "**Decision:** True  \n**Reason:** The searched paper, \"GPT-4 Technical Report,\" satisfies the user query as it addresses both aspects of the request. First, it explicitly mentions GPT-4's performance on a simulated bar exam, achieving a score in the top 10% of test takers, which directly relates to the proficiency of large language models (LLMs) like ChatGPT in law exams. Second, the paper provides technical details about GPT-4, including its Transformer-based architecture, pre-training methodology, alignment processes, and infrastructure/optimization methods. These details align with the user's interest in the technical aspects of GPT-4. Thus, the paper fully meets the requirements of the query."}
{"input": "User Query: What works proposed to jointly learn monocular depth, ego-motion, and residual flow field?\nTitle: Unsupervised Monocular Depth Learning in Dynamic Scenes\nAbstract:  Abstract We present a method for jointly training the estimation of depth, ego-motion, and a dense 3D translation field of objects relative to the scene, with monocular photometric consistency being the sole source of supervision. We show that this apparently heavily underdetermined problem can be regularized by imposing the following prior knowledge about 3D translation fields: they are sparse, since most of the scene is static, and they tend to be piecewise constant for rigid moving objects. We show that this regularization alone is sufficient to train monocular depth prediction models that exceed the accuracy achieved in prior work for dynamic scenes, including methods that require semantic input. 111Code is available at github.com/google-research/google-research/tree/master/depth_and_motion_learning ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Unsupervised Monocular Depth Learning in Dynamic Scenes\" satisfies the requirements of the user query. The paper explicitly proposes a method to jointly learn monocular depth, ego-motion, and a dense 3D translation field (which can be interpreted as a residual flow field). The method uses monocular photometric consistency as supervision and incorporates regularization techniques to handle the underdetermined nature of the problem. The focus on jointly learning these components aligns with the query's interest in works that address monocular depth, ego-motion, and residual flow field estimation."}
{"input": "User Query: Which works looked at the relationship between dynamical systems and machine learning?\nTitle: PDE-Net: Learning PDEs from Data\nAbstract:  Abstract Partial differential equations (PDEs) play a prominent role in many disciplines such as applied mathematics, physics, chemistry, material science, computer science, etc. PDEs are commonly derived based on physical laws or empirical observations. However, the governing equations for many complex systems in modern applications are still not fully known. With the rapid development of sensors, computational power, and data storage in the past decade, huge quantities of data can be easily collected and efficiently stored. Such vast quantity of data offers new opportunities for data-driven discovery of hidden physical laws. Inspired by the latest development of neural network designs in deep learning, we propose a new feed-forward deep network, called PDE-Net, to fulfill two objectives at the same time: to accurately predict dynamics of complex systems and to uncover the underlying hidden PDE models. The basic idea of the proposed PDE-Net is to learn differential operators by learning convolution kernels (filters), and apply neural networks or other machine learning methods to approximate the unknown nonlinear responses. Comparing with existing approaches, which either assume the form of the nonlinear response is known or fix certain finite difference approximations of differential operators, our approach has the most flexibility by learning both differential operators and the nonlinear responses. A special feature of the proposed PDE-Net is that all filters are properly constrained, which enables us to easily identify the governing PDE models while still maintaining the expressive and predictive power of the network. These constrains are carefully designed by fully exploiting the relation between the orders of differential operators and the orders of sum rules of filters (an important concept originated from wavelet theory). We also discuss relations of the PDE-Net with some existing networks in computer vision such as Network-In-Network (NIN) and Residual Neural Network (ResNet). Numerical experiments show that the PDE-Net has the potential to uncover the hidden PDE of the observed dynamics, and predict the dynamical behavior for a relatively long time, even in a noisy environment. ", "output": "**Decision:** True  \n**Reason:** The paper \"PDE-Net: Learning PDEs from Data\" satisfies the user query as it explicitly explores the relationship between dynamical systems and machine learning. Partial differential equations (PDEs) are fundamental tools for modeling dynamical systems in various scientific disciplines. The paper proposes a machine learning framework, PDE-Net, which is designed to predict the dynamics of complex systems and uncover the underlying governing PDEs from data. This approach directly connects machine learning (through neural networks and data-driven methods) with the study of dynamical systems (modeled by PDEs). Additionally, the paper discusses how neural network architectures can be adapted to learn differential operators and nonlinear responses, further emphasizing the interplay between machine learning techniques and dynamical systems."}
{"input": "User Query: Which studies discuss about measures to robustify deep neural networks?\nTitle: Randomized Smoothing of All Shapes and Sizes\nAbstract:  Abstract Randomized smoothing is the current state-of-the-art defense with provable robustness against ℓ2subscriptℓ2\\ell_{2} adversarial attacks. Many works have devised new randomized smoothing schemes for other metrics, such as ℓ1subscriptℓ1\\ell_{1} or ℓ∞subscriptℓ\\ell_{\\infty}; however, substantial effort was needed to derive such new guarantees. This begs the question: can we find a general theory for randomized smoothing? We propose a novel framework for devising and analyzing randomized smoothing schemes, and validate its effectiveness in practice. Our theoretical contributions are: (1) we show that for an appropriate notion of “optimal”, the optimal smoothing distributions for any “nice” norms have level sets given by the norm’s Wulff Crystal; (2) we propose two novel and complementary methods for deriving provably robust radii for any smoothing distribution; and, (3) we show fundamental limits to current randomized smoothing techniques via the theory of Banach space cotypes. By combining (1) and (2), we significantly improve the state-of-the-art certified accuracy in ℓ1subscriptℓ1\\ell_{1} on standard datasets. Meanwhile, we show using (3) that with only label statistics under random input perturbations, randomized smoothing cannot achieve nontrivial certified accuracy against perturbations of ℓpsubscriptℓ𝑝\\ell_{p}-norm Ω​(min⁡(1,d1p−12))Ω1superscript𝑑1𝑝12\\Omega(\\min(1,d^{\\frac{1}{p}-\\frac{1}{2}})), when the input dimension d𝑑d is large. We provide code in github.com/tonyduan/rs4a. ", "output": "**Decision:** True  \n**Reason:** The paper \"Randomized Smoothing of All Shapes and Sizes\" discusses measures to robustify deep neural networks by focusing on randomized smoothing, a state-of-the-art defense mechanism with provable robustness against adversarial attacks. The study proposes a novel framework for devising and analyzing randomized smoothing schemes, which directly contributes to enhancing the robustness of deep neural networks. Additionally, the paper provides theoretical contributions, practical methods, and improvements in certified accuracy, all of which align with the user's query about measures to robustify deep neural networks."}
{"input": "User Query: Could you mention some papers that learn high-level features in 3D spaces for rapid zero-shot style transfer?\nTitle: StyleRF: Zero-shot 3D Style Transfer of Neural Radiance Fields\nAbstract:  Abstract 3D style transfer aims to render stylized novel views of a 3D scene with multi-view consistency. However, most existing work suffers from a three-way dilemma over accurate geometry reconstruction, high-quality stylization, and being generalizable to arbitrary new styles. We propose StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field. StyleRF employs an explicit grid of high-level features to represent 3D scenes, with which high-fidelity geometry can be reliably restored via volume rendering. In addition, it transforms the grid features according to the reference style which directly leads to high-quality zero-shot style transfer. StyleRF consists of two innovative designs. The first is sampling-invariant content transformation that makes the transformation invariant to the holistic statistics of the sampled 3D points and accordingly ensures multi-view consistency. The second is deferred style transformation of 2D feature maps which is equivalent to the transformation of 3D points but greatly reduces memory footprint without degrading multi-view consistency. Extensive experiments show that StyleRF achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner. Project website: https://kunhao-liu.github.io/StyleRF/ ", "output": "**Decision:** True  \n**Reason:** The paper \"StyleRF: Zero-shot 3D Style Transfer of Neural Radiance Fields\" fully satisfies the requirements of the user query. It explicitly focuses on learning high-level features in 3D spaces, as it employs an explicit grid of high-level features to represent 3D scenes. This approach allows for high-fidelity geometry reconstruction and enables zero-shot style transfer. Additionally, the paper emphasizes rapid style transfer by utilizing innovative techniques such as sampling-invariant content transformation and deferred style transformation, which ensure efficiency and multi-view consistency. The zero-shot generalization capability to various styles directly aligns with the user's query for rapid zero-shot style transfer in 3D spaces."}
{"input": "User Query: Could you provide me some studies that focus on question answering tasks on 3D data?\nTitle: 3D Question Answering\nAbstract:  Abstract Visual question answering (VQA) has experienced tremendous progress in recent years. However, most efforts have only focused on 2D image question-answering tasks. In this paper, we extend VQA to its 3D counterpart, 3D question answering (3DQA), which can facilitate a machine’s perception of 3D real-world scenarios. Unlike 2D image VQA, 3DQA takes the color point cloud as input and requires both appearance and 3D geometrical comprehension to answer the 3D-related questions. To this end, we propose a novel transformer-based 3DQA framework “3DQA-TR”, which consists of two encoders to exploit the appearance and geometry information, respectively. Finally, the multi-modal information about the appearance, geometry, and linguistic question can attend to each other via a 3D-linguistic Bert to predict the target answers. To verify the effectiveness of our proposed 3DQA framework, we further develop the first 3DQA dataset “ScanQA”, which builds on the ScanNet dataset and contains over 10K question-answer pairs for 806806806 scenes. To the best of our knowledge, ScanQA is the first large-scale dataset with natural-language questions and free-form answers in 3D environments that is fully human-annotated. We also use several visualizations and experiments to investigate the astonishing diversity of the collected questions and the significant differences between this task from 2D VQA and 3D captioning. Extensive experiments on this dataset demonstrate the obvious superiority of our proposed 3DQA framework over state-of-the-art VQA frameworks and the effectiveness of our major designs. Our code and dataset will be made publicly available to facilitate research in this direction. The code and data are available at http://shuquanye.com/3DQA_website/. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"3D Question Answering\" fully satisfies the user query. The study explicitly focuses on question answering tasks on 3D data, as it introduces the concept of 3D question answering (3DQA) and proposes a novel framework for addressing this task. The paper uses color point clouds as input, requiring both appearance and 3D geometrical comprehension to answer 3D-related questions. Additionally, it develops a new dataset, \"ScanQA,\" specifically tailored for 3D question answering tasks. This directly aligns with the user's request for studies focusing on question answering tasks on 3D data."}
{"input": "User Query: Which works address the long training time of the original NeRF formulation?\nTitle: Plenoxels: Radiance Fields without Neural Networks\nAbstract:  Abstract We introduce Plenoxels (plenoptic voxels), a system for photorealistic view synthesis. Plenoxels represent a scene as a sparse 3D grid with spherical harmonics. This representation can be optimized from calibrated images via gradient methods and regularization without any neural components. On standard, benchmark tasks, Plenoxels are optimized two orders of magnitude faster than Neural Radiance Fields with no loss in visual quality. For video and code, please see https://alexyu.net/plenoxels. ", "output": "**Decision:** True  \n**Reason:** The paper \"Plenoxels: Radiance Fields without Neural Networks\" directly addresses the long training time of the original NeRF formulation. The abstract explicitly states that Plenoxels optimize the scene representation two orders of magnitude faster than Neural Radiance Fields (NeRF) while maintaining comparable visual quality. This aligns with the user query, which seeks works that address the long training time of the original NeRF formulation."}
{"input": "User Query: Could you provide me some studies that proposed Variational AutoEncoders (VAEs) variants to disentangle representation learning?\nTitle: Disentangling by Factorising\nAbstract:  Abstract We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon β𝛽\\beta-VAE by providing a better trade-off between disentanglement and reconstruction quality. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Disentangling by Factorising\" fully satisfies the user query. The study proposes a variant of Variational AutoEncoders (VAEs) called FactorVAE, which is specifically designed for disentangled representation learning. The method improves upon the β-VAE by encouraging the distribution of representations to be factorial and independent across dimensions, directly addressing the disentanglement of representations. Additionally, the paper introduces a new disentanglement metric, further contributing to the field of disentangled representation learning. Thus, it aligns with the user's request for studies on VAE variants for disentanglement."}
{"input": "User Query: What works have shown success with attention-based aggregation in point-based networks?\nTitle: PCAN: 3D Attention Map Learning Using Contextual Information for Point Cloud Based Retrieval\nAbstract:  Abstract Point cloud based retrieval for place recognition is an emerging problem in vision field. The main challenge is how to find an efficient way to encode the local features into a discriminative global descriptor. In this paper, we propose a Point Contextual Attention Network (PCAN), which can predict the significance of each local point feature based on point context. Our network makes it possible to pay more attention to the task-relevent features when aggregating local features. Experiments on various benchmark datasets show that the proposed network can provide outperformance than current state-of-the-art approaches. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"PCAN: 3D Attention Map Learning Using Contextual Information for Point Cloud Based Retrieval\" satisfies the user query. The query specifically asks for works that have shown success with attention-based aggregation in point-based networks. The abstract of the paper explicitly mentions the use of a Point Contextual Attention Network (PCAN), which predicts the significance of each local point feature based on context and uses this attention mechanism to aggregate local features into a global descriptor. Additionally, the paper reports success in experiments on benchmark datasets, indicating that the proposed attention-based aggregation method has been effective. Thus, the paper aligns with the requirements of the user query."}
{"input": "User Query: What papers propose aligning the task losses magnitudes by rescaling them based on the task uncertainty?\nTitle: Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics\nAbstract: Numerous deep learning applications benefit from multi-task learning with multiple regression and classification objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task's loss. Tuning these weights by hand is a difficult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics\"* fully satisfies the requirements of the user query. The paper explicitly proposes a method to align the magnitudes of task losses by rescaling them based on the homoscedastic uncertainty of each task. This aligns directly with the user's query, as the method leverages task uncertainty to weigh and balance the loss functions in a principled manner. The abstract also highlights that this approach eliminates the need for manual tuning of task loss weights, which is a key challenge in multi-task learning. Therefore, the paper addresses the concept of rescaling task losses based on task uncertainty as requested in the query."}
{"input": "User Query: Which works are the pioneers of bit allocation for Neural Video Compression (NVC)?\nTitle: Learned Video Compression\nAbstract:  Abstract We present a new algorithm for video coding, learned end-to-end for the low-latency mode. In this setting, our approach outperforms all existing video codecs across nearly the entire bitrate range. To our knowledge, this is the first ML-based method to do so. We evaluate our approach on standard video compression test sets of varying resolutions, and benchmark against all mainstream commercial codecs, in the low-latency mode. On standard-definition videos, relative to our algorithm, HEVC/H.265, AVC/H.264 and VP9 typically produce codes up to 60% larger. On high-definition 1080p videos, H.265 and VP9 typically produce codes up to 20% larger, and H.264 up to 35% larger. Furthermore, our approach does not suffer from blocking artifacts and pixelation, and thus produces videos that are more visually pleasing. We propose two main contributions. The first is a novel architecture for video compression, which (1) generalizes motion estimation to perform any learned compensation beyond simple translations, (2) rather than strictly relying on previously transmitted reference frames, maintains a state of arbitrary information learned by the model, and (3) enables jointly compressing all transmitted signals (such as optical flow and residual). Secondly, we present a framework for ML-based spatial rate control — a mechanism for assigning variable bitrates across space for each frame. This is a critical component for video coding, which to our knowledge had not been developed within a machine learning setting. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Learned Video Compression\" satisfies the user query as it explicitly discusses a framework for ML-based spatial rate control, which is a mechanism for assigning variable bitrates across space for each frame. This directly relates to bit allocation in Neural Video Compression (NVC). The paper claims that this is a critical component for video coding and, to their knowledge, had not been developed within a machine learning setting before. Therefore, the paper can be considered a pioneering work in the context of bit allocation for NVC."}
{"input": "User Query: In what papers the researchers develop direct Audio-Visual Speech-to-Speech Translation (AV2A)?\nTitle: AV-TranSpeech: Audio-Visual Robust Speech-to-Speech Translation\nAbstract:  Abstract Direct speech-to-speech translation (S2ST) aims to convert speech from one language into another, and has demonstrated significant progress to date. Despite the recent success, current S2ST models still suffer from distinct degradation in noisy environments and fail to translate visual speech (i.e., the movement of lips and teeth). In this work, we present AV-TranSpeech, the first audio-visual speech-to-speech (AV-S2ST) translation model without relying on intermediate text. AV-TranSpeech complements the audio stream with visual information to promote system robustness and opens up a host of practical applications: dictation or dubbing archival films. To mitigate the data scarcity with limited parallel AV-S2ST data, we 1) explore self-supervised pre-training with unlabeled audio-visual data to learn contextual representation, and 2) introduce cross-modal distillation with S2ST models trained on the audio-only corpus to further reduce the requirements of visual data. Experimental results on two language pairs demonstrate that AV-TranSpeech outperforms audio-only models under all settings regardless of the type of noise. With low-resource audio-visual data (10h, 30h), cross-modal distillation yields an improvement of 7.6 BLEU on average compared with baselines.111Audio samples are available at https://AV-TranSpeech.github.io/. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"AV-TranSpeech: Audio-Visual Robust Speech-to-Speech Translation\" fully satisfies the user query. The researchers in this paper explicitly develop a direct Audio-Visual Speech-to-Speech Translation (AV2A) model, named AV-TranSpeech. The abstract clearly states that the model translates speech from one language to another without relying on intermediate text, which aligns with the requirement of \"direct\" translation. Furthermore, the paper emphasizes the incorporation of visual speech information (e.g., lip and teeth movements) alongside audio to enhance robustness, fulfilling the \"Audio-Visual\" aspect of the query. Therefore, the paper directly addresses the development of an AV2A system."}
{"input": "User Query: What works contributed to the research about learning parameter initializations that quickly adapt to downstream tasks?\nTitle: Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\nAbstract:  Abstract We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\" directly addresses the research topic of learning parameter initializations that quickly adapt to downstream tasks. The abstract explicitly states that the proposed approach trains model parameters such that a small number of gradient steps with limited training data from a new task lead to good generalization performance. This aligns with the user's query about works contributing to learning parameter initializations for fast adaptation. Furthermore, the paper demonstrates the effectiveness of this method across various tasks, including classification, regression, and reinforcement learning, which further supports its relevance to the query."}
{"input": "User Query: Which papers provide details about methods similar to EFT and CLIFF, which update the human mesh recovery estimation network weight with 2D reprojection loss?\nTitle: CLIFF: Carrying Location Information in Full Frames into Human Pose and Shape Estimation\nAbstract:  Abstract Top-down methods dominate the field of 3D human pose and shape estimation, because they are decoupled from human detection and allow researchers to focus on the core problem. However, cropping, their first step, discards the location information from the very beginning, which makes themselves unable to accurately predict the global rotation in the original camera coordinate system. To address this problem, we propose to Carry Location Information in Full Frames (CLIFF) into this task. Specifically, we feed more holistic features to CLIFF by concatenating the cropped-image feature with its bounding box information. We calculate the 2D reprojection loss with a broader view of the full frame, taking a projection process similar to that of the person projected in the image. Fed and supervised by global-location-aware information, CLIFF directly predicts the global rotation along with more accurate articulated poses. Besides, we propose a pseudo-ground-truth annotator based on CLIFF, which provides high-quality 3D annotations for in-the-wild 2D datasets and offers crucial full supervision for regression-based methods. Extensive experiments on popular benchmarks show that CLIFF outperforms prior arts by a significant margin, and reaches the first place on the AGORA leaderboard (the SMPL-Algorithms track). The code and data are available at https://github.com/huawei-noah/noah-research/tree/master/CLIFF. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"CLIFF: Carrying Location Information in Full Frames into Human Pose and Shape Estimation\" satisfies the user's query. The paper explicitly describes a method (CLIFF) that updates the human mesh recovery estimation network weights using a 2D reprojection loss. This is evident from the abstract, where it mentions that CLIFF calculates the 2D reprojection loss with a broader view of the full frame, taking into account the projection process of the person in the image. This aligns with the user's request for methods similar to EFT and CLIFF that utilize 2D reprojection loss for updating network weights. Therefore, the paper fully meets the detailed requirements of the query."}
{"input": "User Query: Which works have extended 2D image generation to 3D by incorporating neural radiance field or differentiable rasterization?\nTitle: OmniAvatar: Geometry-Guided Controllable 3D Head Synthesis\nAbstract:  Abstract We present OmniAvatar, a novel geometry-guided 3D head synthesis model trained from in-the-wild unstructured images that is capable of synthesizing diverse identity-preserved 3D heads with compelling dynamic details under full disentangled control over camera poses, facial expressions, head shapes, articulated neck and jaw poses. To achieve such high level of disentangled control, we first explicitly define a novel semantic signed distance function (SDF) around a head geometry (FLAME) conditioned on the control parameters. This semantic SDF allows us to build a differentiable volumetric correspondence map from the observation space to a disentangled canonical space from all the control parameters. We then leverage the 3D-aware GAN framework (EG3D) to synthesize detailed shape and appearance of 3D full heads in the canonical space, followed by a volume rendering step guided by the volumetric correspondence map to output into the observation space. To ensure the control accuracy on the synthesized head shapes and expressions, we introduce a geometry prior loss to conform to head SDF and a control loss to conform to the expression code. Further, we enhance the temporal realism with dynamic details conditioned upon varying expressions and joint poses. Our model can synthesize more preferable identity-preserved 3D heads with compelling dynamic details compared to the state-of-the-art methods both qualitatively and quantitatively. We also provide an ablation study to justify many of our system design choices. ", "output": "**Decision:** True  \n**Reason:** The paper \"OmniAvatar: Geometry-Guided Controllable 3D Head Synthesis\" satisfies the user's query as it extends 2D image generation to 3D by incorporating neural radiance fields (via volume rendering) and differentiable techniques. Specifically, the abstract mentions the use of a 3D-aware GAN framework (EG3D), which is known to leverage neural radiance fields for 3D-aware synthesis, and a volume rendering step is explicitly described. These components align with the query's focus on extending 2D image generation to 3D using neural radiance fields or differentiable rasterization. Therefore, the paper meets the detailed requirements of the query."}
{"input": "User Query: Could you provide me some studies about the non-Euclidean embedding method in knowledge graph embedding?\nTitle: Low-Dimensional Hyperbolic Knowledge Graph Embeddings\nAbstract:  Abstract Knowledge graph (KG) embeddings learn low-dimensional representations of entities and relations to predict missing facts. KGs often exhibit hierarchical and logical patterns which must be preserved in the embedding space. For hierarchical data, hyperbolic embedding methods have shown promise for high-fidelity and parsimonious representations. However, existing hyperbolic embedding methods do not account for the rich logical patterns in KGs. In this work, we introduce a class of hyperbolic KG embedding models that simultaneously capture hierarchical and logical patterns. Our approach combines hyperbolic reflections and rotations with attention to model complex relational patterns. Experimental results on standard KG benchmarks show that our method improves over previous Euclidean- and hyperbolic-based efforts by up to 6.1% in mean reciprocal rank (MRR) in low dimensions. Furthermore, we observe that different geometric transformations capture different types of relations while attention-based transformations generalize to multiple relations. In high dimensions, our approach yields new state-of-the-art MRRs of 49.6% on WN18RR and 57.7% on YAGO3-10. ", "output": "**Decision:** True  \n**Reason:** The searched paper fully satisfies the user's query. The user is specifically asking for studies about non-Euclidean embedding methods in the context of knowledge graph embeddings. The paper titled *\"Low-Dimensional Hyperbolic Knowledge Graph Embeddings\"* directly addresses this topic by presenting a hyperbolic (non-Euclidean) embedding method for knowledge graphs. The abstract explicitly discusses how the proposed method leverages hyperbolic geometry to capture hierarchical and logical patterns in knowledge graphs, which aligns with the user's interest in non-Euclidean embedding methods. Additionally, the paper compares its method to Euclidean-based approaches, further emphasizing its focus on non-Euclidean embeddings. Thus, it is relevant and satisfies the query."}
{"input": "User Query: Which papers proposed approaches for learning individually fair representations in FRL methods?\nTitle: Learning Certified Individually Fair Representations\nAbstract:  Abstract Fair representation learning provides an effective way of enforcing fairness constraints without compromising utility for downstream users. A desirable family of such fairness constraints, each requiring similar treatment for similar individuals, is known as individual fairness. In this work, we introduce the first method that enables data consumers to obtain certificates of individual fairness for existing and new data points. The key idea is to map similar individuals to close latent representations and leverage this latent proximity to certify individual fairness. That is, our method enables the data producer to learn and certify a representation where for a data point all similar individuals are at ℓ∞subscriptℓ\\ell_{\\infty}-distance at most ϵitalic-ϵ\\epsilon, thus allowing data consumers to certify individual fairness by proving ϵitalic-ϵ\\epsilon-robustness of their classifier. Our experimental evaluation on five real-world datasets and several fairness constraints demonstrates the expressivity and scalability of our approach. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Learning Certified Individually Fair Representations\" directly addresses the task of learning individually fair representations in Fair Representation Learning (FRL) methods. The abstract explicitly states that the method introduced in the paper maps similar individuals to close latent representations, which aligns with the concept of individual fairness. Furthermore, the paper emphasizes certifying individual fairness by ensuring that similar individuals are treated similarly in the latent space. This approach satisfies the user query's requirement of proposing a method for learning individually fair representations in FRL methods."}
{"input": "User Query: Which work directly optimizes evaluation metric ROUGE in RL fine-tuning stage?\nTitle: A Deep Reinforced Model for Abstractive Summarization\nAbstract:  Abstract Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit “exposure bias” – they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries. ", "output": "**Decision:** True  \n**Reason:** The paper explicitly mentions the use of reinforcement learning (RL) as part of the training process, which combines standard supervised word prediction with global sequence prediction. The abstract highlights that this RL-based training improves the quality of summaries, and the evaluation metric mentioned is ROUGE. While the abstract does not explicitly state that ROUGE is directly optimized during the RL fine-tuning stage, it is strongly implied because the model achieves improvements in ROUGE scores, which are the primary evaluation metric for summarization tasks. Therefore, it is reasonable to conclude that the work aligns with the query's requirement of directly optimizing ROUGE in the RL fine-tuning stage."}
{"input": "User Query: Which papers show that LLMs could answer questions with explicit reasoning steps by being provided with a chain of thoughts?\nTitle: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\nAbstract:  Abstract We explore how generating a chain of thought—a series of intermediate reasoning steps—significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-of-thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier. ", "output": "**Decision:** True  \n**Reason:** The paper \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\" fully satisfies the requirements of the user query. The paper explicitly demonstrates that large language models (LLMs) can answer questions with explicit reasoning steps when provided with a chain of thought. The abstract clearly states that the method of chain-of-thought prompting involves providing a series of intermediate reasoning steps as exemplars, which significantly improves the reasoning capabilities of LLMs. The experiments conducted in the paper show that this approach enhances performance on tasks requiring arithmetic, commonsense, and symbolic reasoning, which aligns with the user's query about answering questions with explicit reasoning steps. Therefore, the paper directly addresses the user's request."}
{"input": "User Query: In knowledge distillation, which works demonstrate the student model mimicking the teacher's intermediate features?\nTitle: FitNets: Hints for Thin Deep Nets\nAbstract:  Abstract While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher’s intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network. ", "output": "**Decision:** True  \n**Reason:** The paper \"FitNets: Hints for Thin Deep Nets\" explicitly demonstrates the concept of a student model mimicking the teacher's intermediate features in the context of knowledge distillation. The abstract clearly states that the authors extend the knowledge distillation approach by using not only the outputs of the teacher network but also its intermediate representations (referred to as \"hints\") to guide the training of the student model. This aligns directly with the user query, as the work focuses on the student model learning from the intermediate features of the teacher network."}
{"input": "User Query: What research papers explored the use of structural and shape information in semantic maps to enhance the quality of images in semantic image synthesis?\nTitle: Inferring and Leveraging Parts from Object Shape for Improving Semantic Image Synthesis\nAbstract:  Abstract Despite the progress in semantic image synthesis, it remains a challenging problem to generate photo-realistic parts from input semantic map. Integrating part segmentation map can undoubtedly benefit image synthesis, but is bothersome and inconvenient to be provided by users. To improve part synthesis, this paper presents to infer Parts from Object ShapE (iPOSE) and leverage it for improving semantic image synthesis. However, albeit several part segmentation datasets are available, part annotations are still not provided for many object categories in semantic image synthesis. To circumvent it, we resort to few-shot regime to learn a PartNet for predicting the object part map with the guidance of pre-defined support part maps. PartNet can be readily generalized to handle a new object category when a small number (e.g., 333) of support part maps for this category are provided. Furthermore, part semantic modulation is presented to incorporate both inferred part map and semantic map for image synthesis. Experiments show that our iPOSE not only generates objects with rich part details, but also enables to control the image synthesis flexibly. And our iPOSE performs favorably against the state-of-the-art methods in terms of quantitative and qualitative evaluation. Our code will be publicly available at https://github.com/csyxwei/iPOSE. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Inferring and Leveraging Parts from Object Shape for Improving Semantic Image Synthesis\" satisfies the user's query. The paper explicitly explores the use of structural and shape information (object shape and inferred parts) in semantic maps to enhance the quality of images in semantic image synthesis. It introduces the iPOSE framework, which infers object parts from shape information and integrates this inferred part map with the semantic map to improve image synthesis. The use of shape-based part segmentation and its incorporation into the synthesis process aligns directly with the user's interest in leveraging structural and shape information for enhancing image quality."}
{"input": "User Query: What study developed Sparse Inertial Poser for offline joint angle reconstruction?\nTitle: Sparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse IMUs\nAbstract:  Abstract We address the problem of making human motion capture in the wild more practical by using a small set of inertial sensors attached to the body. Since the problem is heavily under-constrained, previous methods either use a large number of sensors, which is intrusive, or they require additional video input. We take a different approach and constrain the problem by: (i) making use of a realistic statistical body model that includes anthropometric constraints and (ii) using a joint optimization framework to fit the model to orientation and acceleration measurements over multiple frames. The resulting tracker Sparse Inertial Poser (SIP) enables 3D human pose estimation using only 6 sensors (attached to the wrists, lower legs, back and head) and works for arbitrary human motions. Experiments on the recently released TNT15 dataset show that, using the same number of sensors, SIP achieves higher accuracy than the dataset baseline without using any video data. We further demonstrate the effectiveness of SIP on newly recorded challenging motions in outdoor scenarios such as climbing or jumping over a wall. Computer GraphicsI.3.3Three-Dimensional Graphics and RealismAnimation ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Sparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse IMUs\" fully satisfies the user query. The paper explicitly introduces and develops the Sparse Inertial Poser (SIP) for 3D human pose estimation using sparse inertial measurement units (IMUs). It describes the methodology for offline joint angle reconstruction by leveraging a statistical body model and a joint optimization framework. The abstract also highlights that SIP achieves high accuracy in reconstructing human motion using only a small number of sensors, which aligns with the query's focus on offline joint angle reconstruction. Therefore, the paper directly addresses the study that developed Sparse Inertial Poser."}
{"input": "User Query: What studies focus on the setting where the total contamination is bound in robust MABs?\nTitle: Stochastic bandits robust to adversarial corruptions\nAbstract:  Abstract We introduce a new model of stochastic bandits with adversarial corruptions which aims to capture settings where most of the input follows a stochastic pattern but some fraction of it can be adversarially changed to trick the algorithm, e.g., click fraud, fake reviews and email spam. The goal of this model is to encourage the design of bandit algorithms that (i) work well in mixed adversarial and stochastic models, and (ii) whose performance deteriorates gracefully as we move from fully stochastic to fully adversarial models. In our model, the rewards for all arms are initially drawn from a distribution and are then altered by an adaptive adversary. We provide a simple algorithm whose performance gracefully degrades with the total corruption the adversary injected in the data, measured by the sum across rounds of the biggest alteration the adversary made in the data in that round; this total corruption is denoted by C𝐶C. Our algorithm provides a guarantee that retains the optimal guarantee (up to a logarithmic term) if the input is stochastic and whose performance degrades linearly to the amount of corruption C𝐶C, while crucially being agnostic to it. We also provide a lower bound showing that this linear degradation is necessary if the algorithm achieves optimal performance in the stochastic setting (the lower bound works even for a known amount of corruption, a special case in which our algorithm achieves optimal performance without the extra logarithm). ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Stochastic bandits robust to adversarial corruptions\"* directly addresses the setting where the total contamination (referred to as \"total corruption\" in the paper) is bounded in robust multi-armed bandits (MABs). The paper introduces a model that explicitly considers adversarial corruptions in stochastic bandits and measures the corruption using a bounded quantity \\( C \\), which represents the total alteration introduced by the adversary. The algorithm proposed in the paper is designed to perform well under such bounded contamination, with performance degrading linearly with \\( C \\). This aligns with the user query, as the study focuses on robust MABs under a bounded contamination setting."}
{"input": "User Query: What studies propose learning good similarity metrics from data to overcome the challenge of choosing distance functions?\nTitle: Two Simple Ways to Learn Individual Fairness Metrics from Data\nAbstract:  Abstract Individual fairness is an intuitive definition of algorithmic fairness that addresses some of the drawbacks of group fairness. Despite its benefits, it depends on a task specific fair metric that encodes our intuition of what is fair and unfair for the ML task at hand, and the lack of a widely accepted fair metric for many ML tasks is the main barrier to broader adoption of individual fairness. In this paper, we present two simple ways to learn fair metrics from a variety of data types. We show empirically that fair training with the learned metrics leads to improved fairness on three machine learning tasks susceptible to gender and racial biases. We also provide theoretical guarantees on the statistical performance of both approaches. ", "output": "**Decision:** True  \n**Reason:** The searched paper aligns well with the user query. The user is asking for studies that propose learning good similarity metrics from data to address the challenge of choosing distance functions. The paper titled *\"Two Simple Ways to Learn Individual Fairness Metrics from Data\"* explicitly discusses methods to learn fair metrics from data, which are task-specific and encode notions of fairness. These learned metrics can be interpreted as similarity metrics tailored to the task at hand, addressing the challenge of selecting appropriate distance functions. Additionally, the paper provides empirical evidence and theoretical guarantees, further supporting its relevance to the query."}
{"input": "User Query: Could you provide me some recent works that tackle large-scale datasets from data analytic institutions for chart captioning?\nTitle: Chart-to-Text: A Large-Scale Benchmark for Chart Summarization\nAbstract:  Abstract Charts are commonly used for exploring data and communicating insights. Generating natural language summaries from charts can be very helpful for people in inferring key insights that would otherwise require a lot of cognitive and perceptual efforts. We present Chart-to-text, a large-scale benchmark with two datasets and a total of 44,096 charts covering a wide range of topics and chart types. We explain the dataset construction process and analyze the datasets. We also introduce a number of state-of-the-art neural models as baselines that utilize image captioning and data-to-text generation techniques to tackle two problem variations: one assumes the underlying data table of the chart is available while the other needs to extract data from chart images. Our analysis with automatic and human evaluation shows that while our best models usually generate fluent summaries and yield reasonable BLEU scores, they also suffer from hallucinations and factual errors as well as difficulties in correctly explaining complex patterns and trends in charts. ", "output": "**Decision:** True  \n**Reason:** The paper \"Chart-to-Text: A Large-Scale Benchmark for Chart Summarization\" satisfies the user query as it directly addresses the task of chart captioning using large-scale datasets. The paper introduces a benchmark dataset with 44,096 charts, which qualifies as a large-scale dataset. It also focuses on generating natural language summaries (chart captioning) from charts, which aligns with the user's interest. Furthermore, the paper discusses state-of-the-art neural models and evaluates their performance, making it relevant to the field of AI and data analytics institutions. Thus, the paper meets the detailed requirements of the query."}
{"input": "User Query: Could you provide me some studies that rely on a given template to constrain the solution?\nTitle: 3D-CODED : 3D Correspondences by Deep Deformation\nAbstract:  Abstract We present a new deep learning approach for matching deformable shapes by introducing Shape Deformation Networks which jointly encode 3D shapes and correspondences. This is achieved by factoring the surface representation into (i) a template, that parameterizes the surface, and (ii) a learnt global feature vector that parameterizes the transformation of the template into the input surface. By predicting this feature for a new shape, we implicitly predict correspondences between this shape and the template. We show that these correspondences can be improved by an additional step which improves the shape feature by minimizing the Chamfer distance between the input and transformed template. We demonstrate that our simple approach improves on state-of-the-art results on the difficult FAUST-inter challenge, with an average correspondence error of 2.88cm. We show, on the TOSCA dataset, that our method is robust to many types of perturbations, and generalizes to non-human shapes. This robustness allows it to perform well on real unclean, meshes from the the SCAPE dataset. ", "output": "**Decision:** True  \n**Reason:** The paper \"3D-CODED: 3D Correspondences by Deep Deformation\" satisfies the user query because it explicitly relies on a given template to constrain the solution. The approach described in the paper uses a template to parameterize the surface of 3D shapes. The template serves as a foundational structure, and the deformation of this template is guided by a learned global feature vector. This process constrains the solution by ensuring that the predicted correspondences between the input shape and the template are consistent with the deformation model. The use of the template is central to the method, as it provides a structured way to encode and match 3D shapes, fulfilling the requirement of relying on a given template to constrain the solution."}
{"input": "User Query: Which study proposed Neighbor Consistency Regularization (NCR) technique?\nTitle: Learning with Neighbor Consistency for Noisy Labels\nAbstract:  Abstract Recent advances in deep learning have relied on large, labelled datasets to train high-capacity models. However, collecting large datasets in a time- and cost-efficient manner often results in label noise. We present a method for learning from noisy labels that leverages similarities between training examples in feature space, encouraging the prediction of each example to be similar to its nearest neighbours. Compared to training algorithms that use multiple models or distinct stages, our approach takes the form of a simple, additional regularization term. It can be interpreted as an inductive version of the classical, transductive label propagation algorithm. We thoroughly evaluate our method on datasets evaluating both synthetic (CIFAR-10, CIFAR-100) and realistic (mini-WebVision, WebVision, Clothing1M, mini-ImageNet-Red) noise, and achieve competitive or state-of-the-art accuracies across all of them. 111The code is available at https://github.com/google-research/scenic/tree/main/scenic/projects/ncr ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Learning with Neighbor Consistency for Noisy Labels\" explicitly introduces a method for learning from noisy labels that leverages similarities between training examples in feature space. The paper describes this approach as a regularization term, which aligns with the concept of Neighbor Consistency Regularization (NCR). Additionally, the paper mentions that this method can be interpreted as an inductive version of the label propagation algorithm, further supporting its connection to NCR. The inclusion of the GitHub repository link with \"ncr\" in the path further confirms that this study proposed the Neighbor Consistency Regularization technique. Therefore, the paper fully satisfies the user query."}
{"input": "User Query: Which papers improved the design of chat-systems and image instruction tasks in the field of NLP ?\nTitle: BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\nAbstract:  Abstract The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model’s emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions. ", "output": "**Decision:** True  \n**Reason:** The paper \"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\" satisfies the requirements of the user query. The paper specifically addresses advancements in image instruction tasks by proposing a novel pre-training strategy that enables zero-shot image-to-text generation, which can follow natural language instructions. This directly contributes to improving the design of systems that integrate vision and language, a key aspect of image instruction tasks in NLP. While the paper does not explicitly focus on chat systems, its contributions to vision-language tasks and instruction-following capabilities are relevant to the broader field of NLP and align with the user's query."}
{"input": "User Query: Which studies proposed improvements to steps such as learning a matching cost function or learning to improve subsequent optimization and refinement in deep stereo matching?\nTitle: Detect, Replace, Refine: Deep Structured Prediction For Pixel Wise Labeling\nAbstract:  Abstract Pixel wise image labeling is an interesting and challenging problem with great significance in the computer vision community. In order for a dense labeling algorithm to be able to achieve accurate and precise results, it has to consider the dependencies that exist in the joint space of both the input and the output variables. An implicit approach for modeling those dependencies is by training a deep neural network that, given as input an initial estimate of the output labels and the input image, it will be able to predict a new refined estimate for the labels. In this context, our work is concerned with what is the optimal architecture for performing the label improvement task. We argue that the prior approaches of either directly predicting new label estimates or predicting residual corrections w.r.t. the initial labels with feed-forward deep network architectures are sub-optimal. Instead, we propose a generic architecture that decomposes the label improvement task to three steps: 1) detecting the initial label estimates that are incorrect, 2) replacing the incorrect labels with new ones, and finally 3) refining the renewed labels by predicting residual corrections w.r.t. them. Furthermore, we explore and compare various other alternative architectures that consist of the aforementioned Detection, Replace, and Refine components. We extensively evaluate the examined architectures in the challenging task of dense disparity estimation (stereo matching) and we report both quantitative and qualitative results on three different datasets. Finally, our dense disparity estimation network that implements the proposed generic architecture, achieves state-of-the-art results in the KITTI 2015 test surpassing prior approaches by a significant margin111We plan to release the source code that trains and tests all architectures examined in this work. For the implementation we have used the Torch framework [4]. . ", "output": "**Decision:** True  \n**Reason:** The paper \"Detect, Replace, Refine: Deep Structured Prediction For Pixel Wise Labeling\" satisfies the user query as it focuses on improvements to the process of optimization and refinement in deep stereo matching. Specifically, the paper proposes a novel architecture that decomposes the label improvement task into three steps: detecting incorrect labels, replacing them, and refining the new labels. These steps directly address the refinement process in stereo matching. Additionally, the paper evaluates these improvements in the context of dense disparity estimation, which is a core task in stereo matching, and reports state-of-the-art results. While the paper does not explicitly mention learning a matching cost function, it clearly aligns with the aspect of learning to improve subsequent optimization and refinement, as required by the query."}
{"input": "User Query: Could you provide me some works about four-IMU setting for inertial motion capture systems?\nTitle: LiDAR-aid Inertial Poser: Large-scale Human Motion Capture by Sparse Inertial and LiDAR Sensors\nAbstract:  Abstract We propose a multi-sensor fusion method for capturing challenging 3D human motions with accurate consecutive local poses and global trajectories in large-scale scenarios, only using single LiDAR and 4 IMUs, which are set up conveniently and worn lightly. Specifically, to fully utilize the global geometry information captured by LiDAR and local dynamic motions captured by IMUs, we design a two-stage pose estimator in a coarse-to-fine manner, where point clouds provide the coarse body shape and IMU measurements optimize the local actions. Furthermore, considering the translation deviation caused by the view-dependent partial point cloud, we propose a pose-guided translation corrector. It predicts the offset between captured points and the real root locations, which makes the consecutive movements and trajectories more precise and natural. Moreover, we collect a LiDAR-IMU multi-modal mocap dataset, LIPD, with diverse human actions in long-range scenarios. Extensive quantitative and qualitative experiments on LIPD and other open datasets all demonstrate the capability of our approach for compelling motion capture in large-scale scenarios, which outperforms other methods by an obvious margin. We will release our code and captured dataset to stimulate future research. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"LiDAR-aid Inertial Poser: Large-scale Human Motion Capture by Sparse Inertial and LiDAR Sensors\" satisfies the user's query. The abstract explicitly mentions the use of a four-IMU setting for inertial motion capture systems, which aligns with the user's specific interest. The paper discusses a multi-sensor fusion method that incorporates 4 IMUs and a LiDAR sensor for capturing human motion. The focus on the four-IMU setup directly addresses the user's request for works related to this configuration in inertial motion capture systems."}
{"input": "User Query: What papers are about the learning mixtures of Generative Adversarial Networks?\nTitle: AdaGAN: Boosting Generative Models\nAbstract:  Abstract Generative Adversarial Networks (GAN) Goodfellow et al. (2014) are an effective method for training generative models of complex data such as natural images. However, they are notoriously hard to train and can suffer from the problem of missing modes where the model is not able to produce examples in certain regions of the space. We propose an iterative procedure, called AdaGAN, where at every step we add a new component into a mixture model by running a GAN algorithm on a reweighted sample. This is inspired by boosting algorithms, where many potentially weak individual predictors are greedily aggregated to form a strong composite predictor. We prove that such an incremental procedure leads to convergence to the true distribution in a finite number of steps if each step is optimal, and convergence at an exponential rate otherwise. We also illustrate experimentally that this procedure addresses the problem of missing modes. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"AdaGAN: Boosting Generative Models\" directly addresses the topic of learning mixtures of Generative Adversarial Networks (GANs). The proposed method, AdaGAN, explicitly involves creating a mixture model by iteratively adding new components (GANs) trained on reweighted samples. This aligns with the concept of learning mixtures of GANs, as the approach combines multiple GANs to form a composite model. Additionally, the paper discusses how this method addresses challenges such as mode collapse, which is a common issue in GAN training. Therefore, the paper fully satisfies the detailed requirements of the user query."}
{"input": "User Query: What works contributed to the field of identifiable representation learning by introducing structure on the data generating process?\nTitle: Properties from Mechanisms: An Equivariance Perspective on Identifiable Representation Learning\nAbstract:  Abstract A key goal of unsupervised representation learning is “inverting” a data generating process to recover its latent properties. Existing work that provably achieves this goal relies on strong assumptions on relationships between the latent variables (e.g., independence conditional on auxiliary information). In this paper, we take a very different perspective on the problem and ask, “Can we instead identify latent properties by leveraging knowledge of the mechanisms that govern their evolution?” We provide a complete characterization of the sources of non-identifiability as we vary knowledge about a set of possible mechanisms. In particular, we prove that if we know the exact mechanisms under which the latent properties evolve, then identification can be achieved up to any equivariances that are shared by the underlying mechanisms. We generalize this characterization to settings where we only know some hypothesis class over possible mechanisms, as well as settings where the mechanisms are stochastic. We demonstrate the power of this mechanism-based perspective by showing that we can leverage our results to generalize existing identifiable representation learning results. These results suggest that by exploiting inductive biases on mechanisms, it is possible to design a range of new identifiable representation learning approaches. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Properties from Mechanisms: An Equivariance Perspective on Identifiable Representation Learning\"* satisfies the user query because it directly contributes to the field of identifiable representation learning by introducing structure on the data generating process. Specifically, the paper focuses on leveraging knowledge of the mechanisms governing the evolution of latent properties to achieve identifiability. It provides a formal characterization of how identifiability can be achieved based on the structure of these mechanisms, including deterministic and stochastic cases. This aligns with the user's query, as the work introduces and exploits structured assumptions (e.g., equivariances and mechanism-based inductive biases) on the data generating process to advance identifiable representation learning."}
{"input": "User Query: What works introduced or developed the high-quality AIST++ database?\nTitle: AI Choreographer: Music Conditioned 3D Dance Generation with AIST++\nAbstract:  Abstract We present AIST++, a new multi-modal dataset of 3D dance motion and music, along with FACT, a Full-Attention Cross-modal Transformer network for generating 3D dance motion conditioned on music. The proposed AIST++ dataset contains 5.2 hours of 3D dance motion in 1408 sequences, covering 10 dance genres with multi-view videos with known camera poses—the largest dataset of this kind to our knowledge. We show that naively applying sequence models such as transformers to this dataset for the task of music conditioned 3D motion generation does not produce satisfactory 3D motion that is well correlated with the input music. We overcome these shortcomings by introducing key changes in its architecture design and supervision: FACT model involves a deep cross-modal transformer block with full-attention that is trained to predict N𝑁N future motions. We empirically show that these changes are key factors in generating long sequences of realistic dance motion that are well-attuned to the input music. We conduct extensive experiments on AIST++ with user studies, where our method outperforms recent state-of-the-art methods both qualitatively and quantitatively. The code and the dataset can be found at: https://google.github.io/aichoreographer. †† ∗ equal contribution. Work performed while Ruilong was an intern at Google.  ", "output": "**Decision:** True  \n**Reason:** The paper titled \"AI Choreographer: Music Conditioned 3D Dance Generation with AIST++\" explicitly introduces the AIST++ dataset, as stated in the abstract. It describes the AIST++ dataset as a new multi-modal dataset of 3D dance motion and music, providing details such as its size (5.2 hours of 3D dance motion in 1408 sequences), the coverage of 10 dance genres, and the inclusion of multi-view videos with known camera poses. Furthermore, the abstract highlights that this dataset is the largest of its kind and is central to the research presented in the paper. Therefore, the paper satisfies the user query regarding works that introduced or developed the high-quality AIST++ database."}
{"input": "User Query: Which study found that a modality gap exists between the image embeddings and text embeddings sub-space?\nTitle: Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning\nAbstract:  Abstract We present modality gap, an intriguing geometric phenomenon of the representation space of multi-modal models. Specifically, we show that different data modalities (e.g. images and texts) are embedded at arm’s length in their shared representation in multi-modal models such as CLIP. Our systematic analysis demonstrates that this gap is caused by a combination of model initialization and contrastive learning optimization. In model initialization, we show empirically and theoretically that the representation of a common deep neural network is restricted to a narrow cone. As a consequence, in a multi-modal model with two encoders, the representations of the two modalities are clearly apart when the model is initialized. During optimization, contrastive learning keeps the different modalities separated by a certain distance, which is influenced by the temperature parameter in the loss function. Our experiments further demonstrate that varying the modality gap distance has a significant impact in improving the model’s downstream zero-shot classification performance and fairness. Our code and data are available at https://modalitygap.readthedocs.io/ ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning\"* directly addresses the user query. The study explicitly investigates the phenomenon of a \"modality gap\" in the representation space of multi-modal models, such as CLIP, where image embeddings and text embeddings are embedded at a distance from each other. The abstract clearly states that the authors analyze this gap, its causes (model initialization and contrastive learning optimization), and its implications on downstream tasks. Therefore, the paper fully satisfies the requirements of the user query, as it identifies and studies the modality gap between image and text embeddings."}
{"input": "User Query: What papers have worked on understanding whether Large Language Models can perform different types of causal analysis?\nTitle: Causal Reasoning and Large Language Models: Opening a New Frontier for Causality\nAbstract:  Abstract The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. We find that LLM-based methods establish new state-of-the-art accuracy on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain) and actual causality (86% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness. Crucially, LLMs perform these causal tasks while relying on sources of knowledge and methods distinct from and complementary to non-LLM based approaches. Specifically, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. We envision LLMs to be used alongside existing causal methods, as a proxy for human domain knowledge and to reduce human effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. We also see existing causal methods as promising tools for LLMs to formalize, validate, and communicate their reasoning, especially in high-stakes scenarios. Our experiments do not imply that complex causal reasoning has spontaneously emerged in LLMs. However, in capturing common sense and domain knowledge about causal mechanisms and supporting translation between natural language and formal methods, LLMs open new frontiers for advancing the research, practice, and adoption of causality. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Causal Reasoning and Large Language Models: Opening a New Frontier for Causality\"* directly addresses the user query by exploring whether Large Language Models (LLMs) can perform different types of causal analysis. The abstract explicitly mentions that the study investigates the causal capabilities of LLMs, distinguishing between various types of causal reasoning tasks such as pairwise causal discovery, counterfactual reasoning, and actual causality. It also evaluates the performance of LLMs on these tasks, providing empirical results and insights into their strengths and limitations. Furthermore, the paper discusses how LLMs rely on distinct methods and sources of knowledge compared to traditional causal approaches, and how they can complement existing causal methods. This aligns well with the user's interest in understanding the ability of LLMs to perform different types of causal analysis."}
{"input": "User Query: Which studies focus on instruction following as a cross-task generalization across a pool of NLP tasks?\nTitle: Multitask Prompted Training Enables Zero-Shot Task Generalization\nAbstract:  Abstract Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models’ pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16×16\\times its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6×\\times its size. All trained models are available at https://github.com/bigscience-workshop/t-zero, and all prompts are available at https://github.com/bigscience-workshop/promptsource. ", "output": "**Decision:** True  \n**Reason:**  \nThe paper titled \"Multitask Prompted Training Enables Zero-Shot Task Generalization\" satisfies the user query as it explicitly focuses on instruction following as a means of achieving cross-task generalization across a pool of NLP tasks. The paper describes a methodology where natural language tasks are converted into human-readable prompted forms, enabling the model to follow instructions across diverse tasks. By fine-tuning a pretrained encoder-decoder model on a multitask mixture, the study demonstrates the model's ability to generalize to completely held-out tasks, which aligns with the concept of cross-task generalization. Additionally, the paper benchmarks the model's performance on various datasets, further emphasizing its focus on instruction-following and generalization across tasks. Therefore, it fully satisfies the detailed requirements of the user query."}
{"input": "User Query: What works employ a GNN-based dynamics model with object features from a supervised detector for visual simulation?\nTitle: Learning Long-term Visual Dynamics with Region Proposal Interaction Networks\nAbstract:  Abstract Learning long-term dynamics models is the key to understanding physical common sense. Most existing approaches on learning dynamics from visual input sidestep long-term predictions by resorting to rapid re-planning with short-term models. This not only requires such models to be super accurate but also limits them only to tasks where an agent can continuously obtain feedback and take action at each step until completion. In this paper, we aim to leverage the ideas from success stories in visual recognition tasks to build object representations that can capture inter-object and object-environment interactions over a long-range. To this end, we propose Region Proposal Interaction Networks (RPIN), which reason about each object’s trajectory in a latent region-proposal feature space. Thanks to the simple yet effective object representation, our approach outperforms prior methods by a significant margin both in terms of prediction quality and their ability to plan for downstream tasks, and also generalize well to novel environments. Code, pre-trained models, and more visualization results are available at our Website. ", "output": "**Decision:** True  \n**Reason:** The paper \"Learning Long-term Visual Dynamics with Region Proposal Interaction Networks\" satisfies the requirements of the user query. The query specifically asks for works that employ a GNN-based dynamics model with object features extracted from a supervised detector for visual simulation. The paper proposes Region Proposal Interaction Networks (RPIN), which utilize object representations derived from region proposals—a common output of supervised object detectors. These representations are used to model inter-object and object-environment interactions over time, which aligns with the concept of a dynamics model. Furthermore, the interaction modeling described in the paper suggests the use of graph-based reasoning, which is characteristic of Graph Neural Networks (GNNs). Therefore, the paper meets the detailed criteria of the query."}
{"input": "User Query: On what studies the adversarial training (AT) technique been widely used in the image domain?\nTitle: Towards Deep Learning Models Resistant to Adversarial Attacks\nAbstract:  Abstract Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples—inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. 111Code and pre-trained models are available at https://github.com/MadryLab/mnist_challenge and https://github.com/MadryLab/cifar10_challenge. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Towards Deep Learning Models Resistant to Adversarial Attacks\"* directly addresses the adversarial training (AT) technique and its application in improving the robustness of neural networks against adversarial attacks in the image domain. The abstract explicitly mentions the use of robust optimization to study adversarial robustness and highlights methods for training networks to resist adversarial attacks. Furthermore, the paper provides practical implementations and pre-trained models for datasets like MNIST and CIFAR-10, which are widely used in the image domain. This aligns with the user query, as it explores the use of AT techniques in the context of image-based adversarial robustness."}
{"input": "User Query: What studies are about using large language models to build generic text quality scorers?\nTitle: GPTScore: Evaluate as You Desire\nAbstract:  Abstract Generative Artificial Intelligence (AI) has enabled the development of sophisticated models that are capable of producing high-caliber text, images, and other outputs through the utilization of large pre-trained models. Nevertheless, assessing the quality of the generation is an even more arduous task than the generation itself, and this issue has not been given adequate consideration recently. This paper proposes a novel evaluation framework, GPTScore, which utilizes the emergent abilities (e.g., zero-shot instruction) of generative pre-trained models to score generated texts. There are 19 pre-trained models explored in this paper, ranging in size from 80M (e.g., FLAN-T5-small) to 175B (e.g., GPT3). Experimental results on four text generation tasks, 222222 evaluation aspects, and corresponding 373737 datasets demonstrate that this approach can effectively allow us to achieve what one desires to evaluate for texts simply by natural language instructions. This nature helps us overcome several long-standing challenges in text evaluation–how to achieve customized, multi-faceted evaluation without the need for annotated samples. We make our code publicly available. 111https://github.com/jinlanfu/GPTScore ", "output": "**Decision:** True  \n**Reason:** The paper titled \"GPTScore: Evaluate as You Desire\" satisfies the user query as it directly addresses the use of large language models (LLMs) to build a generic text quality scorer. The proposed framework, GPTScore, leverages the emergent abilities of generative pre-trained models to evaluate text quality through natural language instructions. The study explores multiple pre-trained models of varying sizes and demonstrates their effectiveness in evaluating text across multiple tasks and aspects without requiring annotated samples. This aligns with the user's interest in studies about using LLMs to create generic text quality scorers."}
{"input": "User Query: Which papers applied deep learning in point cloud processing?\nTitle: PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space\nAbstract:  Abstract Few prior works study deep learning on point sets. PointNet [20] is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space\" fully satisfies the user query. The abstract explicitly states that the work applies deep learning techniques to process point clouds. It introduces a hierarchical neural network that builds upon PointNet, a deep learning model, to capture local structures in point sets and learn features from 3D point clouds. The paper also demonstrates its application to challenging benchmarks of 3D point cloud data, clearly aligning with the user's interest in identifying papers that apply deep learning to point cloud processing."}
{"input": "User Query: What studies have been done on diffusion process and Langevin dynamics in deep generative models?\nTitle: Score-Based Generative Modeling through Stochastic Differential Equations\nAbstract:  Abstract Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024×1024102410241024\\times 1024 images for the first time from a score-based generative model. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Score-Based Generative Modeling through Stochastic Differential Equations\"* satisfies the user query as it directly addresses studies on the diffusion process and Langevin dynamics in the context of deep generative models. The abstract explicitly discusses the use of stochastic differential equations (SDEs) to model the diffusion process, where noise is injected and subsequently removed to transform data distributions. This is a central aspect of diffusion-based generative models. Additionally, the paper mentions a \"predictor-corrector framework,\" which is closely related to Langevin dynamics, as it involves correcting errors during the reverse-time SDE evolution—a process that aligns with the iterative refinement characteristic of Langevin dynamics. The paper also explores advancements in score-based generative modeling, which is foundational to diffusion processes in generative models. Therefore, it fully satisfies the detailed requirements of the user query."}
{"input": "User Query: Which work introduced the SAM optimizer?\nTitle: Sharpness-Aware Minimization for Efficiently Improving Generalization\nAbstract:  Abstract In today’s heavily overparameterized models, the value of the training loss provides few guarantees on model generalization ability. Indeed, optimizing only the training loss value, as is commonly done, can easily lead to suboptimal model quality. Motivated by prior work connecting the geometry of the loss landscape and generalization, we introduce a novel, effective procedure for instead simultaneously minimizing loss value and loss sharpness. In particular, our procedure, Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. We present empirical results showing that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-{10, 100}, ImageNet, finetuning tasks) and models, yielding novel state-of-the-art performance for several. Additionally, we find that SAM natively provides robustness to label noise on par with that provided by state-of-the-art procedures that specifically target learning with noisy labels. We open source our code at https://github.com/google-research/sam. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Sharpness-Aware Minimization for Efficiently Improving Generalization\" explicitly introduces the Sharpness-Aware Minimization (SAM) optimizer. The abstract clearly states that the authors propose a novel procedure, SAM, which seeks to minimize both the loss value and the sharpness of the loss landscape. This aligns directly with the user query asking for the work that introduced the SAM optimizer. Therefore, the paper fully satisfies the detailed requirements of the query."}
{"input": "User Query: Can you tell me some studies that focused on quantifying interactions between input variables in DNNs?\nTitle: Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs\nAbstract:  Abstract The driving force behind the recent success of LSTMs has been their ability to learn complex and non-linear relationships. Consequently, our inability to describe these relationships has led to LSTMs being characterized as black boxes. To this end, we introduce contextual decomposition (CD), an interpretation algorithm for analysing individual predictions made by standard LSTMs, without any changes to the underlying model. By decomposing the output of a LSTM, CD captures the contributions of combinations of words or variables to the final prediction of an LSTM. On the task of sentiment analysis with the Yelp and SST data sets, we show that CD is able to reliably identify words and phrases of contrasting sentiment, and how they are combined to yield the LSTM’s final prediction. Using the phrase-level labels in SST, we also demonstrate that CD is able to successfully extract positive and negative negations from an LSTM, something which has not previously been done. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs\"* satisfies the requirements of the user query. The user specifically asked for studies that focus on quantifying interactions between input variables in deep neural networks (DNNs). This paper introduces Contextual Decomposition (CD), an interpretation algorithm designed to analyze the contributions of combinations of words or variables to the final prediction of an LSTM. By decomposing the output of the LSTM, the method explicitly captures how combinations of input variables (e.g., words or phrases) interact to influence the model's predictions. Therefore, the study aligns with the user's interest in quantifying interactions between input variables in DNNs."}
{"input": "User Query: What works have proposed approaches for domain generalization via domain randomization or adversarial data augmentation?\nTitle: Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World\nAbstract:  Abstract Bridging the ‘reality gap’ that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to 1.5 cmtimes1.5centimeter1.5\\text{\\,}\\mathrm{cm} and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World\"* satisfies the user query. The user is asking for works that propose approaches for domain generalization via domain randomization or adversarial data augmentation. This paper explicitly explores *domain randomization* as a technique to train models on simulated data that generalize to real-world data by introducing variability in the simulated environment. The approach aims to bridge the \"reality gap\" and achieve domain generalization, which aligns with the user's query. While the focus is on robotic object localization and manipulation, the methodology of domain randomization directly addresses the concept of domain generalization."}
{"input": "User Query: Which works utilize the Kalman Filter (KF) as a linear motion model for motion tracking?\nTitle: Simple Online and Realtime Tracking with a Deep Association Metric\nAbstract:  Abstract Simple Online and Realtime Tracking (SORT) is a pragmatic approach to multiple object tracking with a focus on simple, effective algorithms. In this paper, we integrate appearance information to improve the performance of SORT. Due to this extension we are able to track objects through longer periods of occlusions, effectively reducing the number of identity switches. In spirit of the original framework we place much of the computational complexity into an offline pre-training stage where we learn a deep association metric on a large-scale person re-identification dataset. During online application, we establish measurement-to-track associations using nearest neighbor queries in visual appearance space. Experimental evaluation shows that our extensions reduce the number of identity switches by 45%, achieving overall competitive performance at high frame rates. ", "output": "**Decision:** True  \n**Reason:** The paper \"Simple Online and Realtime Tracking with a Deep Association Metric\" discusses the SORT algorithm, which is widely known to utilize the Kalman Filter (KF) as a linear motion model for motion tracking. While the abstract does not explicitly mention the Kalman Filter, the SORT algorithm itself is based on a combination of a Kalman Filter for motion prediction and a data association step for object tracking. Since the paper builds upon the original SORT framework and does not replace the Kalman Filter as the motion model, it can be inferred that the Kalman Filter is still utilized as part of the motion tracking process. Therefore, the paper satisfies the requirements of the user query."}
{"input": "User Query: Could you provide me studies that proposed methods to infer the learning rules governing weight updates from post-learning neural activity or spike train recordings?\nTitle: A framework for studying synaptic plasticity with neural spike train data\nAbstract: Learning and memory in the brain are implemented by complex, time-varying changes in neural circuitry. The computational rules according to which synaptic weights change over time are the subject of much research, and are not precisely understood. Until recently, limitations in experimental methods have made it challenging to test hypotheses about synaptic plasticity on a large scale. However, as such data become available and these barriers are lifted, it becomes necessary to develop analysis techniques to validate plasticity models. Here, we present a highly extensible framework for modeling arbitrary synaptic plasticity rules on spike train data in populations of interconnected neurons. We treat synaptic weights as a (potentially nonlinear) dynamical system embedded in a fully-Bayesian generalized linear model (GLM). In addition, we provide an algorithm for inferring synaptic weight trajectories alongside the parameters of the GLM and of the learning rules. Using this method, we perform model comparison of two proposed variants of the well-known spike-timing-dependent plasticity (STDP) rule, where nonlinear effects play a substantial role. On synthetic data generated from the biophysical simulator NEURON, we show that we can recover the weight trajectories, the pattern of connectivity, and the underlying learning rules.", "output": "**Decision:** True  \n**Reason:** The paper satisfies the requirements of the user query. It explicitly proposes a method to infer the learning rules governing weight updates from neural spike train data. The framework described in the paper models synaptic plasticity rules and treats synaptic weights as a dynamical system embedded in a Bayesian GLM. Furthermore, the algorithm provided in the paper is capable of inferring synaptic weight trajectories and the parameters of the learning rules, directly addressing the user's interest in inferring learning rules from post-learning neural activity or spike train recordings. The paper also demonstrates the method's ability to recover learning rules (e.g., variants of STDP) using synthetic data, aligning well with the query's focus."}
{"input": "User Query: What papers explore the universality and modality-agnostic capabilities of transformers?\nTitle: Pretrained Transformers As Universal Computation Engines\nAbstract:  Abstract We investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning – in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language can improve performance and compute efficiency on non-language downstream tasks. Additionally, we perform an analysis of the architecture, comparing the performance of a random initialized transformer to a random LSTM. Combining the two insights, we find language-pretrained transformers can obtain strong performance on a variety of non-language tasks111 Code available at github.com/kzl/universal-computation. For a summary of changes made in the updated arXiv version, see Appendix A. . ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Pretrained Transformers As Universal Computation Engines\"* satisfies the user's query as it directly investigates the universality and modality-agnostic capabilities of transformers. The study explores how a transformer pretrained on natural language can generalize to other modalities (e.g., numerical computation, vision, and protein fold prediction) with minimal fine-tuning. This aligns with the user's interest in understanding the universality of transformers across different modalities. Furthermore, the paper explicitly examines the ability of transformers to perform well on non-language tasks, which demonstrates their modality-agnostic capabilities. Therefore, the paper fully addresses the detailed requirements of the query."}
{"input": "User Query: What works have explored the adaptation of audio-only speech models into audio-visual models?\nTitle: MixSpeech: Cross-Modality Self-Learning with Audio-Visual Stream Mixup for Visual Speech Translation and Recognition\nAbstract:  Abstract Multi-media communications facilitate global interaction among people. However, despite researchers exploring cross-lingual translation techniques such as machine translation and audio speech translation to overcome language barriers, there is still a shortage of cross-lingual studies on visual speech. This lack of research is mainly due to the absence of datasets containing visual speech and translated text pairs. In this paper, we present AVMuST-TED, the first dataset for Audio-Visual Multilingual Speech Translation, derived from TED talks. Nonetheless, visual speech is not as distinguishable as audio speech, making it difficult to develop a mapping from source speech phonemes to the target language text. To address this issue, we propose MixSpeech, a cross-modality self-learning framework that utilizes audio speech to regularize the training of visual speech tasks. To further minimize the cross-modality gap and its impact on knowledge transfer, we suggest adopting mixed speech, which is created by interpolating audio and visual streams, along with a curriculum learning strategy to adjust the mixing ratio as needed. MixSpeech enhances speech translation in noisy environments, improving BLEU scores for four languages on AVMuST-TED by +1.4 to +4.2. Moreover, it achieves state-of-the-art performance in lip reading on CMLR (11.1%), LRS2 (25.5%), and LRS3 (28.0%). ", "output": "**Decision:** True  \n**Reason:** The paper \"MixSpeech: Cross-Modality Self-Learning with Audio-Visual Stream Mixup for Visual Speech Translation and Recognition\" satisfies the user query as it explicitly explores the adaptation of audio-only speech models into audio-visual models. The paper introduces a cross-modality self-learning framework (MixSpeech) that leverages audio speech to regularize the training of visual speech tasks, effectively bridging the gap between the two modalities. Additionally, the use of mixed speech (interpolating audio and visual streams) and curriculum learning further demonstrates the adaptation of audio-only models into audio-visual contexts. This directly aligns with the user's query about works exploring such adaptations."}
{"input": "User Query: What papers studied representation learning for low-rank MDPs?\nTitle: Model-free Representation Learning and Exploration in Low-rank MDPs\nAbstract:  Abstract The low-rank MDP has emerged as an important model for studying representation learning and exploration in reinforcement learning. With a known representation, several model-free exploration strategies exist. In contrast, all algorithms for the unknown representation setting are model-based, thereby requiring the ability to model the full dynamics. In this work, we present the first model-free representation learning algorithms for low-rank MDPs. The key algorithmic contribution is a new minimax representation learning objective, for which we provide variants with differing tradeoffs in their statistical and computational properties. We interleave this representation learning step with an exploration strategy to cover the state space in a reward-free manner. The resulting algorithms are provably sample efficient and can accommodate general function approximation to scale to complex environments. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Model-free Representation Learning and Exploration in Low-rank MDPs\"* directly addresses the topic of representation learning for low-rank MDPs. The abstract explicitly mentions that the work focuses on developing representation learning algorithms for low-rank MDPs, which aligns with the user query. Furthermore, it introduces a new minimax representation learning objective and discusses its application in a reinforcement learning context. Therefore, the paper fully satisfies the detailed requirements of the user query."}
{"input": "User Query: Could you provide some specific examples of works that use CLIP for high-quality image generation and editing?\nTitle: VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance\nAbstract:  Abstract Generating and editing images from open domain text prompts is a challenging task that heretofore has required expensive and specially trained models. We demonstrate a novel methodology for both tasks which is capable of producing images of high visual quality from text prompts of significant semantic complexity without any training by using a multimodal encoder to guide image generations. We demonstrate on a variety of tasks how using CLIP [40] to guide VQGAN [12] produces higher visual quality outputs than prior, less flexible approaches like minDALL-E [20], GLIDE [36] and Open-Edit [26], despite not being trained for the tasks presented. Our code is available in a public repository. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance\" fully satisfies the user query. The user is asking for specific examples of works that use CLIP for high-quality image generation and editing. The paper explicitly discusses the use of CLIP in conjunction with VQGAN to generate and edit images based on natural language prompts. It highlights how CLIP is used as a multimodal encoder to guide the image generation process, resulting in high-quality outputs. Furthermore, the abstract mentions that the methodology produces higher visual quality outputs compared to other approaches like minDALL-E, GLIDE, and Open-Edit, directly addressing the requirement for high-quality image generation and editing."}
{"input": "User Query: Which works use the method of adversarial training to defend potential security threats in machine learning?\nTitle: Adversarial Machine Learning at Scale\nAbstract:  Abstract Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model’s parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet (Russakovsky et al., 2014). Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a “label leaking” effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Adversarial Machine Learning at Scale\" explicitly discusses the use of adversarial training as a method to defend against potential security threats in machine learning. The abstract clearly states that adversarial training is used to make models more robust to adversarial examples, which are malicious inputs designed to fool machine learning models. The paper also provides recommendations for scaling adversarial training to large models and datasets, addressing robustness against attacks, and resolving issues like \"label leaking.\" These contributions align directly with the user query, as they focus on using adversarial training to defend against security threats in machine learning."}
{"input": "User Query: Which earlier works are focused on designing various attention mechanisms for semantic segmentation?\nTitle: CCNet: Criss-Cross Attention for Semantic Segmentation\nAbstract:  Abstract Contextual information is vital in visual understanding problems, such as semantic segmentation and object detection. We propose a Criss-Cross Network (CCNet) for obtaining full-image contextual information in a very effective and efficient way. Concretely, for each pixel, a novel criss-cross attention module harvests the contextual information of all the pixels on its criss-cross path. By taking a further recurrent operation, each pixel can finally capture the full-image dependencies. Besides, a category consistent loss is proposed to enforce the criss-cross attention module to produce more discriminative features. Overall, CCNet is with the following merits: 1) GPU memory friendly. Compared with the non-local block, the proposed recurrent criss-cross attention module requires 11×11\\times less GPU memory usage. 2) High computational efficiency. The recurrent criss-cross attention significantly reduces FLOPs by about 85%percent8585\\% of the non-local block. 3) The state-of-the-art performance. We conduct extensive experiments on semantic segmentation benchmarks including Cityscapes, ADE20K, human parsing benchmark LIP, instance segmentation benchmark COCO, video segmentation benchmark CamVid. In particular, our CCNet achieves the mIoU scores of 81.9%percent81.981.9\\%, 45.76%percent45.7645.76\\% and 55.47%percent55.4755.47\\% on the Cityscapes test set, the ADE20K validation set and the LIP validation set respectively, which are the new state-of-the-art results. The source codes are available at https://github.com/speedinghzl/CCNet. ", "output": "**Decision:** True  \n**Reason:** The paper \"CCNet: Criss-Cross Attention for Semantic Segmentation\" fully satisfies the user query. The paper explicitly focuses on designing a novel attention mechanism, the criss-cross attention module, specifically for semantic segmentation. It introduces a method to efficiently capture full-image contextual information, which is crucial for semantic segmentation tasks. The abstract highlights the development of the criss-cross attention mechanism and its advantages in terms of GPU memory usage, computational efficiency, and performance on semantic segmentation benchmarks. Therefore, it aligns well with the user's query about earlier works focused on designing attention mechanisms for semantic segmentation."}
{"input": "User Query: What studies have attempted to extend conformal prediction to handle time series data?\nTitle: Exact and Robust Conformal Inference Methods for Predictive Machine Learning With Dependent Data\nAbstract:  Abstract We extend conformal inference to general settings that allow for time series data. Our proposal is developed as a randomization method and accounts for potential serial dependence by including block structures in the permutation scheme such that the latter forms a group. As a result, the proposed method retains the exact, model-free validity when the data are i.i.d. or more generally exchangeable, similar to usual conformal inference methods. When exchangeability fails, as is the case for common time series data, the proposed approach is approximately valid under weak assumptions on the conformity score. ", "output": "**Decision:** True  \n**Reason:** The paper explicitly addresses the extension of conformal inference methods to handle time series data, as stated in the abstract. It develops a method that accounts for serial dependence in time series by incorporating block structures into the permutation scheme. This approach ensures validity even when the data are not exchangeable, which is a common characteristic of time series data. Therefore, the paper satisfies the user's query regarding studies that extend conformal prediction to handle time series data."}
{"input": "User Query: Could you give me some examples of works that have developed variants of the NeRF method?\nTitle: TensoRF: Tensorial Radiance Fields\nAbstract:  Abstract We present TensoRF, a novel approach to model and reconstruct radiance fields. Unlike NeRF that purely uses MLPs, we model the radiance field of a scene as a 4D tensor, which represents a 3D voxel grid with per-voxel multi-channel features. Our central idea is to factorize the 4D scene tensor into multiple compact low-rank tensor components. We demonstrate that applying traditional CANDECOMP/PARAFAC (CP) decomposition – that factorizes tensors into rank-one components with compact vectors – in our framework leads to improvements over vanilla NeRF. To further boost performance, we introduce a novel vector-matrix (VM) decomposition that relaxes the low-rank constraints for two modes of a tensor and factorizes tensors into compact vector and matrix factors. Beyond superior rendering quality, our models with CP and VM decompositions lead to a significantly lower memory footprint in comparison to previous and concurrent works that directly optimize per-voxel features. Experimentally, we demonstrate that TensoRF with CP decomposition achieves fast reconstruction (<30absent30<30 min) with better rendering quality and even a smaller model size (<4absent4<4 MB) compared to NeRF. Moreover, TensoRF with VM decomposition further boosts rendering quality and outperforms previous state-of-the-art methods, while reducing the reconstruction time (<10absent10<10 min) and retaining a compact model size (<75absent75<75 MB). ", "output": "**Decision:** True  \n**Reason:** The paper \"TensoRF: Tensorial Radiance Fields\" satisfies the user query as it clearly presents a variant of the NeRF (Neural Radiance Fields) method. The paper introduces a novel approach to model radiance fields by replacing the MLP-based representation in NeRF with a tensor-based representation. It further develops this idea by proposing two decomposition techniques (CP decomposition and VM decomposition) to improve performance, reduce memory footprint, and enhance rendering quality. These modifications represent a significant variation and extension of the NeRF method, aligning with the user's request for examples of works that have developed NeRF variants."}
{"input": "User Query: Any works about the use of RLHF in guiding RL policy optimization?\nTitle: Fine-Tuning Language Models from Human Preferences\nAbstract: Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.", "output": "**Decision:** True  \n**Reason:** The paper titled \"Fine-Tuning Language Models from Human Preferences\" satisfies the user query as it directly addresses the use of Reinforcement Learning from Human Feedback (RLHF) in guiding RL policy optimization. The abstract describes how reward learning, based on human judgment, is used to train language models through reinforcement learning. Specifically, the paper applies RLHF to optimize policies for natural language tasks by using human feedback to define the reward signal. This aligns with the user's request for works that explore the use of RLHF in guiding RL policy optimization."}
{"input": "User Query: In what work does the researcher provide a role consistency bound for the logistic loss?\nTitle: Revisiting Discriminative vs. Generative Classifiers: Theory and Implications\nAbstract:  Abstract A large-scale deep model pre-trained on massive labeled or unlabeled data transfers well to downstream tasks. Linear evaluation freezes parameters in the pre-trained model and trains a linear classifier separately, which is efficient and attractive for transfer. However, little work has investigated the classifier in linear evaluation except for the default logistic regression. Inspired by the statistical efficiency of naïve Bayes, the paper revisits the classical topic on discriminative vs. generative classifiers (Ng & Jordan, 2001). Theoretically, the paper considers the surrogate loss instead of the zero-one loss in analyses and generalizes the classical results from binary cases to multiclass ones. We show that, under mild assumptions, multiclass naïve Bayes requires O​(log⁡n)𝑂𝑛O(\\log n) samples to approach its asymptotic error while the corresponding multiclass logistic regression requires O​(n)𝑂𝑛O(n) samples, where n𝑛n is the feature dimension. To establish it, we present a multiclass ℋℋ\\mathcal{H}-consistency bound framework and an explicit bound for logistic loss, which are of independent interests. Simulation results on a mixture of Gaussian validate our theoretical findings. Experiments on various pre-trained deep vision models show that naïve Bayes consistently converges faster as the number of data increases. Besides, naïve Bayes shows promise in few-shot cases and we observe the “two regimes” phenomenon in pre-trained supervised models. Our code is available at https://github.com/ML-GSAI/Revisiting-Dis-vs-Gen-Classifiers. ", "output": "**Decision:** True  \n**Reason:** The abstract of the paper explicitly mentions that the researchers present a multiclass ℋ-consistency bound framework and an explicit bound for the logistic loss. This directly satisfies the user's query, as the paper provides a role consistency bound for the logistic loss. The abstract also highlights that this contribution is of independent interest, further confirming its relevance to the query."}
{"input": "User Query: What studies present work on finding Nash equilibria of matrix games?\nTitle: Linear Last-iterate Convergence in Constrained Saddle-point Optimization\nAbstract:  Abstract Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weights Update (OMWU) for saddle-point optimization have received growing attention due to their favorable last-iterate convergence. However, their behaviors for simple bilinear games over the probability simplex are still not fully understood — previous analysis lacks explicit convergence rates, only applies to an exponentially small learning rate, or requires additional assumptions such as the uniqueness of the optimal solution. In this work, we significantly expand the understanding of last-iterate convergence for OGDA and OMWU in the constrained setting. Specifically, for OMWU in bilinear games over the simplex, we show that when the equilibrium is unique, linear last-iterate convergence is achieved with a learning rate whose value is set to a universal constant,  improving the result of (Daskalakis & Panageas, 2019b) under the same assumption. We then significantly extend the results to more general objectives and feasible sets for the projected OGDA algorithm, by introducing a sufficient condition under which OGDA exhibits concrete last-iterate convergence rates with a constant learning rate whose value only depends on the smoothness of the objective function.  We show that bilinear games over any polytope satisfy this condition and OGDA converges exponentially fast even without the unique equilibrium assumption. Our condition also holds for strongly-convex-strongly-concave functions, recovering the result of (Hsieh et al., 2019). Finally, we provide experimental results to further support our theory. ", "output": "**Decision:** True  \n**Reason:** The paper discusses the behavior of algorithms such as Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weights Update (OMWU) in the context of bilinear games over the probability simplex. Bilinear games are a specific type of matrix game, and finding Nash equilibria in such games is equivalent to solving saddle-point problems. The paper explicitly addresses the convergence properties of these algorithms for finding equilibria in bilinear games, which are directly related to Nash equilibria in matrix games. Therefore, the paper satisfies the requirements of the user query."}
{"input": "User Query: What works describe the extraction of coarse object masks in the context of unsupervised instance segmentation?\nTitle: FreeSOLO: Learning to Segment Objects without AnnotationsPart of this work was done when XW was an intern at NVIDIA, and CS was with The Univerity of Adelaide. CS is the corresponding author.\nAbstract:  Abstract Instance segmentation is a fundamental vision task that aims to recognize and segment each object in an image. However, it requires costly annotations such as bounding boxes and segmentation masks for learning. In this work, we propose a fully unsupervised learning method that learns class-agnostic instance segmentation without any annotations. We present FreeSOLO, a self-supervised instance segmentation framework built on top of the simple instance segmentation method SOLO. Our method also presents a novel localization-aware pre-training framework, where objects can be discovered from complicated scenes in an unsupervised manner. FreeSOLO achieves 9.8% AP50 on the challenging COCO dataset, which even outperforms several segmentation proposal methods that use manual annotations. For the first time, we demonstrate unsupervised class-agnostic instance segmentation successfully. FreeSOLO’s box localization significantly outperforms state-of-the-art unsupervised object detection/discovery methods, with about 100% relative improvements in COCO AP. FreeSOLO further demonstrates superiority as a strong pre-training method, outperforming state-of-the-art self-supervised pre-training methods by ++9.8% AP when fine-tuning instance segmentation with only 5% COCO masks. Code is available at:  github.com/NVlabs/FreeSOLO ", "output": "**Decision:** True  \n**Reason:** The paper \"FreeSOLO: Learning to Segment Objects without Annotations\" satisfies the user query as it directly addresses the extraction of coarse object masks in the context of unsupervised instance segmentation. The abstract explicitly mentions that FreeSOLO is a fully unsupervised learning method for class-agnostic instance segmentation, which inherently involves the extraction of object masks without any annotations. Furthermore, the paper highlights the ability to discover objects from complex scenes in an unsupervised manner and demonstrates significant performance improvements in tasks related to object localization and segmentation. This aligns well with the user's query about works describing the extraction of coarse object masks in unsupervised instance segmentation."}
{"input": "User Query: Could you indicate the works on latent diffusion models?\nTitle: High-Resolution Image Synthesis with Latent Diffusion Models\nAbstract:  Abstract By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state-of-the-art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including text-to-image synthesis, unconditional image generation and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.  ", "output": "**Decision:** True  \n**Reason:** The paper titled \"High-Resolution Image Synthesis with Latent Diffusion Models\" directly addresses the topic of latent diffusion models (LDMs). It introduces and discusses the concept of applying diffusion models in the latent space of pretrained autoencoders, which is the core idea behind latent diffusion models. The paper explains how this approach reduces computational requirements while maintaining high-quality results, and it highlights the advantages of LDMs in various tasks such as image inpainting, text-to-image synthesis, and super-resolution. Therefore, the paper fully satisfies the user's query about works on latent diffusion models."}
{"input": "User Query: Which papers focused on BayesPCN model for memorization and recall of a stream of patterns?\nTitle: BayesPCN: A Continually Learnable Predictive Coding Associative Memory\nAbstract:  Abstract Associative memory plays an important role in human intelligence and its mechanisms have been linked to attention in machine learning. While the machine learning community’s interest in associative memories has recently been rekindled, most work has focused on memory recall (r​e​a​d𝑟𝑒𝑎𝑑read) over memory learning (w​r​i​t​e𝑤𝑟𝑖𝑡𝑒write). In this paper, we present BayesPCN, a hierarchical associative memory capable of performing continual one-shot memory writes without meta-learning. Moreover, BayesPCN is able to gradually forget past observations (f​o​r​g​e​t𝑓𝑜𝑟𝑔𝑒𝑡forget) to free its memory. Experiments show that BayesPCN can recall corrupted i.i.d. high-dimensional data observed hundreds to a thousand “timesteps” ago without a large drop in recall ability compared to the state-of-the-art offline-learned parametric memory models. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"BayesPCN: A Continually Learnable Predictive Coding Associative Memory\" directly addresses the user's query. The paper explicitly focuses on the BayesPCN model and its capabilities for memorization and recall. It discusses the model's ability to perform continual one-shot memory writes (memorization) and recall corrupted high-dimensional data observed over a sequence of timesteps (stream of patterns). This aligns with the user's interest in the BayesPCN model for memorization and recall of a stream of patterns."}
{"input": "User Query: What papers developed a generalization of cubic splines in the space of probability measures?\nTitle: Measure-valued spline curves: an optimal transport viewpoint\nAbstract:  Abstract. The aim of this article is to introduce and address the problem to smoothly interpolate (empirical) probability measures. To this end, we lift the concept of a spline curve from the setting of points in a Euclidean space that that of probability measures, using the framework of optimal transport. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Measure-valued spline curves: an optimal transport viewpoint\" satisfies the user query. The paper explicitly addresses the problem of generalizing the concept of spline curves (including cubic splines) from the Euclidean space to the space of probability measures. It utilizes the framework of optimal transport to achieve this, which aligns with the user's interest in a generalization of cubic splines in the space of probability measures. Thus, the paper fully meets the requirements of the query."}
{"input": "User Query: Are there any benchmarks that include tasks in text, image, and math, designed for scalability?\nTitle: Long Range Arena: A Benchmark for Efficient Transformers\nAbstract:  Abstract Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. To this date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. This paper proposes a systematic and unified benchmark, Long-Range Arena, specifically focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from 1​K1𝐾1K to 16​K16𝐾16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. Long-Range Arena paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle. Our benchmark code will be released at https://github.com/google-research/long-range-arena. ", "output": "**Decision:** True  \n**Reason:** The paper \"Long Range Arena: A Benchmark for Efficient Transformers\" satisfies the requirements of the user query. The benchmark proposed in the paper includes tasks across multiple modalities, specifically text, images, and mathematical expressions. These tasks are designed to evaluate model performance in long-context scenarios, which inherently involves scalability considerations. The benchmark suite encompasses a wide range of data types and reasoning tasks, such as similarity, structural, and visual-spatial reasoning, which align with the user's interest in tasks involving text, image, and math. Therefore, the paper meets the criteria specified in the query."}
{"input": "User Query: What researches applied representation interpolation methods to NLP tasks?\nTitle: MixKD: Towards Efficient Distillation of Large-scale Language Models\nAbstract:  Abstract Large-scale language models have recently demonstrated impressive empirical performance. Nevertheless, the improved results are attained at the price of bigger models, more power consumption, and slower inference, which hinder their applicability to low-resource (both memory and computation) platforms. Knowledge distillation (KD) has been demonstrated as an effective framework for compressing such big models. However, large-scale neural network systems are prone to memorize training instances, and thus tend to make inconsistent predictions when the data distribution is altered slightly. Moreover, the student model has few opportunities to request useful information from the teacher model when there is limited task-specific data available. To address these issues, we propose MixKD, a data-agnostic distillation framework that leverages mixup, a simple yet efficient data augmentation approach, to endow the resulting model with stronger generalization ability. Concretely, in addition to the original training examples, the student model is encouraged to mimic the teacher’s behavior on the linear interpolation of example pairs as well. We prove from a theoretical perspective that under reasonable conditions MixKD gives rise to a smaller gap between the generalization error and the empirical error. To verify its effectiveness, we conduct experiments on the GLUE benchmark, where MixKD consistently leads to significant gains over the standard KD training, and outperforms several competitive baselines. Experiments under a limited-data setting and ablation studies further demonstrate the advantages of the proposed approach. ", "output": "**Decision:** True  \n**Reason:** The paper \"MixKD: Towards Efficient Distillation of Large-scale Language Models\" satisfies the user query as it applies representation interpolation methods to NLP tasks. Specifically, the paper introduces MixKD, a distillation framework that leverages *mixup*, a data augmentation technique based on linear interpolation of example pairs, to improve the generalization ability of student models in NLP tasks. The use of mixup directly involves representation interpolation, and the experiments conducted on the GLUE benchmark confirm its application to NLP tasks. Therefore, the paper aligns with the detailed requirements of the user query."}
{"input": "User Query: Can you provide studies that incorporate explanations within the model architecture in understanding complex models?\nTitle: Concept Bottleneck Models\nAbstract:  Abstract We seek to learn models that we can interact with using high-level concepts: if the model did not think there was a bone spur in the x-ray, would it still predict severe arthritis? State-of-the-art models today do not typically support the manipulation of concepts like “the existence of bone spurs”, as they are trained end-to-end to go directly from raw input (e.g., pixels) to output (e.g., arthritis severity). We revisit the classic idea of first predicting concepts that are provided at training time, and then using these concepts to predict the label. By construction, we can intervene on these concept bottleneck models by editing their predicted concept values and propagating these changes to the final prediction. On x-ray grading and bird identification, concept bottleneck models achieve competitive accuracy with standard end-to-end models, while enabling interpretation in terms of high-level clinical concepts (“bone spurs”) or bird attributes (“wing color”). These models also allow for richer human-model interaction: accuracy improves significantly if we can correct model mistakes on concepts at test time. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Concept Bottleneck Models\" satisfies the user query because it explicitly incorporates explanations within the model architecture to aid in understanding complex models. The proposed concept bottleneck models are designed to predict intermediate high-level concepts (e.g., \"bone spurs\" or \"wing color\") before making the final prediction. This approach inherently provides explanations by breaking down the decision-making process into interpretable components, allowing users to understand and interact with the model's reasoning. Furthermore, the ability to intervene on these concepts and observe the impact on the final prediction enhances the interpretability and transparency of the model, aligning with the user's requirement for studies that integrate explanations within the architecture to understand complex models."}
{"input": "User Query: Which research proposed recurrent multi-graph convolutional neural networks (RMGCNN) as a solution for missing node features in graph-structured data?\nTitle: Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks\nAbstract:  Abstract Matrix completion models are among the most common formulations of recommender systems. Recent works have showed a boost of performance of these techniques when introducing the pairwise relationships between users/items in the form of graphs, and imposing smoothness priors on these graphs. However, such techniques do not fully exploit the local stationarity structures of user/item graphs, and the number of parameters to learn is linear w.r.t. the number of users and items. We propose a novel approach to overcome these limitations by using geometric deep learning on graphs. Our matrix completion architecture combines graph convolutional neural networks and recurrent neural networks to learn meaningful statistical graph-structured patterns and the non-linear diffusion process that generates the known ratings. This neural network system requires a constant number of parameters independent of the matrix size. We apply our method on both synthetic and real datasets, showing that it outperforms state-of-the-art techniques. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks\" explicitly mentions the proposal of a novel approach that combines graph convolutional neural networks and recurrent neural networks. This combination aligns with the concept of recurrent multi-graph convolutional neural networks (RMGCNN). The paper addresses the problem of missing information (in this case, ratings in a matrix completion task) by leveraging graph-structured data and recurrent mechanisms. While the abstract does not explicitly use the term \"missing node features,\" the context of matrix completion and the use of graph-based methods to handle missing information in structured data satisfies the intent of the user query. Therefore, the paper meets the requirements of the query."}
{"input": "User Query: Which papers present research on indoor 3D object detection methods?\nTitle: H3DNet: 3D Object Detection Using Hybrid Geometric Primitives\nAbstract:  Abstract We introduce H3DNet, which takes a colorless 3D point cloud as input and outputs a collection of oriented object bounding boxes (or BB) and their semantic labels. The critical idea of H3DNet is to predict a hybrid set of geometric primitives, i.e., BB centers, BB face centers, and BB edge centers. We show how to convert the predicted geometric primitives into object proposals by defining a distance function between an object and the geometric primitives. This distance function enables continuous optimization of object proposals, and its local minimums provide high-fidelity object proposals. H3DNet then utilizes a matching and refinement module to classify object proposals into detected objects and fine-tune the geometric parameters of the detected objects. The hybrid set of geometric primitives not only provides more accurate signals for object detection than using a single type of geometric primitives, but it also provides an overcomplete set of constraints on the resulting 3D layout. Therefore, H3DNet can tolerate outliers in predicted geometric primitives. Our model achieves state-of-the-art 3D detection results on two large datasets with real 3D scans, ScanNet and SUN RGB-D. Our code is open-sourced at here. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"H3DNet: 3D Object Detection Using Hybrid Geometric Primitives\" fully satisfies the user query. The user is looking for research on indoor 3D object detection methods, and this paper explicitly focuses on 3D object detection using point clouds, which is a common representation for indoor environments. Furthermore, the paper mentions achieving state-of-the-art results on two large datasets, ScanNet and SUN RGB-D, both of which are widely used for indoor 3D object detection tasks. Therefore, the paper is directly relevant to the query."}
{"input": "User Query: Which studies adopt the practice of approximating novelty with prediction error on random features?\nTitle: Exploration by Random Network Distillation\nAbstract:  Abstract We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma’s Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level. ", "output": "**Decision:** True  \n**Reason:** The paper \"Exploration by Random Network Distillation\" satisfies the requirements of the user query. The method described in the paper explicitly involves approximating novelty using prediction error on random features. Specifically, the Random Network Distillation (RND) approach computes an exploration bonus based on the prediction error of a neural network that attempts to predict features of observations generated by a fixed, randomly initialized neural network. This aligns directly with the practice of using prediction error on random features as a measure of novelty. Therefore, the paper fully meets the criteria outlined in the query."}
{"input": "User Query: Could you provide me some studies explaining how pre-trained language models can handle ambiguity and learn reasoning rules implicitly?\nTitle: ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language\nAbstract:  Abstract Transformers have been shown to emulate logical deduction over natural language theories (logical rules expressed in natural language), reliably assigning true/false labels to candidate implications. However, their ability to generate implications of a theory has not yet been demonstrated, and methods for reconstructing proofs of answers are imperfect. In this work we show that a generative model, called ProofWriter, can reliably generate both implications of a theory and the natural language proofs that support them. In particular, iterating a 1-step implication generator results in proofs that are highly reliable, and represent actual model decisions (rather than post-hoc rationalizations). On the RuleTaker dataset, the accuracy of ProofWriter’s proofs exceed previous methods by +9% absolute, and in a way that generalizes to proof depths unseen in training and on out-of-domain problems. We also show that generative techniques can perform a type of abduction with high precision: Given a theory and an unprovable conclusion, identify a missing fact that allows the conclusion to be proved, along with a proof. These results significantly improve the viability of neural methods for systematically reasoning over natural language.111Datasets available at https://allenai.org/data/proofwriter ", "output": "**Decision:** True  \n**Reason:** The paper \"ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language\" satisfies the user query as it directly addresses how pre-trained language models (transformers) can handle reasoning tasks and learn reasoning rules implicitly. The paper demonstrates that the ProofWriter model can emulate logical deduction, generate implications, and reconstruct proofs in natural language, which involves reasoning over ambiguous and complex natural language theories. Furthermore, the ability of the model to generalize to unseen proof depths and out-of-domain problems indicates that it has implicitly learned reasoning rules. While the paper does not explicitly focus on \"ambiguity\" in a broad sense, the handling of natural language theories inherently involves managing ambiguities present in natural language. Thus, the study aligns well with the user's request."}
{"input": "User Query: What papers provide examples for the widespread adoption of VQ-VAE or discrete VAE techniques in multi-modal models?\nTitle: Scaling Autoregressive Models for Content-Rich Text-to-Image Generation\nAbstract:  Abstract We present the Pathways [1] Autoregressive Text-to-Image (Parti) model, which generates high-fidelity photorealistic images and supports content-rich synthesis involving complex compositions and world knowledge. Parti treats text-to-image generation as a sequence-to-sequence modeling problem, akin to machine translation, with sequences of image tokens as the target outputs rather than text tokens in another language. This strategy can naturally tap into the rich body of prior work on large language models, which have seen continued advances in capabilities and performance through scaling data and model sizes. Our approach is simple: First, Parti uses a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. Second, we achieve consistent quality improvements by scaling the encoder-decoder Transformer model up to 20B parameters, with a new state-of-the-art zero-shot FID score of 7.23 and finetuned FID score of 3.22 on MS-COCO. Our detailed analysis on Localized Narratives as well as PartiPrompts (P222), a new holistic benchmark of over 1600 English prompts, demonstrate the effectiveness of Parti across a wide variety of categories and difficulty aspects. We also explore and highlight limitations of our models in order to define and exemplify key areas of focus for further improvements. See parti.research.google for high-resolution images. ", "output": "**Decision:** True  \n**Reason:** The searched paper, \"Scaling Autoregressive Models for Content-Rich Text-to-Image Generation,\" discusses the Parti model, which employs a Transformer-based image tokenizer, ViT-VQGAN, to encode images as sequences of discrete tokens. ViT-VQGAN is a variant of VQ-VAE (Vector Quantized Variational Autoencoder), a discrete VAE technique. The paper describes the use of this technique in a multi-modal model for text-to-image generation, demonstrating its application in a large-scale, content-rich setting. This aligns with the user query, as the paper provides an example of the adoption of VQ-VAE (or discrete VAE) techniques in a multi-modal model."}
{"input": "User Query: In which works did authors consider a multi-agent system with honest and malicious agents in the context of K-armed Multi-Armed Bandits?\nTitle: Robust Multi-Agent Bandits Over Undirected Graphs\nAbstract:  Abstract. We consider a multi-agent multi-armed bandit setting in which n𝑛n honest agents collaborate over a network to minimize regret but m𝑚m malicious agents can disrupt learning arbitrarily. Assuming the network is the complete graph, existing algorithms incur O​((m+K/n)​log⁡(T)/Δ)𝑂𝑚𝐾𝑛𝑇ΔO((m+K/n)\\log(T)/\\Delta) regret in this setting, where K𝐾K is the number of arms and ΔΔ\\Delta is the arm gap. For m≪Kmuch-less-than𝑚𝐾m\\ll K, this improves over the single-agent baseline regret of O​(K​log⁡(T)/Δ)𝑂𝐾𝑇ΔO(K\\log(T)/\\Delta). In this work, we show the situation is murkier beyond the case of a complete graph. In particular, we prove that if the state-of-the-art algorithm is used on the undirected line graph, honest agents can suffer (nearly) linear regret until time is doubly exponential in K𝐾K and n𝑛n. In light of this negative result, we propose a new algorithm for which the i𝑖i-th agent has regret O​((dmal​(i)+K/n)​log⁡(T)/Δ)𝑂subscript𝑑mal𝑖𝐾𝑛𝑇ΔO((d_{\\text{mal}}(i)+K/n)\\log(T)/\\Delta) on any connected and undirected graph, where dmal​(i)subscript𝑑mal𝑖d_{\\text{mal}}(i) is the number of i𝑖i’s neighbors who are malicious. Thus, we generalize existing regret bounds beyond the complete graph (where dmal​(i)=msubscript𝑑mal𝑖𝑚d_{\\text{mal}}(i)=m), and show the effect of malicious agents is entirely local (in the sense that only the dmal​(i)subscript𝑑mal𝑖d_{\\text{mal}}(i) malicious agents directly connected to i𝑖i affect its long-term regret). ", "output": "**Decision:** True  \n**Reason:** The searched paper explicitly addresses a multi-agent system with both honest and malicious agents in the context of K-armed Multi-Armed Bandits. The abstract describes a setting where honest agents collaborate to minimize regret while malicious agents disrupt learning. It also discusses regret bounds and the impact of malicious agents on the system's performance, which aligns with the user's query. Therefore, the paper fully satisfies the detailed requirements of the query."}
{"input": "User Query: Which work expanded the vocabulary of detectors to twenty-thousand classes by leveraging image-level supervision from ImageNet-21K?\nTitle: Detecting Twenty-thousand Classes using Image-level Supervision\nAbstract:  Abstract Current object detectors are limited in vocabulary size due to the small scale of detection datasets. Image classifiers, on the other hand, reason about much larger vocabularies, as their datasets are larger and easier to collect. We propose Detic, which simply trains the classifiers of a detector on image classification data and thus expands the vocabulary of detectors to tens of thousands of concepts. Unlike prior work, Detic does not need complex assignment schemes to assign image labels to boxes based on model predictions, making it much easier to implement and compatible with a range of detection architectures and backbones. Our results show that Detic yields excellent detectors even for classes without box annotations. It outperforms prior work on both open-vocabulary and long-tail detection benchmarks. Detic provides a gain of 2.4 mAP for all classes and 8.3 mAP for novel classes on the open-vocabulary LVIS benchmark. On the standard LVIS benchmark, Detic obtains 41.7 mAP when evaluated on all classes, or only rare classes, hence closing the gap in performance for object categories with few samples. For the first time, we train a detector with all the twenty-one-thousand classes of the ImageNet dataset and show that it generalizes to new datasets without finetuning. Code is available at https://github.com/facebookresearch/Detic. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Detecting Twenty-thousand Classes using Image-level Supervision\" fully satisfies the requirements of the user query. The paper explicitly states that it expands the vocabulary of detectors to twenty-thousand classes by leveraging image-level supervision from ImageNet-21K. The abstract mentions that the proposed method, Detic, trains the classifiers of a detector on image classification data (such as ImageNet-21K) and achieves this expansion of vocabulary. Additionally, the abstract confirms that the method generalizes to new datasets without fine-tuning, which aligns with the query's focus on leveraging ImageNet-21K for this purpose. Therefore, the paper matches the detailed requirements of the user query."}
{"input": "User Query: What study proposed training a detector with CLIP text encoder as language embedding with additional knowledge distillation, named ViLD?\nTitle: Open-vocabulary Object Detection via Vision and Language Knowledge Distillation\nAbstract:  Abstract We aim at advancing open-vocabulary object detection, which detects objects described by arbitrary text inputs. The fundamental challenge is the availability of training data. It is costly to further scale up the number of classes contained in existing object detection datasets. To overcome this challenge, we propose ViLD, a training method via Vision and Language knowledge Distillation. Our method distills the knowledge from a pretrained open-vocabulary image classification model (teacher) into a two-stage detector (student). Specifically, we use the teacher model to encode category texts and image regions of object proposals. Then we train a student detector, whose region embeddings of detected boxes are aligned with the text and image embeddings inferred by the teacher. We benchmark on LVIS by holding out all rare categories as novel categories that are not seen during training. ViLD obtains 16.1 mask APr with a ResNet-50 backbone, even outperforming the supervised counterpart by 3.8. When trained with a stronger teacher model ALIGN, ViLD achieves 26.3 APr. The model can directly transfer to other datasets without finetuning, achieving 72.2 AP50 on PASCAL VOC, 36.6 AP on COCO and 11.8 AP on Objects365. On COCO, ViLD outperforms the previous state-of-the-art (Zareian et al., 2021) by 4.8 on novel AP and 11.4 on overall AP. Code and demo are open-sourced at https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Open-vocabulary Object Detection via Vision and Language Knowledge Distillation\" fully satisfies the requirements of the user query. The paper explicitly proposes a method named ViLD (Vision and Language knowledge Distillation), which involves training a detector using a pretrained open-vocabulary image classification model as a teacher. While the abstract does not explicitly mention the use of the CLIP text encoder, it describes the use of a teacher model to encode category texts and align them with image embeddings, which aligns with the concept of using a text encoder for language embedding. Additionally, the method incorporates knowledge distillation, as specified in the query. Therefore, the paper matches the description provided in the user query."}
{"input": "User Query: Which paper specifically suggests that large batch sizes are essential for effective training in SimCLR?\nTitle: A Simple Framework for Contrastive Learning of Visual Representations\nAbstract:  Abstract This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100×\\times fewer labels. 111Code available at https://github.com/google-research/simclr. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"A Simple Framework for Contrastive Learning of Visual Representations\"* explicitly mentions in its abstract that \"contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning.\" This directly addresses the user query by suggesting that large batch sizes are essential for effective training in SimCLR. The paper systematically studies the components of the SimCLR framework and identifies the importance of larger batch sizes, which aligns with the user's question. Therefore, the paper fully satisfies the detailed requirements of the query."}
{"input": "User Query: Could you provide me some examples of recent generative models capable of 'dreaming up' novel images from the input views' view-angles?\nTitle: Sparse3D: Distilling Multiview-Consistent Diffusion for Object Reconstruction from Sparse Views\nAbstract:  Abstract Reconstructing 3D objects from extremely sparse views is a long-standing and challenging problem. While recent techniques employ image diffusion models for generating plausible images at novel viewpoints or for distilling pre-trained diffusion priors into 3D representations using score distillation sampling (SDS), these methods often struggle to simultaneously achieve high-quality, consistent, and detailed results for both novel-view synthesis (NVS) and geometry. In this work, we present Sparse3D, a novel 3D reconstruction method tailored for sparse view inputs. Our approach distills robust priors from a multiview-consistent diffusion model to refine a neural radiance field. Specifically, we employ a controller that harnesses epipolar features from input views, guiding a pre-trained diffusion model, such as Stable Diffusion, to produce novel-view images that maintain 3D consistency with the input. By tapping into 2D priors from powerful image diffusion models, our integrated model consistently delivers high-quality results, even when faced with open-world objects. To address the blurriness introduced by conventional SDS, we introduce category-score distillation sampling (C-SDS) to enhance detail. We conduct experiments on CO3DV2 which is a multi-view dataset of real-world objects. Both quantitative and qualitative evaluations demonstrate that our approach outperforms previous state-of-the-art works on the metrics regarding NVS and geometry reconstruction. ", "output": "**Decision:** True  \n**Reason:** The paper \"Sparse3D: Distilling Multiview-Consistent Diffusion for Object Reconstruction from Sparse Views\" satisfies the requirements of the user query. The user is looking for generative models capable of \"dreaming up\" novel images from input views' view-angles, which implies generating plausible images of a scene or object from new viewpoints based on sparse input views. Sparse3D achieves this by leveraging a multiview-consistent diffusion model to generate novel-view images that maintain 3D consistency with the input views. The method explicitly focuses on novel-view synthesis (NVS) and employs a pre-trained diffusion model (e.g., Stable Diffusion) to generate high-quality, consistent images from new angles. Additionally, the use of epipolar features and category-score distillation sampling (C-SDS) further enhances the detail and consistency of the generated images. Thus, the paper aligns with the user's requirements."}
{"input": "User Query: Any recent works about breakthrough maximum flow algorithm that extends to solve optimal transport?\nTitle: Maximum Flow and Minimum-Cost Flow in Almost-Linear Time\nAbstract:  Abstract We give an algorithm that computes exact maximum flows and minimum-cost flows on directed graphs with m𝑚m edges and polynomially bounded integral demands, costs, and capacities in m1+o​(1)superscript𝑚1𝑜1m^{1+o(1)} time. Our algorithm builds the flow through a sequence of m1+o​(1)superscript𝑚1𝑜1m^{1+o(1)} approximate undirected minimum-ratio cycles, each of which is computed and processed in amortized mo​(1)superscript𝑚𝑜1m^{o(1)} time using a new dynamic graph data structure. Our framework extends to algorithms running in m1+o​(1)superscript𝑚1𝑜1m^{1+o(1)} time for computing flows that minimize general edge-separable convex functions to high accuracy. This gives almost-linear time algorithms for several problems including entropy-regularized optimal transport, matrix scaling, p𝑝p-norm flows, and p𝑝p-norm isotonic regression on arbitrary directed acyclic graphs. ", "output": "**Decision:** True  \n**Reason:** The paper satisfies the user query as it introduces a breakthrough maximum flow algorithm that operates in almost-linear time and extends to solve problems related to optimal transport. Specifically, the abstract mentions that the algorithm can compute flows minimizing general edge-separable convex functions, which includes entropy-regularized optimal transport as one of its applications. This directly aligns with the user's interest in a maximum flow algorithm that extends to solve optimal transport problems."}
{"input": "User Query: Are there studies that emphasize the socially situated nature of fairness and harms in India?\nTitle: Re-imagining Algorithmic Fairness in India and Beyond\nAbstract:  Abstract. Conventional algorithmic fairness is West-centric, as seen in its sub-groups, values, and methods. In this paper, we de-center algorithmic fairness and analyse AI power in India. Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India, we find that several assumptions of algorithmic fairness are challenged. We find that in India, data is not always reliable due to socio-economic factors, ML makers appear to follow double standards, and AI evokes unquestioning aspiration. We contend that localising model fairness alone can be window dressing in India, where the distance between models and oppressed communities is large. Instead, we re-imagine algorithmic fairness in India and provide a roadmap to re-contextualise data and models, empower oppressed communities, and enable Fair-ML ecosystems. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Re-imagining Algorithmic Fairness in India and Beyond\" fully satisfies the user query. It explicitly emphasizes the socially situated nature of fairness and harms in India by analyzing algorithmic fairness through a localized lens. The study critiques the West-centric assumptions of fairness and highlights how socio-economic factors, unreliable data, and systemic inequalities shape the deployment and perception of AI in India. It also discusses the distance between AI models and oppressed communities, providing a roadmap to address these issues. This aligns well with the user's interest in studies that focus on the socially situated aspects of fairness and harms in the Indian context."}
{"input": "User Query: Which work attempted to use Tweedie’s formula as a denoising step but found it ineffective for nfs and vaes?\nTitle: Denoising Deep Generative Models\nAbstract:  Abstract Likelihood-based deep generative models have recently been shown to exhibit pathological behaviour under the manifold hypothesis as a consequence of using high-dimensional densities to model data with low-dimensional structure. In this paper we propose two methodologies aimed at addressing this problem. Both are based on adding Gaussian noise to the data to remove the dimensionality mismatch during training, and both provide a denoising mechanism whose goal is to sample from the model as though no noise had been added to the data. Our first approach is based on Tweedie’s formula, and the second on models which take the variance of added noise as a conditional input. We show that surprisingly, while well motivated, these approaches only sporadically improve performance over not adding noise, and that other methods of addressing the dimensionality mismatch are more empirically adequate. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Denoising Deep Generative Models\" explicitly discusses the use of Tweedie’s formula as a denoising mechanism. The abstract mentions that one of their proposed methodologies is based on Tweedie’s formula, and they evaluate its effectiveness. The paper also concludes that while the approach is well-motivated, it only sporadically improves performance and is not empirically adequate compared to other methods. This aligns with the user query, which seeks a work that attempted to use Tweedie’s formula as a denoising step but found it ineffective for normalizing flows (nfs) and variational autoencoders (vaes). Therefore, the paper fully satisfies the detailed requirements of the user query."}
{"input": "User Query: Could you provide me any studies that examined the implicit bias in smoothed homogeneous neural network trained by gradient descent with exponentially-tailed losses?\nTitle: Gradient Descent Maximizes the Margin of Homogeneous Neural Networks\nAbstract:  Abstract In this paper, we study the implicit regularization of the gradient descent algorithm in homogeneous neural networks, including fully-connected and convolutional neural networks with ReLU or LeakyReLU activations. In particular, we study the gradient descent or gradient flow (i.e., gradient descent with infinitesimal step size) optimizing the logistic loss or cross-entropy loss of any homogeneous model (possibly non-smooth), and show that if the training loss decreases below a certain threshold, then we can define a smoothed version of the normalized margin which increases over time. We also formulate a natural constrained optimization problem related to margin maximization, and prove that both the normalized margin and its smoothed version converge to the objective value at a KKT point of the optimization problem. Our results generalize the previous results for logistic regression with one-layer or multi-layer linear networks, and provide more quantitative convergence results with weaker assumptions than previous results for homogeneous smooth neural networks. We conduct several experiments to justify our theoretical finding on MNIST and CIFAR-10 datasets. Finally, as margin is closely related to robustness, we discuss potential benefits of training longer for improving the robustness of the model. ", "output": "**Decision:** True  \n**Reason:** The searched paper satisfies the requirements of the user query. The paper explicitly examines the implicit bias in homogeneous neural networks trained by gradient descent, focusing on models with ReLU or LeakyReLU activations. It studies the behavior of gradient descent (or gradient flow) when optimizing exponentially-tailed losses such as logistic loss or cross-entropy loss. The paper also introduces a smoothed version of the normalized margin and analyzes its behavior over time, which aligns with the query's focus on smoothed homogeneous neural networks. Additionally, the paper provides theoretical insights and experimental validation, making it relevant to the user's request."}
{"input": "User Query: Are there any studies about the gains from scaling model size?\nTitle: PaLM: Scaling Language Modeling with Pathways\nAbstract:  Abstract Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540540540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM). We trained PaLM on 614461446144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies. ", "output": "**Decision:** True  \n**Reason:** The paper \"PaLM: Scaling Language Modeling with Pathways\" directly addresses the impact of scaling model size, which aligns with the user query. It specifically investigates the gains achieved by increasing the model size to 540 billion parameters. The paper demonstrates the benefits of scaling through improved performance on various benchmarks, including few-shot learning tasks, multi-step reasoning, multilingual tasks, and source code generation. It also highlights discontinuous improvements in performance as the model scale increases, providing evidence of the gains from scaling. Additionally, the paper includes a comprehensive analysis of related aspects such as bias, toxicity, and training data memorization with respect to model scale, further supporting its relevance to the query."}
{"input": "User Query: Can you provide some examples of research utilizing meta-gradient methods for intrinsic reward learning?\nTitle: On Learning Intrinsic Rewards for Policy Gradient Methods\nAbstract:  Abstract In many sequential decision making tasks, it is challenging to design reward functions that help an RL agent efficiently learn behavior that is considered good by the agent designer. A number of different formulations of the reward-design problem, or close variants thereof, have been proposed in the literature. In this paper we build on the Optimal Rewards Framework of  Singh et al. (2010) that defines the optimal intrinsic reward function as one that when used by an RL agent achieves behavior that optimizes the task-specifying or extrinsic reward function. Previous work in this framework has shown how good intrinsic reward functions can be learned for lookahead search based planning agents. Whether it is possible to learn intrinsic reward functions for learning agents remains an open problem. In this paper we derive a novel algorithm for learning intrinsic rewards for policy-gradient based learning agents. We compare the performance of an augmented agent that uses our algorithm to provide additive intrinsic rewards to an A2C-based policy learner (for Atari games) and a PPO-based policy learner (for Mujoco domains) with a baseline agent that uses the same policy learners but with only extrinsic rewards. Our results show improved performance on most but not all of the domains. ", "output": "**Decision:** True  \n**Reason:** The searched paper satisfies the user's query because it explicitly focuses on learning intrinsic rewards for policy-gradient-based reinforcement learning agents. The paper introduces a novel algorithm for learning intrinsic rewards and applies it to policy-gradient methods such as A2C and PPO. While the abstract does not explicitly mention the use of \"meta-gradient methods,\" the context of learning intrinsic rewards for policy-gradient methods strongly suggests that meta-gradient techniques could be involved, as they are commonly used in this area of research. Therefore, this paper is relevant to the user's query about research utilizing meta-gradient methods for intrinsic reward learning."}
{"input": "User Query: Is there any work that studies text-only interactions in the context of Language-conditioned RL?\nTitle: ALFWorld: Aligning Text and Embodied Environments for Interactive Learning\nAbstract:  Abstract Given a simple request like Put a washed apple in the kitchen fridge, humans can reason in purely abstract terms by imagining action sequences and scoring their likelihood of success, prototypicality, and efficiency, all without moving a muscle. Once we see the kitchen in question, we can update our abstract plans to fit the scene. Embodied agents require the same abilities, but existing work does not yet provide the infrastructure necessary for both reasoning abstractly and executing concretely. We address this limitation by introducing ALFWorld, a simulator that enables agents to learn abstract, text-based policies in TextWorld (Côté et al.,, 2018) and then execute goals from the ALFRED benchmark (Shridhar et al.,, 2020) in a rich visual environment. ALFWorld enables the creation of a new BUTLER agent whose abstract knowledge, learned in TextWorld, corresponds directly to concrete, visually grounded actions. In turn, as we demonstrate empirically, this fosters better agent generalization than training only in the visually grounded environment. BUTLER’s simple, modular design factors the problem to allow researchers to focus on models for improving every piece of the pipeline (language understanding, planning, navigation, and visual scene understanding). ", "output": "**Decision:** True  \n**Reason:** The paper \"ALFWorld: Aligning Text and Embodied Environments for Interactive Learning\" satisfies the requirements of the user query. The user is specifically asking for work that studies text-only interactions in the context of language-conditioned reinforcement learning (RL). The paper introduces ALFWorld, a simulator that enables agents to learn abstract, text-based policies in TextWorld, which is a text-only environment. This directly involves studying text-only interactions in the context of RL, as the agents learn policies conditioned on language instructions in a text-based environment. Furthermore, the paper emphasizes the importance of abstract reasoning in text-based settings before transitioning to visually grounded tasks, aligning with the focus on text-only interactions in language-conditioned RL."}
{"input": "User Query: Which works focus on the generalization ability of SSCL on downstream tasks?\nTitle: Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss\nAbstract:  Abstract Recent works in self-supervised learning have advanced the state-of-the-art by relying on the contrastive learning paradigm, which learns representations by pushing positive pairs, or similar examples from the same class, closer together while keeping negative pairs far apart. Despite the empirical successes, theoretical foundations are limited – prior analyses assume conditional independence of the positive pairs given the same class label, but recent empirical applications use heavily correlated positive pairs (i.e., data augmentations of the same image). Our work analyzes contrastive learning without assuming conditional independence of positive pairs using a novel concept of the augmentation graph on data. Edges in this graph connect augmentations of the same datapoint, and ground-truth classes naturally form connected sub-graphs. We propose a loss that performs spectral decomposition on the population augmentation graph and can be succinctly written as a contrastive learning objective on neural net representations. Minimizing this objective leads to features with provable accuracy guarantees under linear probe evaluation. By standard generalization bounds, these accuracy guarantees also hold when minimizing the training contrastive loss. Empirically, the features learned by our objective can match or outperform several strong baselines on benchmark vision datasets. In all, this work provides the first provable analysis for contrastive learning where guarantees for linear probe evaluation can apply to realistic empirical settings. ", "output": "**Decision:** True  \n**Reason:** The paper explicitly addresses the generalization ability of self-supervised contrastive learning (SSCL) on downstream tasks. It provides provable accuracy guarantees for features learned using a spectral contrastive loss, specifically under linear probe evaluation, which is a common downstream task evaluation method. Furthermore, the paper discusses generalization bounds that ensure these guarantees hold when minimizing the training contrastive loss. This aligns with the user's query about works focusing on the generalization ability of SSCL on downstream tasks."}
{"input": "User Query: What works have influenced the advancements in Large Language Models (LLMs) through Reinforcement Learning from Human Feedback (RLHF)?\nTitle: Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\nAbstract:  Abstract We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback\"* directly addresses advancements in Large Language Models (LLMs) through the application of Reinforcement Learning from Human Feedback (RLHF). It explores the use of RLHF to fine-tune language models for alignment purposes, making them more helpful and harmless. The paper discusses key aspects such as preference modeling, iterative training with human feedback, and the robustness of RLHF training. These contributions align with the user query, as they represent significant works that have influenced the development and refinement of LLMs using RLHF. Additionally, the paper provides analyses and comparisons that further contextualize its impact on the field, making it relevant and satisfying the detailed requirements of the query."}
{"input": "User Query: Which research incorporates an atrous spatial pyramid pooling module in DeeplabV3+?\nTitle: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation\nAbstract:  Abstract Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0% and 82.1% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at https://github.com/tensorflow/models/tree/master/research/deeplab. ", "output": "**Decision:** True  \n**Reason:** The searched paper explicitly discusses the incorporation of an Atrous Spatial Pyramid Pooling (ASPP) module in DeepLabv3+. The abstract mentions that the proposed model, DeepLabv3+, extends DeepLabv3 by adding a decoder module to refine segmentation results and applies depthwise separable convolution to both the ASPP and decoder modules. This directly aligns with the user query, which seeks research that incorporates an ASPP module in DeepLabv3+. Therefore, the paper fully satisfies the detailed requirements of the user query."}
{"input": "User Query: Who analyzed the regularization effect of SAM close to a minimum?\nTitle: The Dynamics of Sharpness-Aware Minimization: Bouncing Across Ravines and Drifting Towards Wide Minima\nAbstract:  Abstract We consider Sharpness-Aware Minimization (SAM), a gradient-based optimization method for deep networks that has exhibited performance improvements on image and language prediction problems. We show that when SAM is applied with a convex quadratic objective, for most random initializations it converges to a cycle that oscillates between either side of the minimum in the direction with the largest curvature, and we provide bounds on the rate of convergence. In the non-quadratic case, we show that such oscillations effectively perform gradient descent, with a smaller step-size, on the spectral norm of the Hessian. In such cases, SAM’s update may be regarded as a third derivative—the derivative of the Hessian in the leading eigenvector direction—that encourages drift toward wider minima. ", "output": "**Decision:** True  \n**Reason:** The searched paper, titled *\"The Dynamics of Sharpness-Aware Minimization: Bouncing Across Ravines and Drifting Towards Wide Minima\"*, directly analyzes the behavior of SAM (Sharpness-Aware Minimization) near a minimum. Specifically, it investigates how SAM oscillates around minima in the direction of the largest curvature and how this behavior effectively regularizes the optimization process by encouraging drift toward wider minima. This analysis aligns with the user's query about understanding the regularization effect of SAM close to a minimum. Therefore, the paper satisfies the requirements of the query."}
{"input": "User Query: Which studies have explored the addition of auxiliary learning objectives to aid the learning of the target task?\nTitle: AANG: Automating Auxiliary learniNG\nAbstract:  Abstract Auxiliary objectives, supplementary learning signals that are introduced to help aid learning on data-starved or highly complex end-tasks, are commonplace in machine learning. Whilst much work has been done to formulate useful auxiliary objectives, their construction is still an art which proceeds by slow and tedious hand-design. Intuition for how and when these objectives improve end-task performance has also had limited theoretical backing. In this work, we present an approach for automatically generating a suite of auxiliary objectives. We achieve this by deconstructing existing objectives within a novel unified taxonomy, identifying connections between them, and generating new ones based on the uncovered structure. Next, we theoretically formalize widely-held intuitions about how auxiliary learning improves generalization on the end-task. This leads us to a principled and efficient algorithm for searching the space of generated objectives to find those most useful to a specified end-task. With natural language processing (NLP) as our domain of study, we demonstrate that our automated auxiliary learning pipeline leads to strong improvements over competitive baselines across continued training experiments on a pre-trained model on 5 NLP tasks 111Code available at : https://github.com/ldery/Automating-Auxiliary-Learning.. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"AANG: Automating Auxiliary learniNG\" satisfies the user query as it explicitly explores the addition of auxiliary learning objectives to aid the learning of the target task. The abstract describes how auxiliary objectives are supplementary learning signals designed to improve performance on complex or data-starved tasks. The paper further investigates the formulation, theoretical understanding, and automatic generation of auxiliary objectives to enhance end-task performance. Additionally, it demonstrates the effectiveness of these auxiliary objectives in improving performance on multiple NLP tasks, which aligns directly with the user's query about studies exploring auxiliary learning objectives to aid target task learning."}
{"input": "User Query: What are the relevant works on developing more generalist models that can address diverse problems and quickly adapt to new tasks?\nTitle: Language Models are Few-Shot Learners\nAbstract:  Abstract Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general. ", "output": "**Decision:** True  \n**Reason:** The paper \"Language Models are Few-Shot Learners\" fully satisfies the user query as it presents a relevant work on developing a generalist model, GPT-3, which is designed to address diverse problems and quickly adapt to new tasks. The paper demonstrates how scaling up language models enables task-agnostic few-shot learning, allowing the model to perform various tasks without task-specific fine-tuning. It highlights GPT-3's ability to handle a wide range of NLP tasks, including translation, question-answering, and domain adaptation tasks, using only a few examples or simple text instructions. This aligns with the user's interest in models that can generalize across diverse problems and adapt quickly to new tasks."}
{"input": "User Query: What research has been done on establishing information-theoretic lower bounds in online RL?\nTitle: Episodic Reinforcement Learning in Finite MDPs: Minimax Lower Bounds Revisited\nAbstract: In this paper, we propose new problem-independent lower bounds on the sample complexity and regret in episodic MDPs, with a particular focus on the non-stationary case in which the transition kernel is allowed to change in each stage of the episode. Our main contribution is a novel lower bound of $\\Omega((H^3SA/\\epsilon^2)\\log(1/\\delta))$ on the sample complexity of an $(\\varepsilon,\\delta)$-PAC algorithm for best policy identification in a non-stationary MDP. This lower bound relies on a construction of \"hard MDPs\" which is different from the ones previously used in the literature. Using this same class of MDPs, we also provide a rigorous proof of the $\\Omega(\\sqrt{H^3SAT})$ regret bound for non-stationary MDPs. Finally, we discuss connections to PAC-MDP lower bounds.", "output": "**Decision:** True  \n**Reason:** The paper \"Episodic Reinforcement Learning in Finite MDPs: Minimax Lower Bounds Revisited\" satisfies the user query as it directly addresses the establishment of information-theoretic lower bounds in the context of online reinforcement learning (RL). Specifically, the paper provides new problem-independent lower bounds on sample complexity and regret in episodic Markov Decision Processes (MDPs). These bounds are derived using information-theoretic techniques, such as constructing \"hard MDPs,\" which is a common approach in deriving such bounds. Additionally, the paper explicitly discusses connections to PAC-MDP lower bounds, further aligning with the user's interest in information-theoretic lower bounds. The focus on both sample complexity and regret in the online RL setting makes this paper relevant and fully aligned with the query."}
{"input": "User Query: Which works use the linear programming (LP) formulation of Reinforcement Learning (RL) to derive DICE algorithms for policy evaluation?\nTitle: DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections\nAbstract:  Abstract In many real-world reinforcement learning applications, access to the environment is limited to a fixed dataset, instead of direct (online) interaction with the environment. When using this data for either evaluation or training of a new policy, accurate estimates of discounted stationary distribution ratios — correction terms which quantify the likelihood that the new policy will experience a certain state-action pair normalized by the probability with which the state-action pair appears in the dataset — can improve accuracy and performance. In this work, we propose an algorithm, DualDICE, for estimating these quantities. In contrast to previous approaches, our algorithm is agnostic to knowledge of the behavior policy (or policies) used to generate the dataset. Furthermore, it eschews any direct use of importance weights, thus avoiding potential optimization instabilities endemic of previous methods. In addition to providing theoretical guarantees, we present an empirical study of our algorithm applied to off-policy policy evaluation and find that our algorithm significantly improves accuracy compared to existing techniques.111Find code at https://github.com/google-research/google-research/tree/master/dual_dice. ", "output": "**Decision:** True  \n**Reason:** The paper \"DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections\" satisfies the requirements of the user query. The user specifically asks for works that use the linear programming (LP) formulation of reinforcement learning (RL) to derive DICE algorithms for policy evaluation. The abstract of the paper indicates that the proposed algorithm, DualDICE, is designed for estimating discounted stationary distribution corrections, which are central to policy evaluation in RL. Furthermore, DICE algorithms are a class of methods that are often derived using optimization frameworks, including LP formulations. Although the abstract does not explicitly mention the term \"linear programming,\" it is well-known in the literature that DICE algorithms, including DualDICE, are often derived using LP or related optimization formulations. Therefore, based on the context and the focus of the paper, it aligns with the user query."}
{"input": "User Query: What research papers are about developing large language models (LLMs)?\nTitle: GPT-4 Technical Report\nAbstract:  Abstract We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4’s performance based on models trained with no more than 1/1,000th the compute of GPT-4. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"GPT-4 Technical Report\" fully satisfies the user query as it directly addresses the development of a large language model (LLM), specifically GPT-4. The abstract discusses the creation of GPT-4, a Transformer-based model, its training process, post-training alignment, and the infrastructure and optimization methods used to develop it. These aspects are central to the development of LLMs, making the paper relevant to the user's query."}
{"input": "User Query: What papers have worked on the development of datasets for hand and object interaction?\nTitle: HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction\nAbstract:  Abstract We present HOI4D, a large-scale 4D egocentric dataset with rich annotations, to catalyze the research of category-level human-object interaction. HOI4D consists of 2.4M RGB-D egocentric video frames over 4000 sequences collected by 4 participants interacting with 800 different object instances from 16 categories over 610 different indoor rooms. Frame-wise annotations for panoptic segmentation, motion segmentation, 3D hand pose, category-level object pose and hand action have also been provided, together with reconstructed object meshes and scene point clouds. With HOI4D, we establish three benchmarking tasks to promote category-level HOI from 4D visual signals including semantic segmentation of 4D dynamic point cloud sequences, category-level object pose tracking, and egocentric action segmentation with diverse interaction targets. In-depth analysis shows HOI4D poses great challenges to existing methods and produces huge research opportunities. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction\" fully satisfies the user query. The paper explicitly focuses on the development of a dataset, HOI4D, which is designed for studying human-object interactions. It provides rich annotations, including 3D hand pose and hand action, which are directly relevant to hand and object interaction. The dataset also includes a large-scale collection of egocentric video frames with detailed annotations, making it a significant contribution to the field of human-object interaction research. Thus, it aligns well with the user's query."}
{"input": "User Query: Which work is about Llama 2-Chat?\nTitle: Llama 2: Open Foundation and Fine-Tuned Chat Models\nAbstract:  Abstract In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs. ", "output": "**Decision:** True  \n**Reason:** The searched paper, titled \"Llama 2: Open Foundation and Fine-Tuned Chat Models,\" explicitly discusses Llama 2-Chat, which is a fine-tuned version of the Llama 2 large language models optimized for dialogue use cases. The abstract specifically mentions Llama 2-Chat, detailing its development, performance, and safety improvements. This directly aligns with the user query asking for work about Llama 2-Chat. Therefore, the paper fully satisfies the requirements of the query."}
{"input": "User Query: What work has been done on RL techniques specifically for machine translation?\nTitle: Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation\nAbstract:  Abstract Policy gradient algorithms have found wide adoption in NLP, but have recently become subject to criticism, doubting their suitability for NMT. Choshen et al. (2020) identify multiple weaknesses and suspect that their success is determined by the shape of output distributions rather than the reward. In this paper, we revisit these claims and study them under a wider range of configurations. Our experiments on in-domain and cross-domain adaptation reveal the importance of exploration and reward scaling, and provide empirical counter-evidence to these claims. ", "output": "**Decision:** True  \n**Reason:** The paper explicitly focuses on reinforcement learning (RL) techniques for neural machine translation (NMT), which directly aligns with the user's query. It revisits criticisms of RL methods in NMT, explores their weaknesses, and evaluates their effectiveness under various configurations. The paper discusses specific aspects of RL, such as policy gradient algorithms, exploration, and reward scaling, in the context of machine translation. Therefore, it satisfies the requirements of the user query."}
{"input": "User Query: Which works proposed the adaptation of large models using bottleneck layers?\nTitle: Parameter-Efficient Transfer Learning for NLP\nAbstract:  Abstract Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter’s effectiveness, we transfer the recently proposed BERT Transformer model to 262626 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4%percent0.40.4\\% of the performance of full fine-tuning, adding only 3.6%percent3.63.6\\% parameters per task. By contrast, fine-tuning trains 100%percent100100\\% of the parameters per task.111Code at https://github.com/google-research/adapter-bert ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Parameter-Efficient Transfer Learning for NLP\" satisfies the user query. The user is asking for works that propose the adaptation of large models using bottleneck layers. The paper introduces \"adapter modules,\" which are lightweight, trainable bottleneck layers added to a pre-trained large model (e.g., BERT). These modules are designed to adapt the model for new tasks while keeping the original model parameters fixed. The use of bottleneck layers (adapter modules) aligns with the concept of parameter-efficient adaptation, as described in the query. Therefore, the paper fully addresses the requirements of the user query."}
{"input": "User Query: Could you provide me papers that tackle the unidentifiability of ICA by incorporating supplementary information?\nTitle: Disentanglement by Nonlinear ICA with General Incompressible-flow Networks (GIN)\nAbstract:  Abstract A central question of representation learning asks under which conditions it is possible to reconstruct the true latent variables of an arbitrarily complex generative process. Recent breakthrough work by Khemakhem et al. (2019) on nonlinear ICA has answered this question for a broad class of conditional generative processes. We extend this important result in a direction relevant for application to real-world data. First, we generalize the theory to the case of unknown intrinsic problem dimension and prove that in some special (but not very restrictive) cases, informative latent variables will be automatically separated from noise by an estimating model. Furthermore, the recovered informative latent variables will be in one-to-one correspondence with the true latent variables of the generating process, up to a trivial component-wise transformation. Second, we introduce a modification of the RealNVP invertible neural network architecture (Dinh et al., 2016) which is particularly suitable for this type of problem: the General Incompressible-flow Network (GIN). Experiments on artificial data and EMNIST demonstrate that theoretical predictions are indeed verified in practice. In particular, we provide a detailed set of exactly 22 informative latent variables extracted from EMNIST. ", "output": "**Decision:** True  \n**Reason:** The paper \"Disentanglement by Nonlinear ICA with General Incompressible-flow Networks (GIN)\" satisfies the user query as it addresses the unidentifiability of Independent Component Analysis (ICA) by incorporating supplementary information. Specifically, it builds on the work of Khemakhem et al. (2019), which introduced a framework for nonlinear ICA that leverages additional information (e.g., auxiliary variables or conditions) to resolve the unidentifiability issue. The paper extends this theoretical foundation by proposing a generalized approach to handle unknown intrinsic dimensions and introduces a novel architecture (GIN) to implement the method. This directly aligns with the user's request for papers that tackle ICA unidentifiability using supplementary information."}
{"input": "User Query: Are there any works demonstrating the dependence of the superiority of the query-selector models on the global and local heterogeneous levels?\nTitle: Re-thinking Federated Active Learning based on Inter-class Diversity\nAbstract:  Abstract Although federated learning has made awe-inspiring advances, most studies have assumed that the client’s data are fully labeled. However, in a real-world scenario, every client may have a significant amount of unlabeled instances. Among the various approaches to utilizing unlabeled data, a federated active learning framework has emerged as a promising solution. In the decentralized setting, there are two types of available query selector models, namely ‘global’ and ‘local-only’ models, but little literature discusses their performance dominance and its causes. In this work, we first demonstrate that the superiority of two selector models depends on the global and local inter-class diversity. Furthermore, we observe that the global and local-only models are the keys to resolving the imbalance of each side. Based on our findings, we propose LoGo, a FAL sampling strategy robust to varying local heterogeneity levels and global imbalance ratio, that integrates both models by two steps of active selection scheme. LoGo consistently outperforms six active learning strategies in the total number of 38 experimental settings. The code is available at: https://github.com/raymin0223/LoGo. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Re-thinking Federated Active Learning based on Inter-class Diversity\"* satisfies the requirements of the user query. The abstract explicitly states that the study demonstrates how the superiority of global and local query-selector models depends on the global and local inter-class diversity (heterogeneous levels). This directly aligns with the user's inquiry about works that explore the dependence of query-selector model performance on global and local heterogeneity. Additionally, the paper goes further by proposing a novel strategy, LoGo, which integrates both models to address varying levels of heterogeneity, reinforcing its relevance to the query."}
{"input": "User Query: What papers focus on improving the encoder’s slot-attention module in unsupervised object-centric learning?\nTitle: Improving Object-centric Learning with Query Optimization\nAbstract:  Abstract The ability to decompose complex natural scenes into meaningful object-centric abstractions lies at the core of human perception and reasoning. In the recent culmination of unsupervised object-centric learning, the Slot-Attention module has played an important role with its simple yet effective design and fostered many powerful variants. These methods, however, have been exceedingly difficult to train without supervision and are ambiguous in the notion of object, especially for complex natural scenes. In this paper, we propose to address these issues by investigating the potential of learnable queries as initializations for Slot-Attention learning, uniting it with efforts from existing attempts on improving Slot-Attention learning with bi-level optimization. With simple code adjustments on Slot-Attention, our model, Bi-level Optimized Query Slot Attention, achieves state-of-the-art results on 3 challenging synthetic and 7 complex real-world datasets in unsupervised image segmentation and reconstruction, outperforming previous baselines by a large margin. We provide thorough ablative studies to validate the necessity and effectiveness of our design. Additionally, our model exhibits great potential for concept binding and zero-shot learning. Our work is made publicly available at https://bo-qsa.github.io. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Improving Object-centric Learning with Query Optimization\" explicitly focuses on improving the Slot-Attention module, which is a key component in unsupervised object-centric learning. The abstract mentions that the authors address issues in Slot-Attention learning by introducing learnable queries as initializations and combining it with bi-level optimization. These enhancements directly target the Slot-Attention module and aim to improve its performance in unsupervised tasks such as image segmentation and reconstruction. Furthermore, the paper demonstrates state-of-the-art results and provides ablative studies to validate its contributions. Therefore, it fully satisfies the user's query."}
{"input": "User Query: Are there any papers about representing the complex human-object-scene with the state-conditional dynamic human model and unbounded background model?\nTitle: HOSNeRF: Dynamic Human-Object-Scene Neural Radiance Fields from a Single Video\nAbstract:  Abstract We introduce HOSNeRF, a novel 360° free-viewpoint rendering method that reconstructs neural radiance fields for dynamic human-object-scene from a single monocular in-the-wild video. Our method enables pausing the video at any frame and rendering all scene details (dynamic humans, objects, and backgrounds) from arbitrary viewpoints. The first challenge in this task is the complex object motions in human-object interactions, which we tackle by introducing the new object bones into the conventional human skeleton hierarchy to effectively estimate large object deformations in our dynamic human-object model. The second challenge is that humans interact with different objects at different times, for which we introduce two new learnable object state embeddings that can be used as conditions for learning our human-object representation and scene representation, respectively. Extensive experiments show that HOSNeRF significantly outperforms SOTA approaches on two challenging datasets by a large margin of 40%∼50%similar-topercent40percent5040\\%\\sim 50\\% in terms of LPIPS. The code, data, and compelling examples of 360° free-viewpoint renderings from single videos will be released in https://showlab.github.io/HOSNeRF. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"HOSNeRF: Dynamic Human-Object-Scene Neural Radiance Fields from a Single Video\" satisfies the requirements of the user query. The query specifically asks for a paper that represents the complex human-object-scene with a state-conditional dynamic human model and an unbounded background model. The paper introduces a method for reconstructing neural radiance fields for dynamic human-object-scene interactions, addressing the complexity of human-object interactions with a state-conditional approach. It incorporates object state embeddings as conditions for learning the human-object and scene representations, aligning with the \"state-conditional dynamic human model\" requirement. Additionally, the method handles the entire scene, including the background, which is consistent with the \"unbounded background model\" aspect of the query. Therefore, the paper aligns well with the detailed requirements."}
{"input": "User Query: Which works discussed the phenomenon of over-smoothing in BERT and explored it through the graph perspective?\nTitle: Revisiting Over-smoothing in BERT from the Perspective of Graph\nAbstract:  Abstract Recently over-smoothing phenomenon of Transformer-based models is observed in both vision and language fields. However, no existing work has delved deeper to further investigate the main cause of this phenomenon. In this work, we make the attempt to analyze the over-smoothing problem from the perspective of graph, where such problem was first discovered and explored. Intuitively, the self-attention matrix can be seen as a normalized adjacent matrix of a corresponding graph. Based on the above connection, we provide some theoretical analysis and find that layer normalization plays a key role in the over-smoothing issue of Transformer-based models. Specifically, if the standard deviation of layer normalization is sufficiently large, the output of Transformer stacks will converge to a specific low-rank subspace and result in over-smoothing. To alleviate the over-smoothing problem, we consider hierarchical fusion strategies, which combine the representations from different layers adaptively to make the output more diverse. Extensive experiment results on various data sets illustrate the effect of our fusion method. ", "output": "**Decision:** True  \n**Reason:** The searched paper explicitly discusses the phenomenon of over-smoothing in BERT and explores it through the graph perspective. The abstract mentions that the authors analyze the over-smoothing problem in Transformer-based models (including BERT) from the perspective of graphs, where the problem was initially studied. They draw a connection between the self-attention matrix and the normalized adjacency matrix of a graph, providing theoretical analysis and proposing solutions. This aligns with the user's query, which seeks works that discuss over-smoothing in BERT and explore it through a graph perspective."}
{"input": "User Query: Which studies have investigated issues of over-fitting in low-resource adaptation for ASR models?\nTitle: Exploiting Adapters for Cross-lingual Low-resource Speech Recognition\nAbstract:  Abstract Cross-lingual speech adaptation aims to solve the problem of leveraging multiple rich-resource languages to build models for a low-resource target language. Since the low-resource language has limited training data, speech recognition models can easily overfit. Adapter is a versatile module that can be plugged into Transformer for parameter-efficient learning. In this paper, we propose to use adapters for parameter-efficient cross-lingual speech adaptation. Based on our previous MetaAdapter that implicitly leverages adapters, we propose a novel algorithm called SimAdapter for explicitly learning knowledge from adapters. Our algorithms can be easily integrated into the Transformer structure. MetaAdapter leverages meta-learning to transfer the general knowledge from training data to the test language. SimAdapter aims to learn the similarities between the source and target languages during fine-tuning using the adapters. We conduct extensive experiments on five-low-resource languages in the Common Voice dataset. Results demonstrate that MetaAdapter and SimAdapter can reduce WER by 2.98% and 2.55% with only 2.5% and 15.5% of trainable parameters compared to the strong full-model fine-tuning baseline. Moreover, we show that these two novel algorithms can be integrated for better performance with up to 3.55% relative WER reduction. ", "output": "**Decision:** True  \n**Reason:** The paper explicitly addresses the issue of overfitting in low-resource adaptation for ASR (Automatic Speech Recognition) models. The abstract mentions that \"since the low-resource language has limited training data, speech recognition models can easily overfit.\" The study proposes the use of adapters (MetaAdapter and SimAdapter) to mitigate this issue through parameter-efficient learning and cross-lingual adaptation. Furthermore, the paper conducts experiments on low-resource languages and demonstrates improvements in performance (WER reduction) while using fewer trainable parameters, which aligns with addressing overfitting concerns. Therefore, the paper satisfies the requirements of the user query."}
{"input": "User Query: Could you provide me some works that combined Local SGD with other methods to improve generalization?\nTitle: Don’t Use Large Mini-Batches, Use Local SGD\nAbstract:  Abstract Mini-batch stochastic gradient methods (SGD) are state of the art for distributed training of deep neural networks. Drastic increases in the mini-batch sizes have lead to key efficiency and scalability gains in recent years. However, progress faces a major roadblock, as models trained with large batches often do not generalize well, i.e. they do not show good accuracy on new data. As a remedy, we propose a post-local SGD and show that it significantly improves the generalization performance compared to large-batch training on standard benchmarks while enjoying the same efficiency (time-to-accuracy) and scalability. We further provide an extensive study of the communication efficiency vs. performance trade-offs associated with a host of local SGD variants. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Don’t Use Large Mini-Batches, Use Local SGD\" satisfies the user query. The user is looking for works that combine Local SGD with other methods to improve generalization. The paper explicitly addresses this by proposing a \"post-local SGD\" approach, which is a variant of Local SGD designed to improve generalization performance. The abstract highlights that this method significantly enhances generalization compared to large-batch training while maintaining efficiency and scalability. Additionally, the paper explores various Local SGD variants, which aligns with the user's interest in methods that combine Local SGD with other techniques to enhance generalization."}
{"input": "User Query: Which papers studied the Lottery ticket hypothesis for pre-trained language models?\nTitle: Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization\nAbstract:  Abstract ", "output": "**Decision:** True  \n**Reason:** The title and abstract of the searched paper indicate that it studies \"Super Tickets\" in pre-trained language models, which is a concept closely related to the Lottery Ticket Hypothesis. The Lottery Ticket Hypothesis focuses on identifying sparse sub-networks (or \"winning tickets\") within a larger neural network that can achieve comparable performance to the original network. The paper's focus on \"Super Tickets\" suggests it explores similar ideas in the context of pre-trained language models, including model compression and generalization improvements. Therefore, it satisfies the user query about studying the Lottery Ticket Hypothesis for pre-trained language models."}
{"input": "User Query: Any works addressing interaction in simulated web environments?\nTitle: WebArena: A Realistic Web Environment for Building Autonomous Agents\nAbstract:  Abstract With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress. Our code, data, environment reproduction resources, and video demonstrations are publicly available at https://webarena.dev/. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"WebArena: A Realistic Web Environment for Building Autonomous Agents\" directly addresses interaction in simulated web environments. It describes the creation of a realistic and reproducible simulated web environment for language-guided agents to perform tasks on the web. The environment includes fully functional websites across various domains (e-commerce, social forums, collaborative software development, and content management) and provides tools and external knowledge bases to mimic real-world web interactions. Furthermore, the paper discusses benchmark tasks and experiments with autonomous agents in this environment, demonstrating its focus on interaction within simulated web environments. This aligns well with the user query."}
{"input": "User Query: Which works employed PointNet families for feature extraction from raw point clouds?\nTitle: PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation\nAbstract:  Abstract Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation\" directly introduces PointNet, a neural network designed to consume raw point clouds for feature extraction. The abstract explicitly states that the network respects the permutation invariance of points in the input and provides a unified architecture for tasks such as object classification, part segmentation, and scene semantic parsing. Since the user query is about works that employed PointNet families for feature extraction from raw point clouds, this paper fully satisfies the query as it is the foundational work introducing PointNet for this purpose."}
{"input": "User Query: Which papers involved the proposal and discussion of Projected Gradient Descent-Adversarial Training (PGD-AT)?\nTitle: Towards Deep Learning Models Resistant to Adversarial Attacks\nAbstract:  Abstract Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples—inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. 111Code and pre-trained models are available at https://github.com/MadryLab/mnist_challenge and https://github.com/MadryLab/cifar10_challenge. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Towards Deep Learning Models Resistant to Adversarial Attacks\"* fully satisfies the user query. This paper is widely recognized as the foundational work that proposed and discussed Projected Gradient Descent-Adversarial Training (PGD-AT). The authors explicitly study adversarial robustness through robust optimization and introduce PGD as a universal and reliable method for generating adversarial examples. They also propose adversarial training using PGD as a defense mechanism, which significantly improves the resistance of neural networks to adversarial attacks. This aligns directly with the user's query about papers involving the proposal and discussion of PGD-AT."}
{"input": "User Query: What papers proposed baseline approaches such as Maximum Softmax Probability and MaxLogit to handle overconfidence problem in deep neural networks?\nTitle: Scaling Out-of-Distribution Detection for Real-World Settings\nAbstract:  Abstract Detecting out-of-distribution examples is important for safety-critical machine learning applications such as detecting novel biological phenomena and self-driving cars. However, existing research mainly focuses on simple small-scale settings. To set the stage for more realistic out-of-distribution detection, we depart from small-scale settings and explore large-scale multiclass and multi-label settings with high-resolution images and thousands of classes. To make future work in real-world settings possible, we create new benchmarks for three large-scale settings. To test ImageNet multiclass anomaly detectors, we introduce the Species dataset containing over 700,000700000700,\\!000 images and over a thousand anomalous species. We leverage ImageNet-21K to evaluate PASCAL VOC and COCO multilabel anomaly detectors. Third, we introduce a new benchmark for anomaly segmentation by introducing a segmentation benchmark with road anomalies. We conduct extensive experiments in these more realistic settings for out-of-distribution detection and find that a surprisingly simple detector based on the maximum logit outperforms prior methods in all the large-scale multi-class, multi-label, and segmentation tasks, establishing a simple new baseline for future work. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"Scaling Out-of-Distribution Detection for Real-World Settings\"* explicitly mentions the use of the \"maximum logit\" as a simple and effective baseline for out-of-distribution detection. The user query specifically asks for papers that propose baseline approaches such as Maximum Softmax Probability and MaxLogit to address the overconfidence problem in deep neural networks. While the paper does not explicitly mention Maximum Softmax Probability, it does propose and evaluate MaxLogit as a baseline approach, which directly aligns with the user query. Therefore, the paper satisfies the requirements of the query."}
{"input": "User Query: Could you provide me with some papers about image-language models trained on large amounts of image-text pairs from the internet?\nTitle: Learning Transferable Visual Models From Natural Language Supervision\nAbstract:  Abstract State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Learning Transferable Visual Models From Natural Language Supervision\"* fully satisfies the user query. The abstract explicitly describes the development of an image-language model trained on a large dataset of 400 million (image, text) pairs collected from the internet. The paper focuses on leveraging natural language supervision to train visual models, which aligns directly with the user's request for research on image-language models trained on large-scale image-text data. Additionally, the paper discusses the scalability, performance, and transferability of the model, further demonstrating its relevance to the query."}
{"input": "User Query: In what paper was the score-based generative model proposed?\nTitle: Score-Based Generative Modeling through Stochastic Differential Equations\nAbstract:  Abstract Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024×1024102410241024\\times 1024 images for the first time from a score-based generative model. ", "output": "**Decision:** True  \n**Reason:** The searched paper, titled *\"Score-Based Generative Modeling through Stochastic Differential Equations\"*, satisfies the requirements of the user query. The abstract explicitly discusses the concept of score-based generative modeling and introduces a framework that leverages stochastic differential equations (SDEs) to transform data distributions. It highlights the use of time-dependent gradient fields (scores) to guide the generative process, which is a core idea in score-based generative modeling. Furthermore, the paper encapsulates and extends previous approaches in score-based generative modeling, making it a foundational work in this area. Therefore, this paper is indeed the one where the score-based generative model was proposed."}
{"input": "User Query: What papers attempted to solve the Bayesian exploration POMDP in the context of meta-reinforcement learning?\nTitle: VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning\nAbstract:  Abstract Trading off exploration and exploitation in an unknown environment is key to maximising expected return during learning. A Bayes-optimal policy, which does so optimally, conditions its actions not only on the environment state but on the agent’s uncertainty about the environment. Computing a Bayes-optimal policy is however intractable for all but the smallest tasks. In this paper, we introduce variational Bayes-Adaptive Deep RL (variBAD), a way to meta-learn to perform approximate inference in an unknown environment, and incorporate task uncertainty directly during action selection. In a grid-world domain, we illustrate how variBAD performs structured online exploration as a function of task uncertainty. We further evaluate variBAD on MuJoCo domains widely used in meta-RL and show that it achieves higher online return than existing methods. ", "output": "**Decision:** True  \n**Reason:** The paper \"VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning\" aligns with the user query as it addresses the Bayesian exploration POMDP in the context of meta-reinforcement learning. The paper explicitly focuses on computing approximate Bayes-optimal policies, which involve balancing exploration and exploitation in uncertain environments. It leverages meta-learning to perform approximate inference and incorporates task uncertainty during action selection, which is a key aspect of solving Bayesian exploration POMDPs. Furthermore, the paper evaluates its method in both grid-world and MuJoCo domains, demonstrating its application in meta-RL. Therefore, it satisfies the detailed requirements of the user query."}
{"input": "User Query: Which works have integrated 2D Vision-Language Models with 3D point cloud processing marking recent progress in open vocabulary scene understanding?\nTitle: ULIP-2: Towards Scalable Multimodal Pre-training for 3D Understanding\nAbstract:  Abstract Recent advancements in multimodal pre-training methods have shown promising efficacy in 3D representation learning by aligning multimodal features across 3D shapes, their 2D counterparts, and language descriptions. However, the methods used by existing multimodal pre-training frameworks to gather multimodal data for 3D applications lack scalability and comprehensiveness, potentially constraining the full potential of multimodal learning. The main bottleneck lies in the language modality’s scalability and comprehensiveness. To address this, we introduce ULIP-2, a tri-modal pre-training framework that leverages state-of-the-art large multimodal models to automatically generate holistic language counterparts for 3D objects. It does not require any 3D annotations, and is therefore scalable to large datasets. We conduct experiments on two large-scale 3D datasets, Objaverse and ShapeNet, and augment them with tri-modal datasets of 3D point clouds, images, and language for training ULIP-2. ULIP-2 achieves significant improvements on downstream zero-shot classification on ModelNet40 (74.0% in top-1 accuracy); on the real-world ScanObjectNN benchmark, it obtains 91.5% in overall accuracy with only 1.4 million parameters, signifying a breakthrough in scalable multimodal 3D representation learning without human 3D annotations. The code, along with the generated tri-modal datasets, can be found at https://github.com/salesforce/ULIP. ", "output": "**Decision:** True  \n**Reason:** The paper \"ULIP-2: Towards Scalable Multimodal Pre-training for 3D Understanding\" satisfies the user query as it explicitly integrates 2D vision-language models with 3D point cloud processing. The framework, ULIP-2, leverages multimodal pre-training by aligning 3D shapes, their 2D counterparts (images), and language descriptions. This tri-modal approach demonstrates advancements in open vocabulary scene understanding by enabling scalable and annotation-free 3D representation learning. The use of large multimodal models to generate language counterparts for 3D objects further highlights its relevance to the query. Additionally, the experiments on large-scale datasets and the significant improvements in downstream tasks underscore its contribution to recent progress in this domain."}
{"input": "User Query: What works have been looking into training compressed models by incorporating resource constraints?\nTitle: Compression-aware Training of Deep Networks\nAbstract:  Abstract In recent years, great progress has been made in a variety of application domains thanks to the development of increasingly deeper neural networks. Unfortunately, the huge number of units of these networks makes them expensive both computationally and memory-wise. To overcome this, exploiting the fact that deep networks are over-parametrized, several compression strategies have been proposed. These methods, however, typically start from a network that has been trained in a standard manner, without considering such a future compression. In this paper, we propose to explicitly account for compression in the training process. To this end, we introduce a regularizer that encourages the parameter matrix of each layer to have low rank during training. We show that accounting for compression during training allows us to learn much more compact, yet at least as effective, models than state-of-the-art compression techniques. ", "output": "**Decision:** True  \n**Reason:** The paper explicitly addresses the topic of training compressed models by incorporating resource constraints. It proposes a method to account for compression during the training process by introducing a regularizer that encourages low-rank parameter matrices in each layer. This approach directly incorporates resource constraints (e.g., memory and computational efficiency) into the training phase, rather than applying compression post-training. Therefore, it aligns with the user query's focus on works that explore training compressed models while considering resource constraints."}
{"input": "User Query: Which paper proposed the architecture of DeepONet?\nTitle: DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators\nAbstract:  Abstract While it is widely known that neural networks are universal approximators of continuous functions, a less known and perhaps more powerful result is that a neural network with a single hidden layer can approximate accurately any nonlinear continuous operator [5]. This universal approximation theorem is suggestive of the potential application of neural networks in learning nonlinear operators from data. However, the theorem guarantees only a small approximation error for a sufficient large network, and does not consider the important optimization and generalization errors. To realize this theorem in practice, we propose deep operator networks (DeepONets) to learn operators accurately and efficiently from a relatively small dataset. A DeepONet consists of two sub-networks, one for encoding the input function at a fixed number of sensors xi,i=1,…,mformulae-sequencesubscript𝑥𝑖𝑖1…𝑚x_{i},i=1,\\dots,m (branch net), and another for encoding the locations for the output functions (trunk net). We perform systematic simulations for identifying two types of operators, i.e., dynamic systems and partial differential equations, and demonstrate that DeepONet significantly reduces the generalization error compared to the fully-connected networks. We also derive theoretically the dependence of the approximation error in terms of the number of sensors (where the input function is defined) as well as the input function type, and we verify the theorem with computational results. More importantly, we observe high-order error convergence in our computational tests, namely polynomial rates (from half order to fourth order) and even exponential convergence with respect to the training dataset size. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"DeepONet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators\"* explicitly proposes the architecture of DeepONet. The abstract clearly states that the authors introduce \"deep operator networks (DeepONets)\" to learn operators accurately and efficiently. It describes the architecture of DeepONet, which consists of two sub-networks: the branch network for encoding the input function and the trunk network for encoding the output locations. Additionally, the paper discusses the theoretical and computational aspects of the architecture, including its performance and error convergence. Therefore, this paper fully satisfies the user query as it is the one that proposed the DeepONet architecture."}
{"input": "User Query: Does any paper point out how Segment Anything Model (SAM) contributes to promptable segmentation and its influence in many downstream tasks?\nTitle: Segment Anything\nAbstract:  Abstract We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive – often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision. ", "output": "**Decision:** True  \n**Reason:** The abstract of the paper titled \"Segment Anything\" explicitly mentions that the Segment Anything Model (SAM) is designed and trained to be \"promptable,\" enabling it to transfer zero-shot to new image distributions and tasks. Furthermore, the abstract highlights that the model's capabilities are evaluated on numerous tasks, demonstrating impressive zero-shot performance, which is often competitive with or superior to prior fully supervised results. This directly addresses the user query regarding SAM's contribution to promptable segmentation and its influence on many downstream tasks. Therefore, the paper satisfies the detailed requirements of the user query."}
{"input": "User Query: Which studies have investigated the phenomena of 'benign overfitting' or 'double-descent' in overparameterized models?\nTitle: Benign Overfitting in Linear Regression\nAbstract:  Abstract The phenomenon of benign overfitting is one of the key mysteries uncovered by deep learning methodology: deep neural networks seem to predict well, even with a perfect fit to noisy training data. Motivated by this phenomenon, we consider when a perfect fit to training data in linear regression is compatible with accurate prediction. We give a characterization of linear regression problems for which the minimum norm interpolating prediction rule has near-optimal prediction accuracy. The characterization is in terms of two notions of the effective rank of the data covariance. It shows that overparameterization is essential for benign overfitting in this setting: the number of directions in parameter space that are unimportant for prediction must significantly exceed the sample size. By studying examples of data covariance properties that this characterization shows are required for benign overfitting, we find an important role for finite-dimensional data: the accuracy of the minimum norm interpolating prediction rule approaches the best possible accuracy for a much narrower range of properties of the data distribution when the data lies in an infinite dimensional space versus when the data lies in a finite dimensional space whose dimension grows faster than the sample size. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Benign Overfitting in Linear Regression\" directly investigates the phenomenon of benign overfitting, which is explicitly mentioned in both the title and the abstract. The abstract discusses the conditions under which linear regression models, particularly overparameterized ones, achieve accurate predictions despite perfectly fitting noisy training data. It also explores the role of overparameterization, which is a key aspect of the user's query. While the paper does not explicitly mention \"double-descent,\" it fully satisfies the query's requirement regarding the study of benign overfitting in overparameterized models."}
{"input": "User Query: What studies concerned with the task of dressing-up in the field of garment manipulation?\nTitle: One Policy to Dress Them All: Learning to Dress People with Diverse Poses and Garments\nAbstract:  Abstract Robot-assisted dressing could benefit the lives of many people such as older adults and individuals with disabilities. Despite such potential, robot-assisted dressing remains a challenging task for robotics as it involves complex manipulation of deformable cloth in 3D space. Many prior works aim to solve the robot-assisted dressing task, but they make certain assumptions such as a fixed garment and a fixed arm pose that limit their ability to generalize. In this work, we develop a robot-assisted dressing system that is able to dress different garments on people with diverse poses from partial point cloud observations, based on a learned policy. We show that with proper design of the policy architecture and Q function, reinforcement learning (RL) can be used to learn effective policies with partial point cloud observations that work well for dressing diverse garments. We further leverage policy distillation to combine multiple policies trained on different ranges of human arm poses into a single policy that works over a wide range of different arm poses. We conduct comprehensive real-world evaluations of our system with 510 dressing trials in a human study with 17 participants with different arm poses and dressed garments. Our system is able to dress 86% of the length of the participants’ arms on average. Videos can be found on our project webpage 111https://sites.google.com/view/one-policy-dress. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"One Policy to Dress Them All: Learning to Dress People with Diverse Poses and Garments\" directly addresses the task of dressing-up in the field of garment manipulation. It focuses on robot-assisted dressing, which is a specific subfield of garment manipulation. The study develops a system capable of dressing individuals with diverse poses and garments using reinforcement learning and policy distillation techniques. The abstract highlights the challenges of manipulating deformable cloth in 3D space and presents a solution that generalizes across different garments and human poses. Furthermore, the paper includes real-world evaluations and human trials, demonstrating its relevance and applicability to the task of dressing-up in garment manipulation. Therefore, it fully satisfies the user query."}
{"input": "User Query: Are there any studies that evaluated the ethical and societal impacts of image generation models?\nTitle: Easily Accessible Text-to-Image Generation Amplifies Demographic Stereotypes at Large Scale\nAbstract:  Abstract Machine learning models that convert user-written text descriptions into images are now widely available online and used by millions of users to generate millions of images a day. We investigate the potential for these models to amplify dangerous and complex stereotypes. We find a broad range of ordinary prompts produce stereotypes, including prompts simply mentioning traits, descriptors, occupations, or objects. For example, we find cases of prompting for basic traits or social roles resulting in images reinforcing whiteness as ideal, prompting for occupations resulting in amplification of racial and gender disparities, and prompting for objects resulting in reification of American norms. Stereotypes are present regardless of whether prompts explicitly mention identity and demographic language or avoid such language. Moreover, stereotypes persist despite mitigation strategies; neither user attempts to counter stereotypes by requesting images with specific counter-stereotypes nor institutional attempts to add system “guardrails” have prevented the perpetuation of stereotypes. Our analysis justifies concerns regarding the impacts of today’s models, presenting striking exemplars, and connecting these findings with deep insights into harms drawn from social scientific and humanist disciplines. This work contributes to the effort to shed light on the uniquely complex biases in language-vision models and demonstrates the ways that the mass deployment of text-to-image generation models results in mass dissemination of stereotypes and resulting harms. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"Easily Accessible Text-to-Image Generation Amplifies Demographic Stereotypes at Large Scale\"* directly evaluates the ethical and societal impacts of image generation models. The abstract explicitly discusses the amplification of harmful stereotypes (a societal impact) by text-to-image generation models and examines their persistence despite mitigation strategies. It also connects these findings to broader ethical concerns, such as the perpetuation of biases and harms. The study provides analysis and examples, situating these issues within social scientific and humanist perspectives, which aligns with the user's query about evaluating ethical and societal impacts."}
{"input": "User Query: Which research introduced the AVSBench-Object and AVSBench-Semantic benchmarks for AVS tasks?\nTitle: Audio-Visual Segmentation with Semantics\nAbstract:  Abstract We propose a new problem called audio-visual segmentation (AVS), in which the goal is to output a pixel-level map of the object(s) that produce sound at the time of the image frame. To facilitate this research, we construct the first audio-visual segmentation benchmark, i.e., AVSBench, providing pixel-wise annotations for sounding objects in audible videos. It contains three subsets: AVSBench-object (Single-source subset, Multi-sources subset) and AVSBench-semantic (Semantic-labels subset). Accordingly, three settings are studied: 1) semi-supervised audio-visual segmentation with a single sound source; 2) fully-supervised audio-visual segmentation with multiple sound sources, and 3) fully-supervised audio-visual semantic segmentation. The first two settings need to generate binary masks of sounding objects indicating pixels corresponding to the audio, while the third setting further requires generating semantic maps indicating the object category. To deal with these problems, we propose a new baseline method that uses a temporal pixel-wise audio-visual interaction module to inject audio semantics as guidance for the visual segmentation process. We also design a regularization loss to encourage audio-visual mapping during training. Quantitative and qualitative experiments on AVSBench compare our approach to several existing methods for related tasks, demonstrating that the proposed method is promising for building a bridge between the audio and pixel-wise visual semantics. Code is available at https://github.com/OpenNLPLab/AVSBench. Online benchmark is available at http://www.avlbench.opennlplab.cn. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Audio-Visual Segmentation with Semantics\" explicitly introduces the AVSBench benchmark, which includes the AVSBench-Object and AVSBench-Semantic subsets. The abstract clearly states that AVSBench consists of three subsets, including AVSBench-Object (Single-source subset, Multi-sources subset) and AVSBench-Semantic (Semantic-labels subset). These subsets are designed for audio-visual segmentation tasks, aligning directly with the user query. Therefore, the paper fully satisfies the detailed requirements of the user query."}
{"input": "User Query: Which works explored visual grounding in 3D scenes?\nTitle: Language Grounding with 3D Objects\nAbstract:  Abstract Seemingly simple natural language requests to a robot are generally underspecified, for example Can you bring me the wireless mouse? Flat images of candidate mice may not provide the discriminative information needed for wireless. The world, and objects in it, are not flat images but complex 3D shapes. If a human requests an object based on any of its basic properties, such as color, shape, or texture, robots should perform the necessary exploration to accomplish the task. In particular, while substantial effort and progress has been made on understanding explicitly visual attributes like color and category, comparatively little progress has been made on understanding language about shapes and contours. In this work, we introduce a novel reasoning task that targets both visual and non-visual language about 3D objects. Our new benchmark ShapeNet Annotated with Referring Expressions (SNARE) requires a model to choose which of two objects is being referenced by a natural language description.111https://github.com/snaredataset/snare We introduce several CLIP-based [1] models for distinguishing objects and demonstrate that while recent advances in jointly modeling vision and language are useful for robotic language understanding, it is still the case that these image-based models are weaker at understanding the 3D nature of objects – properties which play a key role in manipulation. We find that adding view estimation to language grounding models improves accuracy on both SNARE and when identifying objects referred to in language on a robot platform, but note that a large gap remains between these models and human performance. ", "output": "**Decision:** True  \n**Reason:** The paper \"Language Grounding with 3D Objects\" satisfies the user query as it explicitly explores visual grounding in 3D scenes. The abstract discusses grounding natural language descriptions to 3D objects, emphasizing the importance of understanding 3D properties such as shape and contours, which are critical for tasks like object manipulation. The work introduces a benchmark (SNARE) for evaluating models on distinguishing 3D objects based on language descriptions and investigates methods for improving grounding performance in 3D contexts. Thus, the paper aligns well with the user's query about works that explore visual grounding in 3D scenes."}
{"input": "User Query: Are there any works that dealt with the class-imbalance problem of pseudo-labeling in SSL?\nTitle: CReST: A Class-Rebalancing Self-Training Framework for Imbalanced Semi-Supervised Learning\nAbstract:  Abstract Semi-supervised learning on class-imbalanced data, although a realistic problem, has been under studied. While existing semi-supervised learning (SSL) methods are known to perform poorly on minority classes, we find that they still generate high precision pseudo-labels on minority classes. By exploiting this property, in this work, we propose Class-Rebalancing Self-Training (CReST), a simple yet effective framework to improve existing SSL methods on class-imbalanced data. CReST iteratively retrains a baseline SSL model with a labeled set expanded by adding pseudo-labeled samples from an unlabeled set, where pseudo-labeled samples from minority classes are selected more frequently according to an estimated class distribution. We also propose a progressive distribution alignment to adaptively adjust the rebalancing strength dubbed CReST+. We show that CReST and CReST+ improve state-of-the-art SSL algorithms on various class-imbalanced datasets and consistently outperform other popular rebalancing methods. Code has been made available at https://github.com/google-research/crest. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"CReST: A Class-Rebalancing Self-Training Framework for Imbalanced Semi-Supervised Learning\" directly addresses the class-imbalance problem in semi-supervised learning (SSL) with a focus on pseudo-labeling. The abstract explicitly mentions that the method (CReST) is designed to handle class-imbalanced data in SSL by leveraging high-precision pseudo-labels from minority classes and rebalancing the class distribution during training. Additionally, the proposed framework iteratively retrains the model by selecting pseudo-labeled samples from minority classes more frequently, which directly tackles the issue of class imbalance in pseudo-labeling. Therefore, the paper satisfies the requirements of the user query."}
{"input": "User Query: Which research papers focus on the topic of Accuracy-on-the-Line?\nTitle: Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization\nAbstract:  Abstract For machine learning systems to be reliable, we must understand their performance in unseen, out-of-distribution environments. In this paper, we empirically show that out-of-distribution performance is strongly correlated with in-distribution performance for a wide range of models and distribution shifts. Specifically, we demonstrate strong correlations between in-distribution and out-of-distribution performance on variants of CIFAR-10 & ImageNet, a synthetic pose estimation task derived from YCB objects, satellite imagery classification in FMoW-WILDS, and wildlife classification in iWildCam-WILDS. The strong correlations hold across model architectures, hyperparameters, training set size, and training duration, and are more precise than what is expected from existing domain adaptation theory. To complete the picture, we also investigate cases where the correlation is weaker, for instance some synthetic distribution shifts from CIFAR-10-C and the tissue classification dataset Camelyon17-WILDS. Finally, we provide a candidate theory based on a Gaussian data model that shows how changes in the data covariance arising from distribution shift can affect the observed correlations.  ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization\"* directly focuses on the topic of \"Accuracy-on-the-Line.\" The abstract explicitly discusses the strong correlation between in-distribution and out-of-distribution performance, which is the essence of the \"Accuracy-on-the-Line\" concept. The paper empirically demonstrates these correlations across various datasets, models, and distribution shifts, and even proposes a theoretical explanation for the observed phenomena. This aligns precisely with the user query, as the paper is centered on exploring and analyzing the topic of \"Accuracy-on-the-Line.\""}
{"input": "User Query: What papers analyze the effect of clipping in DP-(S)GD?\nTitle: Understanding Gradient Clipping in Private SGD: A Geometric Perspective\nAbstract:  Abstract Deep learning models are increasingly popular in many machine learning applications where the training data may contain sensitive information. To provide formal and rigorous privacy guarantee, many learning systems now incorporate differential privacy by training their models with (differentially) private SGD. A key step in each private SGD update is gradient clipping that shrinks the gradient of an individual example whenever its ℓ2subscriptℓ2\\ell_{2} norm exceeds some threshold. We first demonstrate how gradient clipping can prevent SGD from converging to a stationary point. We then provide a theoretical analysis that fully quantifies the clipping bias on convergence with a disparity measure between the gradient distribution and a geometrically symmetric distribution. Our empirical evaluation further suggests that the gradient distributions along the trajectory of private SGD indeed exhibit symmetric structure that favors convergence. Together, our results provide an explanation why private SGD with gradient clipping remains effective in practice despite its potential clipping bias. Finally, we develop a new perturbation-based technique that can provably correct the clipping bias even for instances with highly asymmetric gradient distributions. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Understanding Gradient Clipping in Private SGD: A Geometric Perspective\" directly analyzes the effect of gradient clipping in differentially private stochastic gradient descent (DP-SGD). The abstract explicitly mentions that the paper investigates how gradient clipping can prevent SGD from converging to a stationary point, provides a theoretical analysis of the clipping bias, and explores its impact on convergence. Furthermore, the paper empirically evaluates gradient distributions in private SGD and develops a technique to correct clipping bias. These aspects align well with the user's query, which seeks papers analyzing the effect of clipping in DP-(S)GD. Therefore, the paper fully satisfies the requirements of the user query."}
{"input": "User Query: What studies are about DDPM which models complex data distributions through discrete steps?\nTitle: Denoising Diffusion Probabilistic Models\nAbstract:  Abstract We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Denoising Diffusion Probabilistic Models\" fully satisfies the user query. The abstract explicitly discusses diffusion probabilistic models (DDPM), which are a class of latent variable models. These models are designed to model complex data distributions through a series of discrete steps, inspired by nonequilibrium thermodynamics. The paper also highlights the connection between DDPM and denoising score matching with Langevin dynamics, which further supports the modeling of complex data distributions. Additionally, the paper's focus on progressive steps (discrete steps) in the diffusion process aligns with the user's query. Therefore, the paper is directly relevant to the user's request."}
{"input": "User Query: Are there any works that provide methods to integrate orthogonal constraints in deep neural networks?\nTitle: Cheap Orthogonal Constraints in Neural Networks: A Simple Parametrization of the Orthogonal and Unitary Group\nAbstract:  Abstract We introduce a novel approach to perform first-order optimization with orthogonal and unitary constraints. This approach is based on a parametrization stemming from Lie group theory through the exponential map. The parametrization transforms the constrained optimization problem into an unconstrained one over a Euclidean space, for which common first-order optimization methods can be used. The theoretical results presented are general enough to cover the special orthogonal group, the unitary group and, in general, any connected compact Lie group. We discuss how this and other parametrizations can be computed efficiently through an implementation trick, making numerically complex parametrizations usable at a negligible runtime cost in neural networks. In particular, we apply our results to rnns with orthogonal recurrent weights, yielding a new architecture called exprnn. We demonstrate how our method constitutes a more robust approach to optimization with orthogonal constraints, showing faster, accurate, and more stable convergence in several tasks designed to test rnns. 111Implementation can be found at https://github.com/Lezcano/expRNN ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Cheap Orthogonal Constraints in Neural Networks: A Simple Parametrization of the Orthogonal and Unitary Group\" satisfies the user query. The paper explicitly introduces a method to integrate orthogonal constraints into deep neural networks by leveraging a novel parametrization based on Lie group theory and the exponential map. This approach transforms constrained optimization problems into unconstrained ones, making it feasible to enforce orthogonal constraints efficiently during training. Furthermore, the paper demonstrates the application of this method in recurrent neural networks (RNNs) with orthogonal recurrent weights, showcasing its practical implementation and effectiveness. Therefore, the paper aligns well with the user's query about methods for integrating orthogonal constraints in deep neural networks."}
{"input": "User Query: What are the works that use data augmentation as regularization by simple transformations or advanced techniques?\nTitle: MaxUp: A Simple Way to Improve Generalization of Neural Network Training\nAbstract:  Abstract We propose MaxUp, an embarrassingly simple, highly effective technique for improving the generalization performance of machine learning models, especially deep neural networks. The idea is to generate a set of augmented data with some random perturbations or transforms, and minimize the maximum, or worst case loss over the augmented data. By doing so, we implicitly introduce a smoothness or robustness regularization against the random perturbations, and hence improve the generation performance. For example, in the case of Gaussian perturbation, MaxUp is asymptotically equivalent to using the gradient norm of the loss as a penalty to encourage smoothness. We test MaxUp on a range of tasks, including image classification, language modeling, and adversarial certification, on which MaxUp consistently outperforms the existing best baseline methods, without introducing substantial computational overhead. In particular, we improve ImageNet classification from the state-of-the-art top-1 accuracy 85.5%percent85.585.5\\% without extra data to 85.8%percent85.885.8\\%. Code will be released soon. ", "output": "**Decision:** True  \n**Reason:** The paper \"MaxUp: A Simple Way to Improve Generalization of Neural Network Training\" satisfies the user query as it explicitly discusses the use of data augmentation as a regularization technique. The proposed method, MaxUp, involves generating augmented data through random perturbations or transformations and then minimizing the maximum loss over the augmented data. This approach introduces robustness regularization, which aligns with the query's focus on using data augmentation for regularization purposes. Additionally, the paper mentions both simple transformations (e.g., random perturbations) and advanced techniques (e.g., adversarial certification), further satisfying the detailed requirements of the query."}
{"input": "User Query: What research references focus on quantifying the uncertainty of model’s predictions toward next states and rewards?\nTitle: Latent Offline Model-Based Policy Optimization\nAbstract:  Abstract Offline reinforcement learning (RL) has shown promise in learning policies from a prerecorded dataset without any interaction with the environment, and has the potential to significantly improve the scalability and safety of policy learning. Recent advances in offline model-based RL have further improved previous offline model-free approaches by enabling greater generalization to new states. These advantages make offline model-based RL particularly appealing for learning complex skills from raw sensor observations, such as images, since learning visuomotor policies using deep RL methods is much more sample-inefficient and unsafe (e.g. failure to generalize to lighting changes) than learning from low-dimensional state inputs. Hence, learning vision-based tasks end-to-end from offline data becomes crucial for robotics and control. However, offline model-based RL from visual inputs is challenging because the offline model-based RL problem critically relies on accurate uncertainty quantification of the model’s predictions to avoid falling off the data distribution and estimating uncertainty of high-dimensional visual dynamics models such as learning an ensemble of video prediction models is known to be hard and computationally inefficient. To address these challenges, we propose a new offline model-based RL algorithm that learns a latent dynamics model from images, augments the offline dataset using latent model predictions, and optimizes a policy using the augmented dataset. Crucially, to quantify and leverage uncertainty, we propose to represent model uncertainty in the latent space, construct a penalized MDP w.r.t. this uncertainty, and optimize a policy in the latent penalized MDP. This approach is both tractable in practice and theoretically equivalent to maximizing a lower bound of the average return in the MDP in the latent space whose dynamics are induced by the true transition model in the original observation space. Based on these findings, our method, Latent Offline Model-based Policy Optimization (LOMPO) adopts an ensemble of latent dynamics models and penalizes the reward with the ensemble disagreement. Our approach significantly outperforms previous offline model-free RL methods as well as state-of-the-art online visual model-based RL methods in a range of image-based locomotion and robotic manipulation tasks. Moreover, we also find that our algorithm excels in several image-based multi-task domains.  ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Latent Offline Model-Based Policy Optimization\" directly addresses the quantification of uncertainty in model predictions, particularly in the context of offline model-based reinforcement learning. The abstract explicitly mentions the challenge of accurately quantifying uncertainty in high-dimensional visual dynamics models and proposes a solution that involves representing model uncertainty in the latent space. The method uses an ensemble of latent dynamics models and penalizes the reward based on ensemble disagreement, which is a standard approach to quantify and leverage uncertainty in model predictions. This aligns well with the user query, as the paper focuses on quantifying uncertainty related to the model's predictions of next states and rewards."}
{"input": "User Query: What studies mentioned the issue of slow training and rendering speed in Neural radiance fields (NeRF)?\nTitle: Instant Neural Graphics Primitives with a Multiresolution Hash Encoding\nAbstract:  Abstract. Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920×108019201080{1920\\!\\times\\!1080}. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Instant Neural Graphics Primitives with a Multiresolution Hash Encoding\" explicitly mentions the issue of slow training and rendering speeds in neural graphics primitives, which includes Neural Radiance Fields (NeRF). The abstract highlights that neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. This directly aligns with the user query regarding studies that mention the issue of slow training and rendering speed in NeRF. Furthermore, the paper proposes a solution to this problem by introducing a multiresolution hash encoding that significantly reduces training and rendering times. Thus, the paper satisfies the detailed requirements of the user query."}
{"input": "User Query: Can you cite some studies that have carried out work on domain-generalization through the use of weight averaging on models?\nTitle: Diverse Weight Averaging for Out-of-Distribution Generalization\nAbstract:  Abstract Standard neural networks struggle to generalize under distribution shifts in computer vision. Fortunately, combining multiple networks can consistently improve out-of-distribution generalization. In particular, weight averaging (WA) strategies were shown to perform best on the competitive DomainBed benchmark; they directly average the weights of multiple networks despite their nonlinearities. In this paper, we propose Diverse Weight Averaging (DiWA), a new WA strategy whose main motivation is to increase the functional diversity across averaged models. To this end, DiWA averages weights obtained from several independent training runs: indeed, models obtained from different runs are more diverse than those collected along a single run thanks to differences in hyperparameters and training procedures. We motivate the need for diversity by a new bias-variance-covariance-locality decomposition of the expected error, exploiting similarities between WA and standard functional ensembling. Moreover, this decomposition highlights that WA succeeds when the variance term dominates, which we show occurs when the marginal distribution changes at test time. Experimentally, DiWA consistently improves the state of the art on DomainBed without inference overhead. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Diverse Weight Averaging for Out-of-Distribution Generalization\" satisfies the user query as it explicitly addresses the use of weight averaging (WA) strategies for domain generalization. The paper discusses how WA strategies, including their proposed method (DiWA), improve out-of-distribution generalization, which is a key aspect of domain generalization. The abstract highlights that WA directly averages the weights of multiple networks and demonstrates its effectiveness on the DomainBed benchmark, a standard for evaluating domain generalization methods. Additionally, the paper introduces a novel approach to enhance the diversity of averaged models, further contributing to domain generalization research using weight averaging. Thus, it fully aligns with the requirements of the user query."}
{"input": "User Query: Could you provide me some studies about the cross-attention operation based on the attention mechanism for object tracking?\nTitle: AiATrack: Attention in Attention for Transformer Visual Tracking\nAbstract:  Abstract Transformer trackers have achieved impressive advancements recently, where the attention mechanism plays an important role. However, the independent correlation computation in the attention mechanism could result in noisy and ambiguous attention weights, which inhibits further performance improvement. To address this issue, we propose an attention in attention (AiA) module, which enhances appropriate correlations and suppresses erroneous ones by seeking consensus among all correlation vectors. Our AiA module can be readily applied to both self-attention blocks and cross-attention blocks to facilitate feature aggregation and information propagation for visual tracking. Moreover, we propose a streamlined Transformer tracking framework, dubbed AiATrack, by introducing efficient feature reuse and target-background embeddings to make full use of temporal references. Experiments show that our tracker achieves state-of-the-art performance on six tracking benchmarks while running at a real-time speed. Code and models are publicly available at https://github.com/Little-Podi/AiATrack. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"AiATrack: Attention in Attention for Transformer Visual Tracking\" fully satisfies the user query. The user specifically asked for studies about the cross-attention operation based on the attention mechanism for object tracking. The abstract of the paper explicitly mentions the use of cross-attention blocks in the proposed \"AiA module\" to facilitate feature aggregation and information propagation for visual tracking. This indicates that the paper addresses cross-attention mechanisms in the context of object tracking. Furthermore, the paper focuses on improving the attention mechanism, which aligns with the user's interest in studies related to attention mechanisms for object tracking."}
{"input": "User Query: Which papers argued that disentanglement is not necessary for style edit?\nTitle: Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent Representation\nAbstract:  Abstract Unsupervised text attribute transfer automatically transforms a text to alter a specific attribute (e.g. sentiment) without using any parallel data, while simultaneously preserving its attribute-independent content. The dominant approaches are trying to model the content-independent attribute separately, e.g., learning different attributes’ representations or using multiple attribute-specific decoders. However, it may lead to inflexibility from the perspective of controlling the degree of transfer or transferring over multiple aspects at the same time. To address the above problems, we propose a more flexible unsupervised text attribute transfer framework which replaces the process of modeling attribute with minimal editing of latent representations based on an attribute classifier. Specifically, we first propose a Transformer-based autoencoder to learn an entangled latent representation for a discrete text, then we transform the attribute transfer task to an optimization problem and propose the Fast-Gradient-Iterative-Modification algorithm to edit the latent representation until conforming to the target attribute. Extensive experimental results demonstrate that our model achieves very competitive performance on three public data sets. Furthermore, we also show that our model can not only control the degree of transfer freely but also allow transferring over multiple aspects at the same time.111Our codes are available at https://github.com/Nrgeup/controllable-text-attribute-transfer ", "output": "**Decision:** True  \n**Reason:** The searched paper, titled *\"Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent Representation\"*, explicitly argues against the necessity of disentanglement for style editing. The abstract highlights that the proposed framework learns an **entangled latent representation** for text and performs attribute transfer by directly editing this entangled representation. This approach contrasts with dominant methods that rely on disentangling content and attribute representations. By demonstrating competitive performance and flexibility in style editing without disentanglement, the paper aligns with the user's query about arguments against the necessity of disentanglement for style editing."}
{"input": "User Query: In which papers have the researchers considered Video Anomaly Detection as an unsupervised learning task?\nTitle: Generative Cooperative Learning for Unsupervised Video Anomaly Detection\nAbstract:  Abstract Video anomaly detection is well investigated in weakly-supervised and one-class classification (OCC) settings. However, unsupervised video anomaly detection methods are quite sparse, likely because anomalies are less frequent in occurrence and usually not well-defined, which when coupled with the absence of ground truth supervision, could adversely affect the performance of the learning algorithms. This problem is challenging yet rewarding as it can completely eradicate the costs of obtaining laborious annotations and enable such systems to be deployed without human intervention. To this end, we propose a novel unsupervised Generative Cooperative Learning (GCL) approach for video anomaly detection that exploits the low frequency of anomalies towards building a cross-supervision between a generator and a discriminator. In essence, both networks get trained in a cooperative fashion, thereby allowing unsupervised learning. We conduct extensive experiments on two large-scale video anomaly detection datasets, UCF crime and ShanghaiTech. Consistent improvement over the existing state-of-the-art unsupervised and OCC methods corroborate the effectiveness of our approach. ", "output": "**Decision:** True  \n**Reason:** The searched paper explicitly addresses video anomaly detection as an unsupervised learning task. The abstract states that the authors propose a \"novel unsupervised Generative Cooperative Learning (GCL) approach for video anomaly detection.\" It also highlights the absence of ground truth supervision and the challenges associated with this, which aligns with the definition of unsupervised learning. Furthermore, the paper demonstrates the application of their method to video anomaly detection datasets, confirming its relevance to the user query. Thus, the paper fully satisfies the detailed requirements of the query."}
{"input": "User Query: Are there any theoretical advances providing an understanding of SAM?\nTitle: The Dynamics of Sharpness-Aware Minimization: Bouncing Across Ravines and Drifting Towards Wide Minima\nAbstract:  Abstract We consider Sharpness-Aware Minimization (SAM), a gradient-based optimization method for deep networks that has exhibited performance improvements on image and language prediction problems. We show that when SAM is applied with a convex quadratic objective, for most random initializations it converges to a cycle that oscillates between either side of the minimum in the direction with the largest curvature, and we provide bounds on the rate of convergence. In the non-quadratic case, we show that such oscillations effectively perform gradient descent, with a smaller step-size, on the spectral norm of the Hessian. In such cases, SAM’s update may be regarded as a third derivative—the derivative of the Hessian in the leading eigenvector direction—that encourages drift toward wider minima. ", "output": "**Decision:** True  \n**Reason:** The paper titled *\"The Dynamics of Sharpness-Aware Minimization: Bouncing Across Ravines and Drifting Towards Wide Minima\"* provides theoretical insights into the behavior of Sharpness-Aware Minimization (SAM). Specifically, it analyzes the dynamics of SAM in both convex quadratic and non-quadratic settings. The paper offers theoretical advances by explaining how SAM converges to cycles in the quadratic case and how it effectively performs gradient descent on the spectral norm of the Hessian in non-quadratic cases. Furthermore, it introduces the concept of SAM's update acting as a third derivative that encourages drift toward wider minima, which is a novel theoretical understanding. These contributions align with the user query seeking theoretical advances in understanding SAM."}
{"input": "User Query: Which studies explored the area of GANs, one of the classes of parametric generative models?\nTitle: Generative Adversarial Nets\nAbstract:  Abstract We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G𝐺G that captures the data distribution, and a discriminative model D𝐷D that estimates the probability that a sample came from the training data rather than G𝐺G. The training procedure for G𝐺G is to maximize the probability of D𝐷D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G𝐺G and D𝐷D, a unique solution exists, with G𝐺G recovering the training data distribution and D𝐷D equal to 1212\\frac{1}{2} everywhere. In the case where G𝐺G and D𝐷D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Generative Adversarial Nets\" fully satisfies the user query. The abstract explicitly describes the proposal of a new framework for estimating generative models using an adversarial process, which is the foundational concept behind Generative Adversarial Networks (GANs). GANs are a class of parametric generative models, as they involve training two neural networks (parametric models) in a minimax game to generate data that mimics the training distribution. The paper directly explores the area of GANs and provides both theoretical and experimental insights into their functioning, making it relevant to the query."}
{"input": "User Query: Could you provide me some works related to the alternative metric, GEval, used for assessing NLG tasks?\nTitle: G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment\nAbstract:  Abstract The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.5140.5140.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose analysis on the behavior of LLM-based evaluators, and highlight the potential concern of LLM-based evaluators having a bias towards the LLM-generated texts. 111https://github.com/nlpyang/geval ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment\" directly addresses the user's query. It introduces and discusses GEval, an alternative metric for assessing natural language generation (NLG) tasks. The paper describes the framework of GEval, its methodology, and its application to NLG evaluation tasks such as text summarization and dialogue generation. Furthermore, it provides experimental results demonstrating the effectiveness of GEval in comparison to other metrics. This aligns with the user's request for works related to the alternative metric GEval used for assessing NLG tasks."}
{"input": "User Query: Which works focus on learning a compact representation using a representation learning objective in high-dimensional observations?\nTitle: Dream to Control: Learning Behaviors by Latent Imagination\nAbstract:  Abstract Learned world models summarize an agent’s experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance. ", "output": "**Decision:** True  \n**Reason:** The paper \"Dream to Control: Learning Behaviors by Latent Imagination\" satisfies the user query as it focuses on learning a compact representation using a representation learning objective in high-dimensional observations. Specifically, the paper describes the use of a learned world model to summarize high-dimensional sensory inputs (e.g., images) into a compact latent state space. This compact representation is then used for reinforcement learning by imagining trajectories in the latent space. The process of learning the world model inherently involves a representation learning objective to encode high-dimensional observations into a more compact and useful form. Therefore, the paper aligns with the requirements of the user query."}
{"input": "User Query: What works made significant progress of Large Language Models (LLMs), especially those using a Transformer-decoder architecture?\nTitle: OPT: Open Pre-trained Transformer Language Models\nAbstract:  Abstract Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3,111Following Brown et al. (2020), we use GPT-3 to refer to both the 175B model and the smaller scale models as well. while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models. ", "output": "**Decision:** True  \n**Reason:** The paper \"OPT: Open Pre-trained Transformer Language Models\" satisfies the user query as it directly addresses advancements in large language models (LLMs) using a Transformer-decoder architecture. The paper introduces OPT, a suite of decoder-only pre-trained transformers, which is explicitly aligned with the Transformer-decoder architecture mentioned in the query. It highlights significant progress in the field by comparing OPT-175B to GPT-3, a well-known Transformer-decoder model, and demonstrates comparable performance while achieving a more efficient training process (e.g., 1/7th the carbon footprint). Additionally, the paper contributes to the field by openly sharing model weights, infrastructure details, and code, which promotes further research and understanding of LLMs. Therefore, it meets the detailed requirements of the query."}
{"input": "User Query: What TTA studies constructed self-supervised auxiliary tasks to learn on test data?\nTitle: Test-Time Training with Masked Autoencoders\nAbstract:  Abstract Test-time training adapts to a new test distribution on the fly by optimizing a model for each test input using self-supervision. In this paper, we use masked autoencoders for this one-sample learning problem. Empirically, our simple method improves generalization on many visual benchmarks for distribution shifts. Theoretically, we characterize this improvement in terms of the bias-variance trade-off. ", "output": "**Decision:** True  \n**Reason:** The paper \"Test-Time Training with Masked Autoencoders\" satisfies the requirements of the user query. The user is asking for studies that constructed self-supervised auxiliary tasks to learn on test data. According to the abstract, the paper explicitly mentions using masked autoencoders as a self-supervised method for test-time training (TTA). This involves optimizing a model for each test input using self-supervision, which directly aligns with the user's query about constructing self-supervised auxiliary tasks to adapt to test data."}
{"input": "User Query: What are some references that discuss model learning through stochastic gradient ascent on an evidence lower bound (ELBO)?\nTitle: Stochastic Variational Inference\nAbstract:  Abstract We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets. ", "output": "**Decision:** True  \n**Reason:** The paper \"Stochastic Variational Inference\" satisfies the user query because it discusses stochastic variational inference, which is a method for approximating posterior distributions in Bayesian models. This method involves optimizing the evidence lower bound (ELBO) using stochastic gradient ascent, as it is a core component of variational inference. The abstract explicitly mentions the development of a scalable algorithm for approximating posterior distributions, which aligns with the concept of optimizing the ELBO. Therefore, the paper is relevant to the user's query about model learning through stochastic gradient ascent on an ELBO."}
{"input": "User Query: What studies focus on addressing the encoding discontinuity by handling the Periodicity of Angular (PoA)?\nTitle: Rethinking Boundary Discontinuity Problem for Oriented Object Detection\nAbstract:  Abstract Oriented object detection has been developed rapidly in the past few years, where rotation equivariant is crucial for detectors to predict rotated bounding boxes. It is expected that the prediction can maintain the corresponding rotation when objects rotate, but severe mutational in angular prediction is sometimes observed when objects rotate near the boundary angle, which is well-known boundary discontinuity problem. The problem has been long believed to be caused by the sharp loss increase at the angular boundary during training, and widely used IoU-like loss generally deal with this problem by loss-smoothing. However, we experimentally find that even state-of-the-art IoU-like methods do not actually solve the problem. On further analysis, we find the essential cause of the problem lies at discontinuous angular ground-truth(box), not just discontinuous loss. There always exists an irreparable gap between continuous model ouput and discontinuous angular ground-truth, so angular prediction near the breakpoints becomes highly unstable, which cannot be eliminated just by loss-smoothing in IoU-like methods. To thoroughly solve this problem, we propose a simple and effective Angle Correct Module (ACM) based on polar coordinate decomposition. ACM can be easily plugged into the workflow of oriented object detectors to repair angular prediction. It converts the smooth value of the model output into sawtooth angular value, and then IoU-like loss can fully release their potential. Extensive experiments on multiple datasets show that whether Gaussian-based or SkewIoU methods are improved to the same performance of AP50 and AP75 with the enhancement of ACM. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Rethinking Boundary Discontinuity Problem for Oriented Object Detection\" directly addresses the issue of encoding discontinuity in angular predictions, which is closely related to the Periodicity of Angular (PoA). The abstract explicitly discusses the boundary discontinuity problem in angular predictions caused by the discontinuous angular ground truth and proposes a solution through the Angle Correct Module (ACM). This module leverages polar coordinate decomposition to handle angular periodicity effectively, ensuring stable and accurate angular predictions. Therefore, the paper satisfies the requirements of the user query by focusing on addressing encoding discontinuity and handling angular periodicity."}
{"input": "User Query: Which works are about architectural improvements beyond the conventional feed-forward style, such as FiLM or Ada-In layers?\nTitle: A Style-Based Generator Architecture for Generative Adversarial Networks\nAbstract:  Abstract We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces. ", "output": "**Decision:** True  \n**Reason:** The paper \"A Style-Based Generator Architecture for Generative Adversarial Networks\" proposes an alternative generator architecture for GANs that borrows from the style transfer literature. This architecture introduces a style-based mechanism that allows for scale-specific control of synthesis and disentanglement of high-level attributes and stochastic variations. This approach goes beyond the conventional feed-forward style by incorporating architectural innovations inspired by style transfer techniques, which are conceptually similar to methods like FiLM or Ada-In layers in that they modulate features in a learned and controlled manner. Therefore, the paper satisfies the user's query about architectural improvements beyond the conventional feed-forward style."}
{"input": "User Query: Which studies proposed certified variants of adversarial training?\nTitle: Training Certifiably Robust Neural Networks with Efficient Local Lipschitz Bounds\nAbstract:  Abstract Certified robustness is a desirable property for deep neural networks in safety-critical applications, and popular training algorithms can certify robustness of a neural network by computing a global bound on its Lipschitz constant. However, such a bound is often loose: it tends to over-regularize the neural network and degrade its natural accuracy. A tighter Lipschitz bound may provide a better tradeoff between natural and certified accuracy, but is generally hard to compute exactly due to non-convexity of the network. In this work, we propose an efficient and trainable local Lipschitz upper bound by considering the interactions between activation functions (e.g. ReLU) and weight matrices. Specifically, when computing the induced norm of a weight matrix, we eliminate the corresponding rows and columns where the activation function is guaranteed to be a constant in the neighborhood of each given data point, which provides a provably tighter bound than the global Lipschitz constant of the neural network. Our method can be used as a plug-in module to tighten the Lipschitz bound in many certifiable training algorithms. Furthermore, we propose to clip activation functions (e.g., ReLU and MaxMin) with a learnable upper threshold and a sparsity loss to assist the network to achieve an even tighter local Lipschitz bound. Experimentally, we show that our method consistently outperforms state-of-the-art methods in both clean and certified accuracy on MNIST, CIFAR-10 and TinyImageNet datasets with various network architectures. ", "output": "**Decision:** True  \n**Reason:** The paper discusses the development of a method to train certifiably robust neural networks by proposing an efficient and trainable local Lipschitz upper bound. This approach is directly related to certified robustness, which is a key aspect of certified adversarial training. The method can be integrated into certifiable training algorithms, which implies that the study contributes to the development of certified variants of adversarial training. Therefore, the paper satisfies the requirements of the user query."}
{"input": "User Query: Can you list the papers that first introduced the Equivariant networks as G-Convolution and Steerable CNN?\nTitle: Group Equivariant Convolutional Networks\nAbstract:  Abstract We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"Group Equivariant Convolutional Networks\"* introduces Group Equivariant Convolutional Neural Networks (G-CNNs) and explicitly discusses G-convolutions, which are a foundational concept in equivariant networks. This paper is widely recognized as the first to introduce equivariant networks in the context of G-convolutions. The abstract clearly states the introduction of G-CNNs as a generalization of traditional CNNs to exploit symmetries, which aligns with the user query regarding the introduction of equivariant networks as G-convolutions. Therefore, the paper satisfies the requirements of the query."}
{"input": "User Query: Which papers were concerned with the method of proxy-guided decoding?\nTitle: Tuning Language Models by Proxy\nAbstract:  Abstract Despite the general capabilities of large pretrained language models, they consistently benefit from further adaptation to better achieve desired behaviors. However, tuning these models has become increasingly resource-intensive, or impossible when model weights are private. We introduce proxy-tuning, a lightweight decoding-time algorithm that operates on top of black-box LMs to achieve the result of directly tuning the model, but by accessing only its prediction over the output vocabulary. Our method instead tunes a smaller LM, then applies the difference between the predictions of the small tuned and untuned LMs to shift the original predictions of the base model in the direction of tuning, while retaining the benefits of larger-scale pretraining. In experiments, when we apply proxy-tuning to Llama2-70B using proxies of only 7B size, we can close 88% of the gap between Llama2-70B and its truly-tuned chat version, when evaluated across knowledge, reasoning, and safety benchmarks. Interestingly, when tested on TruthfulQA, proxy-tuned models are actually more truthful than directly tuned models, possibly because decoding-time guidance better retains the model’s factual knowledge. We then demonstrate the generality of proxy-tuning by applying it for domain adaptation on code, and task-specific finetuning on question-answering and math problems. Our work demonstrates the promise of using small tuned LMs to efficiently customize large, potentially proprietary LMs through decoding-time guidance. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Tuning Language Models by Proxy\" fully satisfies the user query. The abstract explicitly describes a method called \"proxy-tuning,\" which involves a lightweight decoding-time algorithm that guides the predictions of a large language model (LM) using a smaller, tuned proxy LM. This approach aligns with the concept of \"proxy-guided decoding,\" as it uses the smaller proxy LM to guide the decoding process of the larger model. The paper discusses how this method is applied to achieve desired behaviors in large LMs without directly tuning their weights, which is consistent with the idea of proxy-guided decoding. Therefore, the paper is directly concerned with the method of proxy-guided decoding."}
{"input": "User Query: Which papers have used OT for clustering tasks?\nTitle: Self-labelling via simultaneous clustering and representation learning\nAbstract:  Abstract Combining clustering and representation learning is one of the most promising approaches for unsupervised learning of deep neural networks. However, doing so naively leads to ill posed learning problems with degenerate solutions. In this paper, we propose a novel and principled learning formulation that addresses these issues. The method is obtained by maximizing the information between labels and input data indices. We show that this criterion extends standard cross-entropy minimization to an optimal transport problem, which we solve efficiently for millions of input images and thousands of labels using a fast variant of the Sinkhorn-Knopp algorithm. The resulting method is able to self-label visual data so as to train highly competitive image representations without manual labels. Our method achieves state of the art representation learning performance for AlexNet and ResNet-50 on SVHN, CIFAR-10, CIFAR-100 and ImageNet and yields the first self-supervised AlexNet that outperforms the supervised Pascal VOC detection baseline. Code and models are available111https://github.com/yukimasano/self-label. ", "output": "**Decision:** True  \n**Reason:** The paper explicitly mentions the use of Optimal Transport (OT) for clustering tasks. In the abstract, it states that the proposed method extends standard cross-entropy minimization to an optimal transport problem, which is solved efficiently using a fast variant of the Sinkhorn-Knopp algorithm. Additionally, the method combines clustering and representation learning, which aligns with the user's query about using OT for clustering tasks. Therefore, the paper satisfies the detailed requirements of the query."}
{"input": "User Query: What research has seen success in text-to-image generation using diffusion models?\nTitle: Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding\nAbstract:  Abstract We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, GLIDE and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See imagen.research.google for an overview of the results. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding\" directly addresses the user query by presenting research on the success of text-to-image generation using diffusion models. The paper introduces \"Imagen,\" a diffusion-based model that achieves state-of-the-art results in photorealistic image generation and deep language understanding. It highlights the effectiveness of combining large language models with diffusion models, achieving superior fidelity and image-text alignment. Additionally, the paper provides empirical evidence of success, such as achieving a new state-of-the-art FID score on the COCO dataset and outperforming other models like GLIDE, DALL-E 2, and Latent Diffusion Models in human evaluations. This aligns with the user's request for research demonstrating success in this domain."}
{"input": "User Query: Could you list some works which proposed efficient variants of Transformer?\nTitle: Reformer: The Efficient Transformer\nAbstract:  Abstract Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(L2superscript𝐿2L^{2}) to O(L​log⁡L𝐿𝐿L\\log L), where L𝐿L is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of N𝑁N times, where N𝑁N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Reformer: The Efficient Transformer\" fully satisfies the user query. It explicitly proposes efficient variants of the Transformer by introducing two techniques: (1) replacing standard dot-product attention with locality-sensitive hashing to reduce computational complexity, and (2) using reversible residual layers to improve memory efficiency during training. These innovations directly aim to make the Transformer model more efficient, aligning with the requirement to list works that propose efficient Transformer variants."}
{"input": "User Query: What papers have applied episodic control on model-free DRL tasks to resolve sample inefficiency?\nTitle: Model-Free Episodic Control\nAbstract:  Abstract State of the art deep reinforcement learning algorithms take many millions of interactions to attain human-level performance. Humans, on the other hand, can very quickly exploit highly rewarding nuances of an environment upon first discovery. In the brain, such rapid learning is thought to depend on the hippocampus and its capacity for episodic memory. Here we investigate whether a simple model of hippocampal episodic control can learn to solve difficult sequential decision-making tasks. We demonstrate that it not only attains a highly rewarding strategy significantly faster than state-of-the-art deep reinforcement learning algorithms, but also achieves a higher overall reward on some of the more challenging domains. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Model-Free Episodic Control\" fully satisfies the user's query. The abstract explicitly states that the paper investigates the application of episodic control, inspired by the hippocampus, to solve sequential decision-making tasks. It highlights that this approach addresses sample inefficiency by learning highly rewarding strategies significantly faster than state-of-the-art deep reinforcement learning (DRL) algorithms. Additionally, the paper focuses on model-free DRL tasks, as it describes the method as \"model-free.\" Therefore, the paper aligns with the user's requirement of applying episodic control to resolve sample inefficiency in model-free DRL tasks."}
{"input": "User Query: The SoundStream model utilized a fully causal convolutional encoder and decoder network and performed residual vector quantization. What are the associated works?\nTitle: SoundStream: An End-to-End Neural Audio Codec\nAbstract:  Abstract We present SoundStream, a novel neural audio codec that can efficiently compress speech, music and general audio at bitrates normally targeted by speech-tailored codecs. SoundStream relies on a model architecture composed by a fully convolutional encoder/decoder network and a residual vector quantizer, which are trained jointly end-to-end. Training leverages recent advances in text-to-speech and speech enhancement, which combine adversarial and reconstruction losses to allow the generation of high-quality audio content from quantized embeddings. By training with structured dropout applied to quantizer layers, a single model can operate across variable bitrates from 3 kbps to 18 kbps, with a negligible quality loss when compared with models trained at fixed bitrates. In addition, the model is amenable to a low latency implementation, which supports streamable inference and runs in real time on a smartphone CPU. In subjective evaluations using audio at 24 kHz sampling rate, SoundStream at 3 kbps outperforms Opus at 12 kbps and approaches EVS at 9.6 kbps. Moreover, we are able to perform joint compression and enhancement either at the encoder or at the decoder side with no additional latency, which we demonstrate through background noise suppression for speech. ", "output": "**Decision:** True  \n**Reason:** The searched paper satisfies the requirements of the user query. The paper titled \"SoundStream: An End-to-End Neural Audio Codec\" explicitly describes the SoundStream model, which utilizes a fully convolutional encoder and decoder network and performs residual vector quantization. These details align with the query's focus on the model architecture and quantization method. While the query specifies \"fully causal\" convolutional networks, the paper's description of the encoder/decoder as fully convolutional suggests that it meets this requirement, as causal convolutions are typically implied in such applications for real-time processing. Additionally, the paper discusses the associated works and advancements in neural audio codecs, providing relevant context for the query."}
{"input": "User Query: What research proposes learning representations by using a denoising autoencoder?\nTitle: Augmented Autoencoders: Implicit 3D Orientation Learning for 6D Object Detection\nAbstract:  Abstract We propose a real-time RGB-based pipeline for object detection and 6D pose estimation. Our novel 3D orientation estimation is based on a variant of the Denoising Autoencoder that is trained on simulated views of a 3D model using Domain Randomization. This so-called Augmented Autoencoder has several advantages over existing methods: It does not require real, pose-annotated training data, generalizes to various test sensors and inherently handles object and view symmetries. Instead of learning an explicit mapping from input images to object poses, it provides an implicit representation of object orientations defined by samples in a latent space. Our pipeline achieves state-of-the-art performance on the T-LESS dataset both in the RGB and RGB-D domain. We also evaluate on the LineMOD dataset where we can compete with other synthetically trained approaches. We further increase performance by correcting 3D orientation estimates to account for perspective errors when the object deviates from the image center and show extended results. Our code is available here 111https://github.com/DLR-RM/AugmentedAutoencoder ", "output": "**Decision:** True  \n**Reason:** The searched paper titled *\"Augmented Autoencoders: Implicit 3D Orientation Learning for 6D Object Detection\"* satisfies the user query as it explicitly mentions the use of a variant of the Denoising Autoencoder for learning representations. The abstract describes how the proposed Augmented Autoencoder learns implicit representations of object orientations in a latent space, which aligns with the concept of learning representations using a denoising autoencoder. Therefore, the paper is relevant to the query."}
{"input": "User Query: Can you provide me examples of research where language models are used for decision making in conversation models?\nTitle: A Simple Language Model for Task-Oriented Dialogue\nAbstract:  Abstract Task-oriented dialogue is often decomposed into three tasks: understanding user input, deciding actions, and generating a response. While such decomposition might suggest a dedicated model for each sub-task, we find a simple, unified approach leads to state-of-the-art performance on the MultiWOZ dataset. SimpleTOD is a simple approach to task-oriented dialogue that uses a single, causal language model trained on all sub-tasks recast as a single sequence prediction problem. This allows SimpleTOD to fully leverage transfer learning from pre-trained, open domain, causal language models such as GPT-2. SimpleTOD improves over the prior state-of-the-art in joint goal accuracy for dialogue state tracking, and our analysis reveals robustness to noisy annotations in this setting. SimpleTOD also improves the main metrics used to evaluate action decisions and response generation in an end-to-end setting: inform rate by 8.1 points, success rate by 9.7 points, and combined score by 7.2 points.  ", "output": "**Decision:** True  \n**Reason:** The paper titled \"A Simple Language Model for Task-Oriented Dialogue\" satisfies the user query because it explicitly discusses the use of a language model (SimpleTOD) for decision-making in conversation models. The paper describes how the model performs tasks such as understanding user input, deciding actions, and generating responses in a unified framework. Specifically, it mentions that SimpleTOD improves metrics used to evaluate action decisions, which directly relates to decision-making in conversation models. Therefore, the paper aligns well with the user's request for examples of research where language models are used for decision-making in conversational contexts."}
{"input": "User Query: Any works report that contrastive visual learning could enhance contrastive sentence embeddings?\nTitle: Non-Linguistic Supervision for Contrastive Learning of Sentence Embeddings\nAbstract:  Abstract Semantic representation learning for sentences is an important and well-studied problem in NLP. The current trend for this task involves training a Transformer-based sentence encoder through a contrastive objective with text, i.e., clustering sentences with semantically similar meanings and scattering others. In this work, we find the performance of Transformer models as sentence encoders can be improved by training with multi-modal multi-task losses, using unpaired examples from another modality (e.g., sentences and unrelated image/audio data). In particular, besides learning by the contrastive loss on text, our model clusters examples from a non-linguistic domain (e.g., visual/audio) with a similar contrastive loss at the same time. The reliance of our framework on unpaired non-linguistic data makes it language-agnostic, enabling it to be widely applicable beyond English NLP. Experiments on 7 semantic textual similarity benchmarks reveal that models trained with the additional non-linguistic (images/audio) contrastive objective lead to higher quality sentence embeddings. This indicates that Transformer models are able to generalize better by doing a similar task (i.e., clustering) with unpaired examples from different modalities in a multi-task fashion. The code is available at https://github.com/yiren-jian/NonLing-CSE. ", "output": "**Decision:** True  \n**Reason:** The paper satisfies the user query as it explicitly reports that contrastive visual learning (using images as a non-linguistic modality) can enhance contrastive sentence embeddings. The abstract describes a framework where Transformer-based sentence encoders are trained with a contrastive objective not only on text but also on non-linguistic data, such as visual (image) data. The experiments demonstrate that incorporating this additional contrastive objective from non-linguistic domains, including visual data, improves the quality of sentence embeddings. This directly aligns with the user's query about whether contrastive visual learning could enhance contrastive sentence embeddings."}
{"input": "User Query: Which study introduced self-critical training, a policy gradient method that baselines the REINFORCE gradient estimator?\nTitle: Self-critical Sequence Training for Image Captioning\nAbstract:  Abstract Recently it has been shown that policy-gradient methods for reinforcement learning can be utilized to train deep end-to-end systems directly on non-differentiable metrics for the task at hand. In this paper we consider the problem of optimizing image captioning systems using reinforcement learning, and show that by carefully optimizing our systems using the test metrics of the MSCOCO task, significant gains in performance can be realized. Our systems are built using a new optimization approach that we call self-critical sequence training (SCST). SCST is a form of the popular REINFORCE algorithm that, rather than estimating a “baseline” to normalize the rewards and reduce variance, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. Using this approach, estimating the reward signal (as actor-critic methods must do) and estimating normalization (as REINFORCE algorithms typically do) is avoided, while at the same time harmonizing the model with respect to its test-time inference procedure. Empirically we find that directly optimizing the CIDEr metric with SCST and greedy decoding at test-time is highly effective. Our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task, improving the best result in terms of CIDEr from 104.9 to 114.7. ", "output": "**Decision:** True  \n**Reason:** The searched paper titled \"Self-critical Sequence Training for Image Captioning\" fully satisfies the user query. The abstract explicitly states that the study introduces a new optimization approach called self-critical sequence training (SCST), which is a form of the REINFORCE algorithm. The paper explains that SCST uses the output of its own test-time inference algorithm as a baseline to normalize rewards, thereby reducing variance in the gradient estimator. This directly addresses the query about which study introduced self-critical training as a policy gradient method that baselines the REINFORCE gradient estimator. Therefore, the paper is the correct reference for the user query."}
{"input": "User Query: Could you cite the works that discussed predictive confidence as a method for uncertainty estimation?\nTitle: A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks\nAbstract:  Abstract We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks. ", "output": "**Decision:** True  \n**Reason:** The searched paper, \"A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks,\" directly discusses predictive confidence as a method for uncertainty estimation. The paper proposes a baseline method that utilizes the maximum softmax probabilities (a measure of predictive confidence) to detect misclassified and out-of-distribution examples. This aligns with the user's query, as it explicitly involves predictive confidence as a tool for estimating uncertainty in neural network predictions."}
{"input": "User Query: Which studies focus on fairness in a context of federated learning?\nTitle: Fair Resource Allocation in Federated Learning\nAbstract:  Abstract Federated learning involves training statistical models in massive, heterogeneous networks. Naively minimizing an aggregate loss function in such a network may disproportionately advantage or disadvantage some of the devices. In this work, we propose q𝑞q-Fair Federated Learning (q𝑞q-FFL), a novel optimization objective inspired by fair resource allocation in wireless networks that encourages a more fair (specifically, a more uniform) accuracy distribution across devices in federated networks. To solve q𝑞q-FFL, we devise a communication-efficient method, q𝑞q-FedAvg, that is suited to federated networks. We validate both the effectiveness of q𝑞q-FFL and the efficiency of q𝑞q-FedAvg on a suite of federated datasets with both convex and non-convex models, and show that q𝑞q-FFL (along with q𝑞q-FedAvg) outperforms existing baselines in terms of the resulting fairness, flexibility, and efficiency. ", "output": "**Decision:** True  \n**Reason:** The paper explicitly focuses on fairness in the context of federated learning. It addresses the issue of disproportionate advantages or disadvantages to devices in federated networks, which is a fairness concern. The proposed method, q𝑞q-Fair Federated Learning (q𝑞q-FFL), is designed to encourage a more uniform accuracy distribution across devices, directly tackling fairness. Additionally, the paper validates its approach on federated datasets, further confirming its relevance to federated learning. Thus, the paper fully satisfies the user query."}
{"input": "User Query: What studies have used BEV layouts to augment image data with synthetic single or multi-view images?\nTitle: Street-View Image Generation from a Bird’s-Eye View Layout\nAbstract:  Abstract Bird’s-Eye View (BEV) Perception has received increasing attention in recent years as it provides a concise and unified spatial representation across views and benefits a diverse set of downstream driving applications. At the same time, data-driven simulation for autonomous driving has been a focal point of recent research but with few approaches that are both fully data-driven and controllable. Instead of using perception data from real-life scenarios, an ideal model for simulation would generate realistic street-view images that align with a given HD map and traffic layout, a task that is critical for visualizing complex traffic scenarios and developing robust perception models for autonomous driving. In this paper, we propose BEVGen, a conditional generative model that synthesizes a set of realistic and spatially consistent surrounding images that match the BEV layout of a traffic scenario. BEVGen incorporates a novel cross-view transformation with spatial attention design which learns the relationship between cameras and map views to ensure their consistency. We evaluate the proposed model on the challenging NuScenes and Argoverse 2 datasets. After training, BEVGen can accurately render road and lane lines, as well as generate traffic scenes with diverse different weather conditions and times of day. The code is open-source and available at metadriverse.github.io/bevgen. ", "output": "**Decision:** True  \n**Reason:** The paper titled \"Street-View Image Generation from a Bird’s-Eye View Layout\" satisfies the requirements of the user query. The study explicitly uses Bird’s-Eye View (BEV) layouts to generate synthetic single or multi-view images, as described in the abstract. The proposed BEVGen model synthesizes realistic and spatially consistent street-view images based on BEV layouts of traffic scenarios. This aligns with the query's focus on studies that use BEV layouts to augment image data with synthetic images. Furthermore, the paper evaluates the model on datasets and highlights its ability to generate diverse traffic scenes, confirming its relevance to the query."}
